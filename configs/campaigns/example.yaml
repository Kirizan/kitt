campaign_name: example-campaign
description: "Example campaign testing 2 models across llama.cpp and Ollama"

models:
  - name: Llama-3.1-8B-Instruct
    params: "8B"
    safetensors_repo: meta-llama/Llama-3.1-8B-Instruct
    gguf_repo: bartowski/Meta-Llama-3.1-8B-Instruct-GGUF
    ollama_tag: "llama3.1:8b"
    estimated_size_gb: 16.0

  - name: Qwen2.5-7B-Instruct
    params: "7B"
    safetensors_repo: Qwen/Qwen2.5-7B-Instruct
    gguf_repo: Qwen/Qwen2.5-7B-Instruct-GGUF
    ollama_tag: "qwen2.5:7b"
    estimated_size_gb: 14.0

engines:
  - name: llama_cpp
    suite: standard
    formats: [gguf]
    config: {}

  - name: ollama
    suite: standard
    formats: [gguf]
    config: {}

disk:
  reserve_gb: 100.0
  cleanup_after_run: true

notifications:
  desktop: true
  on_complete: true
  on_failure: true

quant_filter:
  skip_patterns:
    - "IQ1_*"
    - "IQ2_*"
    # Q4_0 repacking variants removed from llama.cpp â€” use plain Q4_0 instead
    - "Q4_0_4_4"
    - "Q4_0_4_8"
    - "Q4_0_8_8"

parallel: false
devon_managed: true
