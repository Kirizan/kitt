campaign_name: rerun-failures
description: >
  Re-run 4 non-GGUF failures from the first DGX Spark campaign
  with improved error capture. Targets:
  1. Llama-3.1-8B-Instruct | vllm | bf16 (empty error, 605s)
  2. Llama-3.1-8B-Instruct | llama_cpp | Q4_0 (empty error, sharded Q4_0_4_4)
  3. Llama-3.3-70B-Instruct | ollama | 70b-instruct-fp16 (OOM â€” should be skipped)
  4. Phi-4 | llama_cpp | f32 (init failed, sharded f32)

models:
  - name: Llama-3.1-8B-Instruct
    params: "8B"
    safetensors_repo: meta-llama/Llama-3.1-8B-Instruct
    gguf_repo: bartowski/Meta-Llama-3.1-8B-Instruct-GGUF
    estimated_size_gb: 16.0

  - name: Llama-3.3-70B-Instruct
    params: "70B"
    ollama_tag: "llama3.3:70b"
    estimated_size_gb: 70.0

  - name: Phi-4
    params: "14B"
    gguf_repo: bartowski/Phi-4-GGUF
    estimated_size_gb: 28.0

engines:
  # vllm for Llama-3.1-8B bf16
  - name: vllm
    suite: quick
    formats: [safetensors]
    config: {}

  # llama_cpp for Llama-3.1-8B Q4_0 and Phi-4 f32
  - name: llama_cpp
    suite: quick
    formats: [gguf]
    config: {}

  # ollama for Llama-3.3-70B fp16
  - name: ollama
    suite: quick
    formats: [gguf]
    config: {}

disk:
  reserve_gb: 50.0
  cleanup_after_run: true

quant_filter:
  # Only include the specific quants that failed (minus removed variants)
  include_only:
    - "Q4_0"
    - "f32"
    - "F32"
  skip_patterns:
    # Q4_0 repacking variants removed from llama.cpp
    - "Q4_0_4_4"
    - "Q4_0_4_8"
    - "Q4_0_8_8"

resource_limits:
  # Skip models > 100GB (catches 70B fp16 @ ~154GB)
  max_model_size_gb: 100.0

notifications:
  desktop: true
  on_complete: true
  on_failure: true

parallel: false
devon_managed: true
