campaign_name: dgx-spark-full
description: "Full DGX Spark benchmark campaign: 5 models × 3 engines"

models:
  - name: Llama-3.1-8B-Instruct
    params: "8B"
    safetensors_repo: meta-llama/Llama-3.1-8B-Instruct
    gguf_repo: bartowski/Meta-Llama-3.1-8B-Instruct-GGUF
    ollama_tag: "llama3.1:8b"
    estimated_size_gb: 16.0

  - name: Qwen2.5-7B-Instruct
    params: "7B"
    safetensors_repo: Qwen/Qwen2.5-7B-Instruct
    gguf_repo: Qwen/Qwen2.5-7B-Instruct-GGUF
    ollama_tag: "qwen2.5:7b"
    estimated_size_gb: 14.0

  - name: Mistral-7B-Instruct-v0.3
    params: "7B"
    safetensors_repo: mistralai/Mistral-7B-Instruct-v0.3
    gguf_repo: bartowski/Mistral-7B-Instruct-v0.3-GGUF
    ollama_tag: "mistral:7b"
    estimated_size_gb: 14.0

  - name: Llama-3.3-70B-Instruct
    params: "70B"
    gguf_repo: bartowski/Llama-3.3-70B-Instruct-GGUF
    ollama_tag: "llama3.3:70b"
    estimated_size_gb: 70.0

  - name: Phi-4
    params: "14B"
    safetensors_repo: microsoft/Phi-4
    gguf_repo: bartowski/Phi-4-GGUF
    ollama_tag: "phi4:latest"
    estimated_size_gb: 28.0

engines:
  - name: vllm
    suite: standard
    formats: [safetensors]
    config: {}

  - name: llama_cpp
    suite: standard
    formats: [gguf]
    config: {}

  - name: ollama
    suite: standard
    formats: [gguf]
    config: {}

disk:
  reserve_gb: 100.0
  cleanup_after_run: true

notifications:
  desktop: true
  on_complete: true
  on_failure: true

quant_filter:
  skip_patterns:
    - "IQ1_*"
    - "IQ2_*"
    # Q4_0 repacking variants removed from llama.cpp — use plain Q4_0 instead
    - "Q4_0_4_4"
    - "Q4_0_4_8"
    - "Q4_0_8_8"

resource_limits:
  # DGX Spark has 128GB unified memory — skip models that won't fit
  max_model_size_gb: 100.0

parallel: false
devon_managed: true
