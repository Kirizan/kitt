# KITT-managed Dockerfile for llama.cpp on DGX Spark (GB10, ARM64, sm_121)
#
# Builds llama.cpp server with CUDA 13.1.1 targeting Blackwell edge GPU.
# Multi-stage build: devel image for compilation, runtime image for execution.
#
# Build: docker build -f docker/llama_cpp/Dockerfile.spark \
#          --target server -t kitt/llama-cpp:spark .
# Run:   docker run --gpus all --network host kitt/llama-cpp:spark ...
#
# Typical build time: 10-20 minutes on DGX Spark.

ARG CUDA_VERSION=13.1.1
ARG UBUNTU_VERSION=24.04

# ---- Stage 1: Build ----
FROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu${UBUNTU_VERSION} AS build

RUN apt-get update && apt-get install -y --no-install-recommends \
        git cmake build-essential libcurl4-openssl-dev \
    && rm -rf /var/lib/apt/lists/*

# Clone llama.cpp at build time (pin via LLAMA_CPP_REF if needed)
ARG LLAMA_CPP_REF=master
RUN git clone --depth 1 --branch ${LLAMA_CPP_REF} \
        https://github.com/ggerganov/llama.cpp.git /llama.cpp

WORKDIR /llama.cpp

# Build with native optimisations and sm_121 CUDA arch
#
# The CUDA driver library (libcuda.so) ships with the host driver, not the
# toolkit.  CUDA devel images include a stub at /usr/local/cuda/lib64/stubs/
# but only as libcuda.so (no versioned SONAME symlink).
#
# 1. Create libcuda.so.1 symlink so the linker can resolve the SONAME that
#    gets recorded as a DT_NEEDED entry in libggml-cuda.so.
# 2. LIBRARY_PATH lets the *direct* link step (-lcuda) find the stub.
# 3. -rpath-link lets the *transitive* dependency resolution find libcuda.so.1
#    when linking the final executable against libggml-cuda.so.
RUN ln -sf /usr/local/cuda/lib64/stubs/libcuda.so \
           /usr/local/cuda/lib64/stubs/libcuda.so.1
ENV LIBRARY_PATH=/usr/local/cuda/lib64/stubs:${LIBRARY_PATH}

RUN cmake -B build \
        -DGGML_CUDA=ON \
        -DGGML_NATIVE=ON \
        -DCMAKE_CUDA_ARCHITECTURES=121 \
        -DLLAMA_CURL=ON \
        -DCMAKE_EXE_LINKER_FLAGS="-Wl,-rpath-link=/usr/local/cuda/lib64/stubs" \
    && cmake --build build --config Release --target llama-server -j$(nproc)

# ---- Stage 2: Runtime ----
FROM nvidia/cuda:${CUDA_VERSION}-runtime-ubuntu${UBUNTU_VERSION} AS server

RUN apt-get update && apt-get install -y --no-install-recommends \
        libcurl4 libgomp1 \
    && rm -rf /var/lib/apt/lists/*

COPY --from=build /llama.cpp/build/bin/llama-server /usr/local/bin/llama-server
COPY --from=build /llama.cpp/build/bin/lib*.so* /usr/local/lib/

RUN ldconfig

ENV LLAMA_ARG_HOST=0.0.0.0

HEALTHCHECK --interval=10s --timeout=5s --retries=3 \
    CMD curl -f http://localhost:8080/health || exit 1

ENTRYPOINT ["llama-server"]
