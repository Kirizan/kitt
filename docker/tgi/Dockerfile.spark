# KITT-managed Dockerfile for TGI on DGX Spark (GB10, ARM64, sm_121)
#
# !! NON-FUNCTIONAL — KEPT FOR REFERENCE ONLY !!
#
# TGI entered maintenance mode in December 2025 and has no official ARM64 or
# Blackwell support.  After extensive testing, TGI is NOT viable on DGX Spark
# due to hard dependencies on custom CUDA kernels with no fallback path:
#
#   - dropout_layer_norm (from flash-attention) — imported unconditionally by
#     layers/layernorm.py when SYSTEM == "cuda".  No aarch64+sm_121 wheels.
#   - flash_attn_2_cuda / flash_attn_cuda — attention layer.  flash-attention
#     does not support sm_121 (see flash-attention issue #1969).
#   - flashinfer — alternative attention backend.  No aarch64 pre-built wheels.
#   - vllm._custom_ops — rotary embeddings, linear, KV cache.  Not available
#     standalone for sm_121.
#
# The DISABLE_CUSTOM_KERNELS flag only affects the Rust launcher, NOT the
# Python-level module imports that crash at startup.
#
# What DOES work in this Dockerfile (useful if TGI ever gets ARM64 support):
#   - Rust binaries (launcher + router) build cleanly on aarch64.
#   - PyTorch cu130 wheels include sm_120 kernels (binary-compat with sm_121).
#   - Protobuf generation for the gRPC server.
#   - The kernel loader patch gracefully handles missing optional kernels.
#
# Build: docker build -f docker/tgi/Dockerfile.spark \
#          -t kitt/tgi:spark .
# Run:   docker run --gpus all --network host kitt/tgi:spark \
#          --model-id /models/MyModel --disable-custom-kernels
#
# Typical build time: 30-60 minutes (Rust + PyTorch + Python deps).

ARG CUDA_VERSION=13.1.1
ARG UBUNTU_VERSION=24.04

# ---- Stage 1: Build ----
FROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu${UBUNTU_VERSION} AS build

ENV DEBIAN_FRONTEND=noninteractive

RUN apt-get update && apt-get install -y --no-install-recommends \
        git curl build-essential pkg-config libssl-dev protobuf-compiler \
        python3 python3-dev python3-pip python3-venv \
    && rm -rf /var/lib/apt/lists/*

# Install Rust toolchain
RUN curl --proto '=https' --tlsv1.2 -sSf https://sh.rustup.rs | sh -s -- -y
ENV PATH="/root/.cargo/bin:${PATH}"

# Clone TGI
ARG TGI_REF=main
RUN git clone --depth 1 --branch ${TGI_REF} \
        https://github.com/huggingface/text-generation-inference.git /tgi

WORKDIR /tgi

# Build the Rust launcher and router
ENV CUDA_COMPUTE_CAP=121
RUN cargo build --release --bin text-generation-launcher --bin text-generation-router

# Python server: install PyTorch cu130 (has sm_120 kernels, binary-compat with
# sm_121), then the TGI server package without CUDA extras (flash-attention
# doesn't support sm_121).
RUN python3 -m venv /opt/tgi-env \
    && /opt/tgi-env/bin/pip install --no-cache-dir --upgrade pip setuptools wheel \
    && /opt/tgi-env/bin/pip install --no-cache-dir ninja packaging

# PyTorch cu130 — aarch64 wheels with sm_120 support (binary-compat with sm_121)
RUN /opt/tgi-env/bin/pip install --no-cache-dir \
        torch --index-url https://download.pytorch.org/whl/cu130

# Generate protobuf Python files (grpc_tools.protoc compiles .proto -> _pb2.py).
# This is a separate step from pip install — TGI's Makefile handles it.
RUN /opt/tgi-env/bin/pip install --no-cache-dir grpcio-tools \
    && cd server \
    && mkdir -p text_generation_server/pb \
    && /opt/tgi-env/bin/python -m grpc_tools.protoc \
        -I../proto/v3 \
        --python_out=text_generation_server/pb \
        --grpc_python_out=text_generation_server/pb \
        ../proto/v3/generate.proto \
    && sed -i 's/^import \(.*_pb2\)/from . import \1/' text_generation_server/pb/*.py \
    && sed -i 's/^from.*import \(.*_pb2\)/from . import \1/' text_generation_server/pb/*.py \
    && touch text_generation_server/pb/__init__.py

# Install TGI server (non-editable, without [cuda] extras to avoid pulling
# flash-attention which doesn't support sm_121).
# Then install optional deps that are imported unconditionally by the server.
RUN cd server \
    && /opt/tgi-env/bin/pip install --no-cache-dir ".[peft,outlines,accelerate,compressed-tensors]"

# Patch TGI's kernel loader for sm_121 compatibility.
# The kernels package API changed (requires lockfile arg), and custom CUDA
# kernels (punica, marlin, etc.) aren't available for sm_121 anyway.
# This makes load_kernel return None on failure instead of crashing at import.
RUN KERNELS_PY="/opt/tgi-env/lib/python3.12/site-packages/text_generation_server/utils/kernels.py" \
    && cat > "$KERNELS_PY" << 'PYEOF'
import importlib
import os

from loguru import logger


def load_kernel(*, module: str, repo_id: str):
    """Load a kernel, returning None if unavailable (e.g. on sm_121)."""
    try:
        m = importlib.import_module(module)
        logger.info(f"Using local module for `{module}`")
        return m
    except (ModuleNotFoundError, ImportError):
        pass

    # Try HuggingFace kernel hub (may fail on unsupported architectures)
    if os.environ.get("DISABLE_CUSTOM_KERNELS", "").lower() not in ("1", "true"):
        try:
            from kernels import load_kernel as hf_load_kernel
            import inspect
            sig = inspect.signature(hf_load_kernel)
            if "lockfile" in sig.parameters:
                return hf_load_kernel(repo_id=repo_id, lockfile=None)
            return hf_load_kernel(repo_id=repo_id)
        except Exception as e:
            logger.warning(f"Could not load kernel `{module}`: {e}")

    return None


__all__ = ["load_kernel"]
PYEOF

# ---- Stage 2: Runtime ----
FROM nvidia/cuda:${CUDA_VERSION}-runtime-ubuntu${UBUNTU_VERSION} AS runtime

ENV DEBIAN_FRONTEND=noninteractive

RUN apt-get update && apt-get install -y --no-install-recommends \
        python3 libpython3-dev libssl3 ca-certificates curl \
    && rm -rf /var/lib/apt/lists/*

COPY --from=build /tgi/target/release/text-generation-launcher /usr/local/bin/
COPY --from=build /tgi/target/release/text-generation-router /usr/local/bin/
COPY --from=build /opt/tgi-env /opt/tgi-env

ENV PATH="/opt/tgi-env/bin:${PATH}"
ENV PORT=8080
# Custom CUDA kernels (flash-attention, punica, marlin) don't support sm_121.
ENV DISABLE_CUSTOM_KERNELS=true

HEALTHCHECK --interval=10s --timeout=5s --retries=3 \
    CMD curl -f http://localhost:${PORT}/health || exit 1

ENTRYPOINT ["text-generation-launcher"]
