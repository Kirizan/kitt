{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"KITT Documentation","text":"<p>End-to-end testing suite for LLM inference engines.</p> <p>KITT (Kirizan's Inference Testing Tools) measures quality consistency and performance across LLM inference engines -- vLLM, TGI, llama.cpp, and Ollama -- so you can make informed deployment decisions backed by reproducible data.</p> <p>Every engine runs in a Docker container. You supply a model path, pick an engine, and KITT handles the rest: container lifecycle, benchmark execution, metric collection, and result storage.</p>"},{"location":"#feature-highlights","title":"Feature Highlights","text":"Multi-Engine Support Test across vLLM, TGI, llama.cpp, and Ollama with a single command. Compare results side by side across engines. Docker-Only Engines Every engine runs in a container -- no host installs required. KITT manages the full container lifecycle automatically. Quality Benchmarks Evaluate model accuracy with MMLU, GSM8K, TruthfulQA, and HellaSwag. Checkpoint recovery keeps long runs safe. Performance Benchmarks Measure throughput, latency, memory usage, and warmup characteristics under controlled conditions. Hardware Fingerprinting Automatic GPU, CPU, RAM, storage, and CUDA detection. Results are tagged with a compact fingerprint for reproducibility. KARR Results Storage All benchmark results persisted through KARR (Kitt's AI Results Repository). Database-backed by default (SQLite or PostgreSQL) with full query, aggregation, and export support. Multiple Output Formats Export results as JSON, Markdown summaries, or comparison tables. JSON output integrates directly with CI pipelines. Custom YAML Benchmarks Define your own test cases in YAML. Mix custom benchmarks with built-in ones in any suite configuration. Docker Deployment Stacks Generate composable <code>docker-compose.yaml</code> files with optional web dashboard, monitoring, PostgreSQL, and agent components. Web Dashboard + REST API Flask-powered UI for browsing results and a REST API for programmatic access to benchmark data. Monitoring Prometheus metrics collection with pre-built Grafana dashboards for real-time engine and system observability. CI Integration JSON output and meaningful exit codes make KITT easy to integrate into automated testing pipelines."},{"location":"#quick-links","title":"Quick Links","text":""},{"location":"#getting-started","title":"Getting Started","text":"<p>Install KITT, run your first benchmark, and explore Docker deployment workflows.</p> <ul> <li>Installation -- Docker and source install</li> <li>Tutorial: First Benchmark -- end-to-end walkthrough</li> <li>Tutorial: Docker Quickstart -- container-based usage</li> </ul>"},{"location":"#guides","title":"Guides","text":"<p>In-depth guides for engines, benchmarks, results management, deployment, and more.</p>"},{"location":"#reference","title":"Reference","text":"<p>CLI reference, configuration schema, REST API, Docker files, and environment variables.</p>"},{"location":"#concepts","title":"Concepts","text":"<p>Architecture overview, hardware fingerprinting, KARR results storage, engine lifecycle, and the benchmark system.</p>"},{"location":"#how-it-works","title":"How It Works","text":"<pre><code> You                    KITT                     Docker\n \u2500\u2500\u2500                    \u2500\u2500\u2500\u2500                     \u2500\u2500\u2500\u2500\u2500\u2500\n  \u2502  kitt run             \u2502                        \u2502\n  \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt;\u2502  docker run engine     \u2502\n  \u2502                       \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt;\u2502\n  \u2502                       \u2502  health-check loop     \u2502\n  \u2502                       \u2502 &lt;\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt;\u2502\n  \u2502                       \u2502  run benchmarks        \u2502\n  \u2502                       \u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt;\u2502\n  \u2502                       \u2502  collect metrics       \u2502\n  \u2502                       \u2502 &lt;\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502\n  \u2502  results + summary    \u2502  docker stop           \u2502\n  \u2502 &lt;\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2502 \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500&gt;\u2502\n</code></pre> <p>KITT is purpose-built for generative LLMs (Llama, Qwen, Mistral, and similar). Encoder-only models such as BERT are not supported because they cannot produce text through the inference engine APIs.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>All notable changes to KITT are documented on this page.</p>"},{"location":"changelog/#121","title":"1.2.1","text":"<ul> <li>Fixed agent benchmark results never reaching the server \u2014 <code>_execute_test()</code> now reads <code>metrics.json</code> from the output directory and forwards it as <code>result_data</code> in <code>_report()</code></li> <li>Fixed <code>PermissionError</code> when <code>kitt run</code> defaults to relative <code>kitt-results/</code> inside a Docker container \u2014 agent now passes <code>-o</code> with a writable temp directory to <code>kitt run</code></li> <li>Changed default output directory for <code>kitt run</code> from relative <code>kitt-results/</code> to <code>~/.kitt/results/</code> for robustness across environments</li> <li>Temp output directories (<code>/tmp/kitt-results-*</code>) are cleaned up after agent benchmarks complete</li> <li>Fixed architecture mismatch in agent Docker image selection \u2014 now checks image arch against host arch before use, falling back to locally-built <code>kitt:latest</code> when the registry image is the wrong platform</li> <li>Fixed <code>_report()</code> using agent name instead of agent ID in URL, causing 404 on result submission</li> <li>Fixed Docker entrypoint override for benchmark containers \u2014 added <code>--entrypoint kitt</code> since the KITT image has <code>ENTRYPOINT [\"kitt\", \"web\"]</code></li> <li>Reverted Docker CLI package name in Dockerfiles back to <code>docker.io</code> \u2014 <code>docker-cli</code> does not exist in Debian bookworm repos</li> <li>Updated hardcoded <code>kitt_version</code> references from <code>1.1.0</code> to <code>1.2.1</code></li> <li>Fixed <code>_check_auth()</code> timing attack \u2014 replaced <code>==</code> with <code>hmac.compare_digest</code> for constant-time token comparison</li> <li>Fixed thread safety \u2014 write lock now wraps entire SQLite transactions (execute + commit), not just the commit</li> <li>Fixed <code>_find_on_share()</code> path traversal \u2014 resolved candidates are validated against the share mount root</li> <li>Fixed result detail 404 \u2014 <code>query()</code> and <code>get_result()</code> now include the database ID in returned results</li> <li>Fixed <code>kitt --version</code> reporting wrong version \u2014 now reads from <code>kitt.__version__</code> dynamically</li> <li>Fixed cleanup button in agent detail page targeting the wrong endpoint (API instead of blueprint)</li> <li>Added <code>@require_auth</code> to <code>GET /api/v1/agents/&lt;id&gt;/settings</code> endpoint</li> <li>Added <code>kitt_image</code> to migration v8 defaults for consistency with <code>AgentManager._DEFAULT_SETTINGS</code></li> <li>Added CSRF protection (<code>@csrf_protect</code>) to all state-changing web blueprint endpoints</li> <li>Added SHA-256 integrity verification for agent package downloads and build context</li> <li>Added Docker environment variable redaction in container start logs</li> <li>Added rotating file logging to agent (<code>~/.kitt/logs/agent.log</code>, 5MB per file, 3 backups)</li> <li>Fixed preflight server reachability check \u2014 tries TLS verification first, falls back to insecure for self-signed certs</li> <li>Fixed preflight <code>URLError</code>-wrapped SSL errors bypassing the insecure fallback</li> <li>Removed unused <code>verify</code> and <code>client_cert</code> parameters from <code>_report()</code> function</li> <li>Fixed cleanup endpoints bypassing <code>_write_lock</code> \u2014 extracted <code>AgentManager.queue_cleanup_command()</code> method</li> <li>Fixed <code>register()</code> TOCTOU race \u2014 moved SELECT inside write lock to prevent concurrent duplicate inserts</li> <li>Fixed <code>_find_on_share()</code> glob results not validated against share root (symlink traversal)</li> <li>Fixed <code>_find_on_share()</code> path validation to use <code>relative_to()</code> instead of <code>str.startswith()</code> for robustness</li> <li>Fixed <code>csrf_protect</code> Bearer token exemption \u2014 now validates the token before bypassing CSRF check</li> <li>Fixed hardcoded <code>kitt_version</code> in <code>run.py</code> and <code>json_reporter.py</code> \u2014 now uses <code>kitt.__version__</code> dynamically</li> <li>Fixed HTMX storage polling in agent detail page targeting JSON endpoint instead of HTML page</li> <li>Fixed <code>tarfile.extractall(filter=\"data\")</code> incompatibility with Python 3.10 \u2014 guarded with version check</li> <li>Removed obsolete <code>docker/agent/Dockerfile</code> referencing deleted full agent</li> <li>Fixed <code>docker-cli</code> references in documentation (<code>docs/reference/docker-files.md</code>)</li> <li>Replaced f-string logger calls with lazy <code>%s</code> formatting across agent package and server</li> <li>Refactored API token verification \u2014 extracted <code>check_agent_auth()</code> and <code>check_agent_auth_by_name()</code> methods on <code>AgentManager</code>, removing raw <code>_conn</code> access from API endpoints</li> <li>Fixed <code>tarfile.extractall(filter=\"data\")</code> guard to use <code>try/except TypeError</code> instead of version check, correctly handling Python 3.11.4+ backport</li> <li>Fixed <code>docs/reference/api.md</code> auth column for <code>GET /api/v1/agents/&lt;id&gt;/settings</code> \u2014 was incorrectly listed as unauthenticated</li> <li>Removed stale <code>docker/agent/Dockerfile</code> row from <code>docs/reference/docker-files.md</code></li> <li>Removed dead <code>docker/agent/Dockerfile</code> reference in stack generator</li> <li>Fixed remaining f-string logger calls in <code>migrations.py</code> and <code>agent_install.py</code></li> <li>Updated stale <code>kitt_version</code> in test fixtures from <code>1.1.0</code> to <code>1.2.1</code></li> <li>Reverted <code>docker-cli</code> back to <code>docker.io</code> in Dockerfiles \u2014 <code>docker-cli</code> does not exist in Debian bookworm repos</li> <li>Added <code>save_result()</code> method to <code>ResultService</code> \u2014 <code>report_result</code> endpoint no longer accesses private <code>_store</code> directly</li> <li>Fixed <code>docs/reference/api.md</code> auth columns for <code>PATCH</code> and <code>DELETE</code> agent endpoints \u2014 were incorrectly listed as unauthenticated</li> <li>Fixed <code>docs/concepts/architecture.md</code> result reporting URL from <code>{name}</code> to <code>{id}</code></li> <li>Fixed install script preflight check \u2014 <code>if [ $? -ne 0 ]</code> was dead code under <code>set -euo pipefail</code>, replaced with <code>if ! command</code> pattern</li> <li>Fixed Docker container leak on health check timeout \u2014 container is now stopped before reporting failure</li> <li>Removed unused <code>--foreground</code> flag from <code>kitt agent start</code> proxy command</li> <li>Fixed all <code>docs/reference/api.md</code> auth columns to match actual <code>@require_auth</code> decorators (results, campaigns, models sections)</li> <li>Updated README thin agent architecture description to reflect model resolution, Docker benchmark execution, and heartbeat command dispatch</li> </ul>"},{"location":"changelog/#120","title":"1.2.0","text":"<ul> <li>Agent model workflow: copy models from NFS share to local storage, benchmark, cleanup</li> <li>Per-agent settings configurable from the web UI (model storage, share mount, cleanup, heartbeat interval)</li> <li>Agent settings synced to agents via heartbeat response</li> <li>NFS share mounting support with fstab and explicit mount fallback</li> <li>Preflight prerequisite checks (<code>kitt-agent preflight</code>) \u2014 Docker, GPU, drivers, NFS, disk space, connectivity</li> <li>Install script runs preflight before completing installation</li> <li>Heartbeat throttling during benchmarks (auto-increases interval to 60s minimum)</li> <li><code>cleanup_storage</code> command for remote model cleanup via heartbeat dispatch</li> <li>Storage usage reporting in heartbeat payload</li> <li>Removed full agent (<code>src/kitt/agent/</code>) \u2014 thin agent (<code>agent-package/</code>) is now the only agent</li> <li><code>kitt agent</code> CLI commands now proxy to <code>kitt-agent</code> binary</li> <li>Agent settings REST API endpoints (<code>GET</code>/<code>PUT /api/v1/agents/&lt;id&gt;/settings</code>)</li> <li>Storage cleanup REST API (<code>POST /api/v1/agents/&lt;id&gt;/cleanup</code>)</li> <li>DB migration v8: <code>agent_settings</code> key-value table per agent</li> <li>Daemon refactored \u2014 consolidated duplicated run methods into shared helpers</li> <li>Version policy: every PR must increment version going forward</li> <li><code>kitt-agent build</code> command for native-arch Docker image building</li> <li>Docker container is the preferred benchmark execution method (local CLI is fallback)</li> <li>Build context API endpoint (<code>/api/v1/agent/build-context</code>)</li> <li>Install script auto-builds Docker image during agent installation</li> <li>Preflight check for KITT Docker image availability</li> </ul>"},{"location":"changelog/#110","title":"1.1.0","text":"<ul> <li>Added composable Docker deployment stacks (<code>kitt stack</code>)</li> <li>Added web UI and distributed agent architecture</li> <li>Added monitoring stack generation and remote deployment</li> <li>Added documentation site with MkDocs Material</li> <li>Added UI-configurable settings \u2014 Model Directory, Devon URL, and Results Directory can be edited from the Settings page with live updates</li> <li>Added inline Devon URL setup form on the Devon page</li> <li>Added searchable model dropdown to Quick Test \u2014 loads from Devon's <code>manifest.json</code> with fuzzy search</li> <li>Added heartbeat-based command dispatch \u2014 agents pull queued quick tests via heartbeat response</li> <li>Added live SSE log streaming to Quick Test \u2014 real-time output with status progression</li> <li>Added Quick Test API endpoints for log forwarding and status updates</li> <li>Added Quick Test history page with status filtering and pagination</li> <li>Added Quick Test detail page with SSE live logs and stored log retrieval</li> <li>Added persistent log storage \u2014 log lines are saved to the database for post-run viewing</li> <li>Added <code>kitt-agent test list</code> and <code>kitt-agent test stop</code> CLI commands for managing tests from the agent host</li> <li>Fixed thin agent (<code>kitt-agent</code>) log forwarding and command dispatch \u2014 heartbeat now processes queued commands, <code>run_test</code>/<code>run_container</code> extract <code>test_id</code> and forward logs and status updates to the server</li> <li>Added <code>agent_name</code> query parameter to the quick test list API endpoint</li> </ul>"},{"location":"changelog/#100","title":"1.0.0","text":"<ul> <li>Initial release</li> <li>Multi-engine support: vLLM, TGI, llama.cpp, Ollama</li> <li>Quality benchmarks: MMLU, GSM8K, TruthfulQA, HellaSwag</li> <li>Performance benchmarks: throughput, latency, memory, warmup</li> <li>Hardware fingerprinting</li> <li>KARR results storage</li> <li>Web dashboard and REST API</li> <li>CI integration</li> </ul>"},{"location":"concepts/","title":"Concepts","text":"<p>This section explains the key ideas, design decisions, and internal architecture behind KITT. These pages are intended to help you understand why KITT works the way it does, not just how to use it.</p> <p>If you are looking for step-by-step instructions, see the Guides section. If you need command-line reference, see the CLI Reference.</p>"},{"location":"concepts/#topics","title":"Topics","text":""},{"location":"concepts/#architecture","title":"Architecture","text":"<p>How KITT is structured: the engine plugin system, Docker-based container management, the sibling container pattern, and the overall project layout.</p>"},{"location":"concepts/#hardware-fingerprinting","title":"Hardware Fingerprinting","text":"<p>How KITT uniquely identifies the hardware it runs on, the fingerprint format, detection methods for GPUs, CPUs, RAM, and storage, and the supported environment types.</p>"},{"location":"concepts/#karr-results-storage","title":"KARR \u2014 Results Storage","text":"<p>KARR (Kitt's AI Results Repository) is KITT's results storage system. Covers the database backend (SQLite / PostgreSQL), the hybrid data model, schema migrations, and the evolution from flat files through Git-backed storage to the current database architecture.</p>"},{"location":"concepts/#engine-lifecycle","title":"Engine Lifecycle","text":"<p>The full lifecycle of an inference engine container: image pull, container creation, health checking with exponential backoff, benchmark execution, GPU memory tracking, and cleanup.</p>"},{"location":"concepts/#benchmark-system","title":"Benchmark System","text":"<p>The benchmark plugin architecture, built-in performance and quality benchmarks, YAML-defined custom benchmarks, checkpoint recovery, and suite orchestration.</p>"},{"location":"concepts/#security","title":"Security","text":"<p>Mutual TLS for agent-server communication, automatic certificate generation, bearer token authentication, and development-mode options.</p>"},{"location":"concepts/architecture/","title":"Architecture","text":"<p>KITT is built around a plugin architecture for inference engines and benchmarks, with Docker containers as the execution environment for all engines. This page describes the major components and how they fit together.</p>"},{"location":"concepts/architecture/#high-level-overview","title":"High-Level Overview","text":"<pre><code>Host (KITT CLI)                         Docker Container\n+-------------------+                  +------------------+\n| kitt run           |   HTTP/JSON      | Engine Server    |\n|   engine.generate()| &lt;==============&gt; |   API endpoint   |\n|   GPUMemoryTracker |  localhost:PORT  |   /health        |\n+-------------------+                  |   --gpus all     |\n                                       |   /models (mount)|\n                                       +------------------+\n</code></pre> <p>KITT runs on the host (or in its own container) and manages inference engine containers via the Docker CLI. All communication between KITT and engine containers happens over HTTP on localhost, using each engine's native API.</p>"},{"location":"concepts/architecture/#engine-plugin-system","title":"Engine Plugin System","text":"<p>The engine system is built on three components:</p>"},{"location":"concepts/architecture/#inferenceengine-abc","title":"InferenceEngine ABC","text":"<p>The abstract base class in <code>engines/base.py</code> defines the contract every engine must fulfill:</p> <ul> <li><code>initialize()</code> -- Pull the Docker image, create and start the container, wait for the health check to pass.</li> <li><code>generate(prompt, **kwargs)</code> -- Send a generation request to the running engine via its HTTP API and return the result.</li> <li><code>cleanup()</code> -- Stop and remove the Docker container.</li> </ul>"},{"location":"concepts/architecture/#engineregistry","title":"EngineRegistry","text":"<p>The registry in <code>engines/registry.py</code> maintains a mapping of engine names to their implementation classes. Engines register themselves using the <code>@register_engine</code> decorator:</p> <pre><code>@register_engine(\"vllm\")\nclass VLLMEngine(InferenceEngine):\n    ...\n</code></pre>"},{"location":"concepts/architecture/#auto-discovery","title":"Auto-Discovery","text":"<p><code>EngineRegistry.auto_discover()</code> imports all built-in engine modules from the <code>engines/</code> package. This is called at startup so that all engines are available without manual imports.</p>"},{"location":"concepts/architecture/#built-in-engines","title":"Built-in Engines","text":"Engine Docker Image API Style Default Port vLLM <code>vllm/vllm-openai:latest</code> OpenAI <code>/v1/completions</code> 8000 TGI <code>ghcr.io/huggingface/text-generation-inference:latest</code> HF <code>/generate</code> 8080 llama.cpp <code>ghcr.io/ggerganov/llama.cpp:server</code> OpenAI <code>/v1/completions</code> 8081 Ollama <code>ollama/ollama:latest</code> Ollama <code>/api/generate</code> 11434"},{"location":"concepts/architecture/#docker-management","title":"Docker Management","text":""},{"location":"concepts/architecture/#no-docker-sdk","title":"No Docker SDK","text":"<p>KITT deliberately avoids the Docker Python SDK. Instead, <code>DockerManager</code> in <code>engines/docker_manager.py</code> provides static methods that call the <code>docker</code> CLI via <code>subprocess</code>. This keeps the dependency footprint small and avoids version-pinning issues with the SDK.</p>"},{"location":"concepts/architecture/#container-naming","title":"Container Naming","text":"<p>Containers are named <code>kitt-{timestamp}</code> to allow multiple concurrent runs without conflicts.</p>"},{"location":"concepts/architecture/#network-mode","title":"Network Mode","text":"<p>All engine containers use <code>--network host</code> so the engine server binds directly to localhost on the host. This avoids Docker's port-mapping overhead and simplifies connectivity.</p>"},{"location":"concepts/architecture/#gpu-access","title":"GPU Access","text":"<p>Engine containers are started with <code>--gpus all</code> to expose all host GPUs to the inference server.</p>"},{"location":"concepts/architecture/#sibling-container-pattern","title":"Sibling Container Pattern","text":"<p>KITT can itself run inside a Docker container. In this mode, the Docker socket (<code>/var/run/docker.sock</code>) is mounted into the KITT container, allowing it to create and manage engine containers as siblings rather than nested containers. This avoids Docker-in-Docker complexity.</p>"},{"location":"concepts/architecture/#remote-agent-architecture","title":"Remote Agent Architecture","text":"<p>KITT supports deploying thin agents to remote GPU servers. The agent is a standalone Python package (<code>kitt-agent</code>) served directly from the KITT web server, ensuring version compatibility.</p> <pre><code>KITT Server                              GPU Server (Agent)\n+--------------------+                  +--------------------+\n| Web UI / REST API  |  Register/HB    | kitt-agent daemon   |\n|   /api/v1/agent/*  | &lt;=============&gt; |   heartbeat thread  |\n|   agent_install.py |  Docker cmds    |   Docker orchestr.  |\n|   (serves tarball) | ==============&gt; |   log streaming     |\n+--------------------+                  +--------------------+\n</code></pre>"},{"location":"concepts/architecture/#agent-installation-flow","title":"Agent installation flow","text":"<ol> <li>User runs <code>curl -fL &lt;server&gt;/api/v1/agent/install.sh | bash</code></li> <li>The script creates a venv, downloads the agent sdist from <code>/api/v1/agent/package</code>, and installs it</li> <li><code>kitt-agent init</code> writes <code>~/.kitt/agent.yaml</code> with server URL, token, name, and port</li> <li><code>kitt-agent start</code> registers with the server, starts the heartbeat thread, and listens for commands</li> </ol>"},{"location":"concepts/architecture/#agent-command-protocol","title":"Agent command protocol","text":"<p>The server sends JSON commands to the agent's <code>/api/commands</code> endpoint:</p> Command Payload Action <code>run_container</code> image, port, volumes, env, health_url Pull image, start container, stream logs <code>run_test</code> model_path, engine_name, suite_name, benchmark_name Resolve model, run <code>kitt run</code>, stream logs <code>stop_container</code> command_id Stop a running container <code>check_docker</code> (none) Verify Docker is available <code>cleanup_storage</code> model_path (optional) Delete specific or all cached models <p>The agent reports results back to the server at <code>/api/v1/agents/{id}/results</code>.</p>"},{"location":"concepts/architecture/#model-storage-workflow","title":"Model storage workflow","text":"<p>When executing <code>run_test</code>, the agent uses <code>ModelStorageManager</code> to resolve the model:</p> <ol> <li>Check if already in local <code>model_storage_dir</code></li> <li>Mount NFS share if configured (<code>model_share_source</code> \u2192 <code>model_share_mount</code>)</li> <li>Copy model from share to local storage</li> <li>Run benchmark with local path</li> <li>Clean up local copy if <code>auto_cleanup</code> is enabled</li> </ol>"},{"location":"concepts/architecture/#standalone-agent-package","title":"Standalone agent package","text":"<p>The agent package lives in <code>agent-package/</code> at the repository root:</p> <pre><code>agent-package/\n\u251c\u2500\u2500 pyproject.toml          # Standalone package (kitt-agent)\n\u2514\u2500\u2500 src/kitt_agent/\n    \u251c\u2500\u2500 cli.py              # Click CLI: init, start, status, update, stop, test, service, preflight\n    \u251c\u2500\u2500 config.py           # Pydantic config models\n    \u251c\u2500\u2500 daemon.py           # Flask mini-app receiving commands\n    \u251c\u2500\u2500 docker_ops.py       # Docker container management\n    \u251c\u2500\u2500 hardware.py         # Hardware detection with unified memory support\n    \u251c\u2500\u2500 heartbeat.py        # Heartbeat thread with settings sync\n    \u251c\u2500\u2500 log_streamer.py     # SSE log streaming\n    \u251c\u2500\u2500 model_storage.py    # NFS mount, local copy, cleanup\n    \u251c\u2500\u2500 preflight.py        # Prerequisite checks (Docker, GPU, disk, etc.)\n    \u2514\u2500\u2500 registration.py     # Server registration\n</code></pre>"},{"location":"concepts/architecture/#project-structure","title":"Project Structure","text":"<pre><code>src/kitt/\n\u251c\u2500\u2500 cli/           # Click commands (run, engines, test, results, compare, web, fingerprint, stack, agent, monitoring)\n\u251c\u2500\u2500 engines/       # Inference engine plugins (base ABC, registry, vllm, tgi, llama_cpp, ollama)\n\u251c\u2500\u2500 benchmarks/    # Benchmark plugins (base ABC, registry, performance/*, quality/*)\n\u251c\u2500\u2500 config/        # Pydantic models + YAML loader\n\u251c\u2500\u2500 hardware/      # System fingerprinting (GPU, CPU, RAM, storage, CUDA)\n\u251c\u2500\u2500 runners/       # Suite/single test runners + checkpoint recovery\n\u251c\u2500\u2500 collectors/    # GPU memory tracking, system metrics\n\u251c\u2500\u2500 reporters/     # JSON, Markdown, comparison output\n\u251c\u2500\u2500 git_ops/       # KARR legacy Git-backed storage\n\u251c\u2500\u2500 monitoring/    # Monitoring stack config, generator, deployer\n\u251c\u2500\u2500 stack/         # Composable Docker stack config + generator\n\u251c\u2500\u2500 security/      # TLS cert generation and config\n\u251c\u2500\u2500 web/           # Flask dashboard + REST API + blueprints + Devon iframe\n\u2514\u2500\u2500 utils/         # Compression, validation, versioning\n\nagent-package/     # Standalone thin agent (installed on GPU servers)\n\u2514\u2500\u2500 src/kitt_agent/ # Self-contained agent daemon\n</code></pre>"},{"location":"concepts/architecture/#key-design-decisions","title":"Key Design Decisions","text":"<ul> <li>Dataclasses are used for result types and internal data structures.</li> <li>Pydantic v2 is used for configuration validation (YAML configs are loaded and validated through Pydantic models).</li> <li>Click is the CLI framework; Rich provides tables, panels, and spinners for terminal output.</li> <li>Logging uses <code>logging.getLogger(__name__)</code> throughout all modules.</li> <li>Full type hints are required on all public methods.</li> </ul>"},{"location":"concepts/architecture/#web-dashboard","title":"Web Dashboard","text":"<p>The web UI is a Flask application (<code>web/app.py</code>) using TailwindCSS, HTMX, and Alpine.js. It registers page blueprints (Dashboard, Agents, Devon, Models, Campaigns, Quick Test, Results, Settings) and API v1 blueprints under <code>/api/v1/</code>.</p>"},{"location":"concepts/architecture/#devon-tab","title":"Devon Tab","text":"<p>When <code>DEVON_URL</code> is set, the Devon tab embeds the Devon web UI in an iframe for integrated model management. A <code>/api/v1/devon/status</code> endpoint checks connectivity. The tab's visibility is controlled via the Settings page and persisted in the <code>web_settings</code> SQLite table.</p>"},{"location":"concepts/architecture/#settings-persistence","title":"Settings Persistence","text":"<p>UI preferences (such as Devon tab visibility) are stored in a <code>web_settings</code> key-value table managed by <code>SettingsService</code>. Settings are injected into all templates via a Flask context processor.</p>"},{"location":"concepts/architecture/#relationship-to-devon","title":"Relationship to DEVON","text":"<p>KITT tests models; DEVON manages and stores them. The two projects share the same technical stack (Poetry, Click, Rich, Python 3.10+, plugin registry pattern). The KITT web dashboard embeds Devon's UI directly when <code>DEVON_URL</code> is configured. DEVON can also export model paths in a format KITT consumes:</p> <pre><code>devon export --format kitt -o models.txt\n</code></pre>"},{"location":"concepts/architecture/#next-steps","title":"Next Steps","text":"<ul> <li>Engine Lifecycle -- detailed container lifecycle and health checks</li> <li>Benchmark System -- how benchmarks are defined and executed</li> <li>Hardware Fingerprinting -- system identification for result organization</li> </ul>"},{"location":"concepts/benchmark-system/","title":"Benchmark System","text":"<p>KITT's benchmark system is built on a plugin architecture that supports both built-in and custom benchmarks. Benchmarks are registered via decorators, can be defined in Python or YAML, and support checkpoint recovery for long-running evaluations.</p>"},{"location":"concepts/benchmark-system/#benchmark-abc","title":"Benchmark ABC","text":"<p>The <code>LLMBenchmark</code> abstract base class in <code>benchmarks/base.py</code> defines the interface every benchmark must implement:</p> <ul> <li><code>run(engine, config)</code> -- Public entry point. Handles setup, calls <code>_execute()</code>, and collects results. This method manages checkpoint loading and saving.</li> <li><code>_execute(engine, config)</code> -- The actual benchmark logic. Subclasses override this to implement their specific evaluation.</li> </ul> <p>All benchmarks receive an initialized engine instance and a configuration object, and return structured result data.</p>"},{"location":"concepts/benchmark-system/#benchmarkregistry","title":"BenchmarkRegistry","text":"<p>The <code>BenchmarkRegistry</code> in <code>benchmarks/registry.py</code> maintains the mapping from benchmark names to their implementation classes. Registration uses the <code>@register_benchmark</code> decorator:</p> <pre><code>@register_benchmark(\"throughput\")\nclass ThroughputBenchmark(LLMBenchmark):\n    def _execute(self, engine, config):\n        ...\n</code></pre> <p>At startup, all built-in benchmarks are auto-discovered and registered, similar to the engine plugin system.</p>"},{"location":"concepts/benchmark-system/#built-in-benchmarks","title":"Built-in Benchmarks","text":""},{"location":"concepts/benchmark-system/#performance-benchmarks","title":"Performance Benchmarks","text":"<p>Performance benchmarks measure inference engine speed and resource usage. They do not evaluate output quality.</p> Benchmark What It Measures <code>throughput</code> Tokens per second at various batch sizes and sequence lengths <code>latency</code> Time-to-first-token (TTFT) and inter-token latency (ITL) <code>memory</code> Peak GPU memory usage under different loads <code>warmup_analysis</code> Performance difference between cold-start and warmed-up inference"},{"location":"concepts/benchmark-system/#quality-benchmarks","title":"Quality Benchmarks","text":"<p>Quality benchmarks evaluate the correctness and consistency of model outputs. These use standard academic evaluation datasets.</p> Benchmark Dataset What It Measures <code>mmlu</code> MMLU Multitask language understanding across 57 subjects <code>gsm8k</code> GSM8K Grade-school math reasoning with chain-of-thought <code>truthfulqa</code> TruthfulQA Resistance to generating common falsehoods <code>hellaswag</code> HellaSwag Common-sense sentence completion"},{"location":"concepts/benchmark-system/#yaml-defined-benchmarks","title":"YAML-Defined Benchmarks","text":"<p>Custom benchmarks can be defined in YAML without writing Python code. The <code>YAMLBenchmark</code> class in <code>benchmarks/loader.py</code> loads YAML files and creates benchmark instances at runtime.</p> <pre><code>name: custom_throughput\ncategory: performance\nbase: throughput\nconfig:\n  batch_sizes: [1, 4, 8]\n  sequence_lengths: [128, 512]\n  num_iterations: 5\n</code></pre> <p>YAML benchmarks reference a <code>base</code> benchmark and override specific configuration values. This makes it easy to create variants of built-in benchmarks tuned for specific hardware or use cases.</p> <p>Create a new custom benchmark with:</p> <pre><code>kitt test new my-benchmark\n</code></pre>"},{"location":"concepts/benchmark-system/#checkpoint-recovery","title":"Checkpoint Recovery","text":"<p>Long-running benchmarks (especially quality evaluations that process thousands of dataset items) save checkpoints periodically so that interrupted runs can be resumed.</p> <p>The <code>CheckpointManager</code> handles checkpoint persistence:</p> <ul> <li>Save interval -- Every 100 items processed.</li> <li>Checkpoint contents -- Completed items, partial results, progress index, configuration snapshot.</li> <li>Recovery -- On restart, if a matching checkpoint exists, the benchmark resumes from where it left off rather than starting over.</li> </ul> <p>Checkpoints are stored locally and are identified by a combination of model, engine, and benchmark name.</p>"},{"location":"concepts/benchmark-system/#test-suites","title":"Test Suites","text":"<p>Suites group multiple benchmarks into a single run. KITT ships with three predefined suites:</p> Suite Purpose Benchmarks Runs per Benchmark <code>quick</code> Smoke test throughput only 1 <code>standard</code> Full evaluation all quality + all performance 3 <code>performance</code> Performance-focused throughput, latency, memory, warmup 3"},{"location":"concepts/benchmark-system/#suite-configuration","title":"Suite Configuration","text":"<p>Suites are defined in YAML configuration files. A suite config specifies which benchmarks to run, global settings, and optional per-test overrides:</p> <pre><code>name: standard\nruns: 3\nglobal_config:\n  max_tokens: 256\n  temperature: 0.0\nbenchmarks:\n  - throughput\n  - latency\n  - memory\n  - warmup_analysis\n  - mmlu\n  - gsm8k\n  - truthfulqa\n  - hellaswag\ntest_overrides:\n  mmlu:\n    max_tokens: 64\n  gsm8k:\n    max_tokens: 512\n</code></pre> <p>The <code>global_config</code> applies to all benchmarks unless overridden. The <code>test_overrides</code> section allows per-test configuration within the suite.</p>"},{"location":"concepts/benchmark-system/#suiterunner","title":"SuiteRunner","text":"<p>The <code>SuiteRunner</code> in <code>runners/</code> orchestrates suite execution:</p> <ol> <li>Load and validate the suite configuration.</li> <li>For each benchmark in the suite, for each run iteration:<ul> <li>Initialize the engine (if not already running).</li> <li>Execute the benchmark via <code>run()</code>.</li> <li>Collect results and GPU memory data.</li> </ul> </li> <li>Aggregate results across runs (mean, stddev, min, max).</li> <li>Generate reports (JSON, Markdown).</li> <li>Persist results through KARR.</li> </ol>"},{"location":"concepts/benchmark-system/#next-steps","title":"Next Steps","text":"<ul> <li>Engine Lifecycle -- how engines are started before benchmarks run</li> <li>KARR \u2014 Results Storage -- how benchmark results are stored</li> </ul>"},{"location":"concepts/engine-lifecycle/","title":"Engine Lifecycle","text":"<p>Every inference engine in KITT follows the same container lifecycle: pull the image, create and start the container, wait for health checks, execute benchmarks, and clean up. This page describes each stage in detail.</p>"},{"location":"concepts/engine-lifecycle/#lifecycle-stages","title":"Lifecycle Stages","text":""},{"location":"concepts/engine-lifecycle/#1-image-pull-check","title":"1. Image Pull / Check","text":"<p>Before starting an engine, KITT checks whether the required Docker image is available locally. If not, it pulls the image from the configured registry.</p> <pre><code># Manual image setup\nkitt engines setup vllm\nkitt engines setup vllm --dry-run  # show what would be pulled\n</code></pre> <p>The <code>kitt engines check</code> command verifies that an engine's image is available without starting a container.</p>"},{"location":"concepts/engine-lifecycle/#2-container-creation","title":"2. Container Creation","text":"<p>KITT creates a Docker container with the following standard flags:</p> Flag Purpose <code>--network host</code> Bind engine server directly to localhost (no port mapping overhead) <code>--gpus all</code> Expose all host GPUs to the container <code>--name kitt-{timestamp}</code> Unique container name to avoid conflicts with concurrent runs <code>-v /path/to/models:/models</code> Mount the model directory into the container <p>The <code>DockerManager</code> constructs and executes the <code>docker run</code> command via <code>subprocess</code>. Each engine implementation adds its own engine-specific flags (API ports, model paths, quantization settings, etc.).</p>"},{"location":"concepts/engine-lifecycle/#3-health-check","title":"3. Health Check","text":"<p>After the container starts, KITT waits for the engine's health endpoint to respond successfully. This uses exponential backoff to avoid hammering a server that is still loading a model.</p> <p>Health check parameters:</p> Parameter Value Timeout 300 seconds Initial interval 1 second Maximum interval 10 seconds Backoff strategy Exponential (interval doubles each attempt, capped at max) <p>The health check makes HTTP GET requests to the engine's health endpoint. A <code>200 OK</code> response indicates the engine is ready to accept generation requests. If the timeout expires before a successful response, KITT stops the container and raises an error.</p>"},{"location":"concepts/engine-lifecycle/#4-health-endpoints-by-engine","title":"4. Health Endpoints by Engine","text":"<p>Each engine exposes a different health endpoint:</p> Engine Health Endpoint Default Port Success Indicator vLLM <code>/health</code> 8000 200 OK TGI <code>/info</code> 8080 200 OK with model info Ollama <code>/api/tags</code> 11434 200 OK with model list llama.cpp <code>/health</code> 8081 200 OK"},{"location":"concepts/engine-lifecycle/#5-benchmark-execution","title":"5. Benchmark Execution","text":"<p>Once the health check passes, KITT sends benchmark requests to the engine via its HTTP API on <code>localhost:PORT</code>. The engine's <code>generate()</code> method handles request formatting, sending, and response parsing for each engine's specific API.</p> <p>During execution, KITT tracks GPU memory usage through the <code>GPUMemoryTracker</code> context manager (see below).</p>"},{"location":"concepts/engine-lifecycle/#6-gpu-memory-tracking","title":"6. GPU Memory Tracking","text":"<p>The <code>GPUMemoryTracker</code> in <code>collectors/</code> is a context manager that monitors GPU memory utilization during benchmark execution:</p> <pre><code>with GPUMemoryTracker() as tracker:\n    results = engine.generate(prompt)\n# tracker.peak_memory_mb, tracker.samples, etc.\n</code></pre> <p>It periodically samples GPU memory via pynvml and records:</p> <ul> <li>Peak memory usage (MB)</li> <li>Average memory usage (MB)</li> <li>Memory samples over time</li> <li>VRAM utilization percentage</li> </ul> <p>This data is included in the benchmark results and persisted through KARR.</p>"},{"location":"concepts/engine-lifecycle/#7-container-stop-and-removal","title":"7. Container Stop and Removal","text":"<p>After all benchmarks complete (or if an error occurs), KITT stops and removes the engine container. The <code>cleanup()</code> method on each engine calls <code>DockerManager</code> to:</p> <ol> <li>Stop the container (<code>docker stop</code>)</li> <li>Remove the container (<code>docker rm</code>)</li> </ol> <p>This ensures no orphaned containers are left running. If KITT is interrupted (e.g., Ctrl+C), a signal handler attempts cleanup before exiting.</p>"},{"location":"concepts/engine-lifecycle/#network-and-port-binding","title":"Network and Port Binding","text":"<p>All engines use <code>--network host</code>, which means the engine server binds directly to the host's network interfaces. This has several implications:</p> <ul> <li>No port mapping -- The engine listens on <code>localhost:PORT</code> directly, not through Docker's port forwarding.</li> <li>Lower latency -- Eliminates the overhead of Docker's userland proxy.</li> <li>Port conflicts -- Only one engine can use a given port at a time. KITT handles this by running engines sequentially within a suite.</li> </ul> <p>When KITT itself runs in a Docker container (the sibling container pattern), <code>--network host</code> ensures both the KITT container and engine containers share the host network namespace, so <code>localhost</code> communication works as expected.</p>"},{"location":"concepts/engine-lifecycle/#sequence-diagram","title":"Sequence Diagram","text":"<pre><code>KITT CLI          DockerManager         Engine Container\n   |                   |                      |\n   |  docker pull      |                      |\n   |------------------&gt;|                      |\n   |                   |  docker run          |\n   |                   |---------------------&gt;|\n   |                   |                      | (loading model)\n   |  GET /health      |                      |\n   |-----------------------------------------&gt;|\n   |  503              |                      |\n   |&lt;-----------------------------------------|\n   |  (backoff)        |                      |\n   |  GET /health      |                      |\n   |-----------------------------------------&gt;|\n   |  200 OK           |                      |\n   |&lt;-----------------------------------------|\n   |  POST /generate   |                      |\n   |-----------------------------------------&gt;|\n   |  200 + response   |                      |\n   |&lt;-----------------------------------------|\n   |                   |  docker stop/rm      |\n   |                   |---------------------&gt;|\n   |                   |                      X\n</code></pre>"},{"location":"concepts/engine-lifecycle/#next-steps","title":"Next Steps","text":"<ul> <li>Architecture -- the engine plugin system and Docker management overview</li> <li>Benchmark System -- what runs during stage 5</li> </ul>"},{"location":"concepts/hardware-fingerprinting/","title":"Hardware Fingerprinting","text":"<p>Hardware fingerprinting gives KITT a way to uniquely identify the system it runs on. This identity is used to organize benchmark results so that runs from different machines are never mixed together, and results from the same hardware can be compared over time.</p>"},{"location":"concepts/hardware-fingerprinting/#purpose","title":"Purpose","text":"<p>Benchmark results are only meaningful when compared against runs from the same hardware configuration. A throughput measurement on an RTX 4090 cannot be directly compared with one from an A100. Hardware fingerprinting solves this by generating a compact, deterministic string that captures the key hardware characteristics of the system.</p> <p>This fingerprint is embedded in every result stored by KARR, tagging each run with the exact hardware that produced it.</p>"},{"location":"concepts/hardware-fingerprinting/#fingerprint-format","title":"Fingerprint Format","text":"<p>The fingerprint is a human-readable string that encodes GPU, CPU, RAM, storage, CUDA, driver, and OS information:</p> <pre><code>{gpu}-{vram}_{cpu}-{cores}_{ram}-{type}_{storage}_{cuda}_{driver}_{os}\n</code></pre> <p>Example:</p> <pre><code>rtx4090-24gb_i9-13900k-24c_64gb-ddr5_samsung-990pro-nvme_cuda-12.4_550.90_linux-6.8\n</code></pre> <p>The format is designed to be both machine-parseable and readable at a glance. Each segment is separated by underscores, with dashes used within segments.</p>"},{"location":"concepts/hardware-fingerprinting/#api","title":"API","text":""},{"location":"concepts/hardware-fingerprinting/#hardwarefingerprintgenerate","title":"<code>HardwareFingerprint.generate()</code>","text":"<p>Returns the compact fingerprint string. This is the primary entry point used by KARR and the CLI to identify hardware.</p> <pre><code>kitt fingerprint\n# rtx4090-24gb_i9-13900k-24c_64gb-ddr5_samsung-990pro-nvme_cuda-12.4_550.90_linux-6.8\n\nkitt fingerprint --verbose\n# Displays full SystemInfo details in a Rich table\n</code></pre>"},{"location":"concepts/hardware-fingerprinting/#hardwarefingerprintdetect_system","title":"<code>HardwareFingerprint.detect_system()</code>","text":"<p>Returns a <code>SystemInfo</code> dataclass containing all detected hardware attributes in structured form. This is used internally by <code>generate()</code> and is also available for code that needs individual fields.</p>"},{"location":"concepts/hardware-fingerprinting/#detection-methods","title":"Detection Methods","text":"<p>KITT detects hardware through a combination of Python libraries and CLI fallbacks:</p> Component Primary Method Fallback GPU pynvml (nvidia-ml-py) <code>nvidia-smi</code> CLI output parsing CPU py-cpuinfo <code>/proc/cpuinfo</code> on Linux RAM psutil None Storage Device type detection Assumes SSD if unknown CUDA pynvml <code>nvidia-smi</code> CLI Driver pynvml <code>nvidia-smi</code> CLI <p>The dual-path approach for GPU detection ensures fingerprinting works even when pynvml is not installed, as long as the NVIDIA driver and <code>nvidia-smi</code> are available on the system.</p>"},{"location":"concepts/hardware-fingerprinting/#environment-types","title":"Environment Types","text":"<p>KITT detects the runtime environment and includes it in the system information. This helps distinguish between otherwise identical hardware that may behave differently depending on virtualization or containerization.</p> Environment Detection Criteria <code>dgx_spark</code> NVIDIA DGX Spark system <code>dgx</code> NVIDIA DGX system <code>wsl2</code> Windows Subsystem for Linux 2 (kernel string contains <code>microsoft</code> or <code>WSL</code>) <code>docker</code> Running inside Docker (<code>.dockerenv</code> exists or <code>docker</code> in cgroup) <code>container</code> Running in a non-Docker container (generic container detection) <code>native_linux</code> Bare-metal or VM Linux <code>native_macos</code> macOS system <code>native_windows</code> Native Windows (not WSL) <p>Environment type detection is ordered from most specific to least specific. DGX systems are checked first, then containerized environments, and finally native OS detection.</p>"},{"location":"concepts/hardware-fingerprinting/#usage-in-karr","title":"Usage in KARR","text":"<p>In KARR's current database backend, the full fingerprint is stored in the <code>hardware.fingerprint</code> column for every run, enabling queries like \"show all results from this machine.\" The fingerprint is also included in flat-file JSON output.</p> <p>In KARR's legacy Git-backed storage (Gen 2), the fingerprint was truncated to 40 characters for directory naming:</p> <pre><code>karr-rtx4090-24gb_i9-13900k-24c_64gb-ddr/\n  \u2514\u2500\u2500 meta-llama--Llama-3.1-8B/\n      \u2514\u2500\u2500 vllm/\n          \u2514\u2500\u2500 20250115-143022/\n              \u251c\u2500\u2500 metrics.json\n              \u2514\u2500\u2500 ...\n</code></pre>"},{"location":"concepts/hardware-fingerprinting/#next-steps","title":"Next Steps","text":"<ul> <li>KARR \u2014 Results Storage -- how fingerprints are used in result storage</li> <li>Architecture -- overall system design</li> </ul>"},{"location":"concepts/karr/","title":"KARR \u2014 Results Storage","text":"<p>KARR (Kitt's AI Results Repository) is KITT's results storage system. Every benchmark run, hardware snapshot, and metric is persisted through KARR, giving you a queryable history of all testing activity.</p> <p>KARR has evolved alongside KITT through multiple generations:</p> Generation Backend Status Gen 1 Flat JSON files Still written for convenience Gen 2 Git repository with LFS Legacy, available via <code>--store-karr</code> Gen 3 Relational database (SQLite / PostgreSQL) Current default <p>The underlying storage mechanism has changed, but the purpose has not: KARR is where your results live.</p>"},{"location":"concepts/karr/#current-backend-database","title":"Current Backend \u2014 Database","text":"<p>The current generation of KARR uses a relational database accessed through the abstract <code>ResultStore</code> interface. This makes the backend pluggable while keeping the API consistent.</p> <pre><code>ResultStore (abstract interface)\n\u251c\u2500\u2500 SQLiteResultStore   -- default, ~/.kitt/kitt.db\n\u2514\u2500\u2500 PostgresResultStore -- optional, requires psycopg2\n</code></pre> <p>The <code>ResultStore</code> interface exposes a small, consistent API:</p> Method Purpose <code>save_result()</code> Persist a completed benchmark run <code>get_result()</code> Retrieve a single run by ID <code>query()</code> Filter runs by model, engine, date range, etc. <code>list_results()</code> List runs with optional filters and pagination <code>aggregate()</code> Group-by aggregation (e.g., average throughput per engine) <code>delete_result()</code> Remove a run and its related rows (CASCADE) <code>count()</code> Return the total number of stored runs"},{"location":"concepts/karr/#sqlite-default","title":"SQLite (Default)","text":"<p>SQLite is the default backend. No configuration is required -- KITT creates <code>~/.kitt/kitt.db</code> on first use.</p> <p>Key characteristics:</p> <ul> <li>WAL mode -- concurrent readers do not block writers, so the web dashboard can query while a benchmark is running.</li> <li>Foreign keys with CASCADE DELETE -- deleting a run automatically removes its benchmarks, metrics, and hardware rows.</li> <li>Indexes on common query columns -- model, engine, suite name, and timestamp are indexed for fast filtering.</li> </ul>"},{"location":"concepts/karr/#postgresql-production-distributed","title":"PostgreSQL (Production / Distributed)","text":"<p>For multi-agent or web-scale deployments, KARR supports PostgreSQL. Install the extra dependency and provide a DSN connection string:</p> <pre><code>poetry install -E postgres        # installs psycopg2\nexport KITT_DB_DSN=\"postgresql://user:pass@db-host:5432/kitt\"\nkitt storage init                  # creates tables in the target database\n</code></pre> <p>PostgreSQL uses native types where SQLite uses text approximations -- <code>TIMESTAMPTZ</code> instead of <code>TEXT</code> for timestamps, <code>JSONB</code> instead of <code>TEXT</code> for raw JSON, <code>BOOLEAN</code> instead of <code>INTEGER</code>, and <code>DOUBLE PRECISION</code> instead of <code>REAL</code>.</p>"},{"location":"concepts/karr/#hybrid-data-model","title":"Hybrid Data Model","text":"<p>Every run is stored in two complementary forms:</p> <ol> <li>Normalized tables -- <code>runs</code>, <code>benchmarks</code>, <code>metrics</code>, and <code>hardware</code> break each run into queryable columns with proper types and foreign-key relationships. This powers filtered listing, cross-run comparison, and group-by aggregation.</li> <li><code>runs.raw_json</code> -- the full JSON output produced by KITT is stored verbatim in a single column. This guarantees lossless round-tripping: export always returns exactly what was originally recorded, even if the schema evolves.</li> </ol>"},{"location":"concepts/karr/#schema-versioning-migrations","title":"Schema Versioning &amp; Migrations","text":"<p>The database tracks its schema version in the <code>schema_version</code> table. KITT ships with an ordered set of migration scripts and applies any that are newer than the recorded version:</p> <pre><code>kitt storage migrate              # apply pending migrations\n</code></pre> <p>The current schema version is 2. Migrations are forward-only; downgrades are not supported.</p>"},{"location":"concepts/karr/#cli-quick-reference","title":"CLI Quick Reference","text":"Command Description <code>kitt storage init</code> Create tables (SQLite file or PostgreSQL schema) <code>kitt storage migrate</code> Apply pending schema migrations <code>kitt storage import</code> Import JSON result files into the database <code>kitt storage export</code> Export runs from the database as JSON <code>kitt storage list</code> List stored runs with optional filters <code>kitt storage stats</code> Show summary statistics (run count, models, engines)"},{"location":"concepts/karr/#flat-file-output-gen-1","title":"Flat File Output (Gen 1)","text":"<p>When you run <code>kitt run</code>, JSON result files are still written to the <code>kitt-results/</code> directory in the current working directory. These files are useful for quick inspection, piping into <code>jq</code>, or archiving. The database is the primary storage mechanism and source of truth, but flat files remain as a convenience layer.</p>"},{"location":"concepts/karr/#git-backed-storage-gen-2","title":"Git-Backed Storage (Gen 2)","text":"<p>Legacy</p> <p>Git-backed KARR storage is the previous generation. It remains functional and may suit single-machine dev/test workflows, but the database backend (Gen 3) is recommended for all new deployments.</p> <p>The second generation of KARR stored results in a Git repository with LFS tracking. You can still enable it with <code>--store-karr</code>.</p>"},{"location":"concepts/karr/#directory-structure-gen-2","title":"Directory Structure (Gen 2)","text":"<pre><code>karr-{fingerprint[:40]}/\n  {model}/\n    {engine}/\n      {timestamp}/\n        metrics.json\n        summary.md\n        hardware.json\n        config.json\n        outputs/\n          *.jsonl.gz\n</code></pre> <p>Large output files in <code>outputs/</code> were compressed in 50 MB chunks and tracked by Git LFS. For Docker-based production deployments this approach introduces unnecessary complexity -- mounting Git repos, configuring LFS, and managing repository growth -- that the database backend eliminates entirely.</p>"},{"location":"concepts/karr/#next-steps","title":"Next Steps","text":"<ul> <li>Database Schema Reference -- full table and column documentation</li> <li>Results Guide -- practical workflows for storing, querying, and comparing results</li> <li>Hardware Fingerprinting -- how system identity is captured alongside results</li> </ul>"},{"location":"concepts/security/","title":"Security","text":"<p>KITT includes a security layer for its distributed agent architecture and web API. This covers mutual TLS (mTLS) for agent-server communication, automatic certificate generation, and bearer token authentication for the REST API.</p>"},{"location":"concepts/security/#mutual-tls-mtls","title":"Mutual TLS (mTLS)","text":"<p>When KITT agents communicate with the KITT web server, both sides authenticate each other using TLS certificates. This prevents unauthorized agents from connecting and ensures the agent is talking to the legitimate server.</p>"},{"location":"concepts/security/#certificate-chain","title":"Certificate Chain","text":"<p>KITT uses a three-tier certificate chain:</p> <ol> <li>CA certificate -- A self-signed Certificate Authority generated by KITT. This is the root of trust.</li> <li>Server certificate -- Issued by the CA, used by the KITT web server to prove its identity to agents.</li> <li>Client certificate -- Issued by the CA, used by each agent to prove its identity to the server.</li> </ol> <p>Both the server and client certificates are signed by the same CA, so each side can verify the other.</p>"},{"location":"concepts/security/#automatic-certificate-generation","title":"Automatic Certificate Generation","text":"<p>The <code>cert_manager.py</code> module in <code>security/</code> generates all certificates automatically when the KITT stack is initialized. No manual OpenSSL commands or external CA infrastructure is required.</p> <pre><code># Certificates are generated when starting a stack with the agent component\nkitt stack generate my-stack --web --agent\nkitt stack start --name my-stack\n</code></pre> <p>Certificates are stored in the stack's configuration directory and are mounted into the relevant containers.</p>"},{"location":"concepts/security/#ca-fingerprint","title":"CA Fingerprint","text":"<p>The CA certificate's SHA-256 fingerprint is displayed when the stack starts. Agents can use this fingerprint to verify they are connecting to the correct server, providing an out-of-band verification mechanism.</p>"},{"location":"concepts/security/#bearer-token-authentication","title":"Bearer Token Authentication","text":"<p>The KITT REST API supports bearer token authentication for programmatic access. This is simpler than mTLS and is intended for direct API usage (scripts, CI pipelines, dashboards).</p>"},{"location":"concepts/security/#configuration","title":"Configuration","text":"<p>Set an authentication token when starting the web server or agent:</p> <pre><code># Web server with auth token\nkitt web --auth-token my-secret-token\n\n# Agent with auth token\nkitt agent start --auth-token my-secret-token\n</code></pre> <p>API requests must include the token in the <code>Authorization</code> header:</p> <pre><code>Authorization: Bearer my-secret-token\n</code></pre> <p>Requests without a valid token receive a <code>401 Unauthorized</code> response.</p> <p>Note: The agent installation endpoints (<code>/api/v1/agent/install.sh</code> and <code>/api/v1/agent/package</code>) do not require authentication, allowing agents to be bootstrapped from within the network without a token. Authentication is enforced once the agent registers with the server.</p>"},{"location":"concepts/security/#development-mode","title":"Development Mode","text":"<p>For local development and testing, security can be relaxed with the <code>--insecure</code> flag:</p> <pre><code>kitt web --insecure\nkitt agent start --insecure\n</code></pre> <p>This disables TLS certificate verification and token authentication. The <code>--insecure</code> flag should never be used in production or on untrusted networks.</p>"},{"location":"concepts/security/#cryptography-dependency","title":"Cryptography Dependency","text":"<p>The TLS certificate generation requires the <code>cryptography</code> Python package, which is an optional dependency. Install it via the <code>web</code> extra:</p> <pre><code>poetry install -E web\n</code></pre> <p>This extra also installs Flask and other web-related dependencies. The <code>cryptography</code> package is not needed for core KITT functionality (running benchmarks, fingerprinting, results management).</p>"},{"location":"concepts/security/#summary","title":"Summary","text":"Feature Use Case Configuration mTLS Agent-server communication Automatic via <code>cert_manager.py</code> Bearer token REST API access <code>--auth-token</code> flag Insecure mode Local development <code>--insecure</code> flag"},{"location":"concepts/security/#next-steps","title":"Next Steps","text":"<ul> <li>Architecture -- overall system design including the agent and web components</li> <li>Engine Lifecycle -- how engines run within the secured infrastructure</li> </ul>"},{"location":"getting-started/","title":"Getting Started","text":"<p>Everything you need to install KITT, run your first benchmark, and deploy with Docker.</p>"},{"location":"getting-started/#whats-in-this-section","title":"What's in This Section","text":""},{"location":"getting-started/#installation","title":"Installation","text":"<p>Install KITT using Docker (recommended) or from source with Poetry. Covers prerequisites, optional extras, and GPU setup.</p>"},{"location":"getting-started/#tutorial-first-benchmark","title":"Tutorial: First Benchmark","text":"<p>A step-by-step walkthrough from hardware fingerprinting through benchmark execution to storing results in KARR.</p>"},{"location":"getting-started/#tutorial-docker-quickstart","title":"Tutorial: Docker Quickstart","text":"<p>Run KITT entirely from a container -- build the image, mount models and results, use Docker Compose, and generate production deployment stacks.</p>"},{"location":"getting-started/#prerequisites-at-a-glance","title":"Prerequisites at a Glance","text":"Requirement Why Docker All inference engines run as containers NVIDIA GPU + drivers Required for GPU-accelerated inference NVIDIA Container Toolkit Lets Docker containers access the GPU A model on disk GGUF, safetensors, or PyTorch format depending on engine <p>Tip</p> <p>The fastest path to your first result is the Docker method described in the Installation guide -- no Python environment needed on the host.</p>"},{"location":"getting-started/docker-quickstart/","title":"Tutorial: Docker Quickstart","text":"<p>This tutorial covers running KITT entirely from Docker -- no Python environment on the host required. It progresses from single commands through Docker Compose to full production stacks.</p> <p>Prerequisites</p> <p>Docker and the NVIDIA Container Toolkit must be installed. See the Installation guide for details.</p>"},{"location":"getting-started/docker-quickstart/#build-the-kitt-image","title":"Build the KITT Image","text":"<pre><code>docker build -t kitt .\n</code></pre> <p>The Dockerfile installs KITT with all optional extras so every feature is available inside the container.</p>"},{"location":"getting-started/docker-quickstart/#run-a-benchmark-from-the-container","title":"Run a Benchmark from the Container","text":"<p>KITT needs three mounts to work from inside a container:</p> <pre><code>docker run --rm --network host \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  -v /path/to/models:/models:ro \\\n  -v ./kitt-results:/app/kitt-results \\\n  kitt run -m /models/llama-7b -e vllm -s quick\n</code></pre> Mount Purpose <code>/var/run/docker.sock</code> KITT manages engine containers via the Docker socket <code>/path/to/models</code> (read-only) Model weights shared between KITT and the engine <code>./kitt-results</code> Results written back to the host <p>Warning</p> <p>The Docker socket mount gives the KITT container full Docker access on the host. Use this only in trusted environments.</p> <p>You can run any KITT command this way -- not just <code>kitt run</code>:</p> <pre><code>docker run --rm kitt fingerprint --verbose\ndocker run --rm kitt test list\ndocker run --rm -v /var/run/docker.sock:/var/run/docker.sock kitt engines list\n</code></pre>"},{"location":"getting-started/docker-quickstart/#docker-compose","title":"Docker Compose","text":"<p>The repository includes a <code>docker-compose.yaml</code> for convenience. Set the <code>MODEL_PATH</code> environment variable and use <code>docker compose run</code>:</p> <pre><code>MODEL_PATH=/path/to/models docker compose run kitt run -m /models/llama-7b -e vllm\n</code></pre> <p>To run the standard suite across multiple engines:</p> <pre><code>MODEL_PATH=/path/to/models docker compose run kitt run -m /models/llama-7b -e vllm -s standard\nMODEL_PATH=/path/to/models docker compose run kitt run -m /models/llama-7b -e tgi -s standard\n</code></pre> <p>Tip</p> <p>Export <code>MODEL_PATH</code> in your shell profile so you don't have to set it on every invocation: <code>export MODEL_PATH=/path/to/models</code>.</p>"},{"location":"getting-started/docker-quickstart/#production-stacks-with-kitt-stack","title":"Production Stacks with <code>kitt stack</code>","text":"<p>For production deployments, KITT generates composable Docker Compose stacks that bundle the web dashboard, database, monitoring, and agent components.</p>"},{"location":"getting-started/docker-quickstart/#generate-a-stack","title":"Generate a Stack","text":"<pre><code>kitt stack generate prod --web --postgres --monitoring\n</code></pre> <p>This creates a <code>docker-compose.yaml</code> under <code>~/.kitt/stacks/prod/</code> with:</p> <ul> <li>Web dashboard -- Flask UI and REST API</li> <li>PostgreSQL -- persistent result storage</li> <li>Prometheus + Grafana -- metrics collection and dashboards</li> </ul> <p>Add <code>--agent</code> to include the distributed agent daemon for remote execution:</p> <pre><code>kitt stack generate prod --web --postgres --monitoring --agent\n</code></pre>"},{"location":"getting-started/docker-quickstart/#manage-stack-lifecycle","title":"Manage Stack Lifecycle","text":"<p>Start, check, and stop stacks by name:</p> <pre><code>kitt stack start --name prod\nkitt stack status --name prod\nkitt stack stop --name prod\n</code></pre>"},{"location":"getting-started/docker-quickstart/#list-and-remove-stacks","title":"List and Remove Stacks","text":"<pre><code>kitt stack list\nkitt stack remove prod --delete-files\n</code></pre> <p>Note</p> <p>The <code>--web</code> and <code>--reporting</code> flags are mutually exclusive. Use <code>--web</code> for the full dashboard or <code>--reporting</code> for headless report generation.</p>"},{"location":"getting-started/docker-quickstart/#example-full-production-deployment","title":"Example: Full Production Deployment","text":"<p>A complete workflow from image build to running benchmarks through a production stack:</p> <pre><code># Build the KITT image\ndocker build -t kitt .\n\n# Generate and start a stack with all components\nkitt stack generate prod --web --postgres --monitoring --agent\nkitt stack start --name prod\n\n# Run benchmarks (results flow to PostgreSQL and Grafana automatically)\ndocker run --rm --network host \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  -v /path/to/models:/models:ro \\\n  kitt run -m /models/llama-7b -e vllm -s standard\n\n# Check the web dashboard at http://localhost:5000\n# Check Grafana dashboards at http://localhost:3000\n\n# When finished\nkitt stack stop --name prod\n</code></pre>"},{"location":"getting-started/docker-quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Learn about engine configuration: Engines Guide</li> <li>Set up monitoring dashboards: Monitoring Guide</li> <li>Explore the REST API: API Reference</li> </ul>"},{"location":"getting-started/first-benchmark/","title":"Tutorial: First Benchmark","text":"<p>This tutorial walks through a complete benchmark run -- from checking your hardware to storing results in KARR.</p> <p>Prerequisites</p> <p>KITT must be installed and your GPU must be accessible to Docker. See the Installation guide if you haven't set that up yet.</p>"},{"location":"getting-started/first-benchmark/#1-check-your-hardware-fingerprint","title":"1. Check Your Hardware Fingerprint","text":"<p>KITT identifies each machine by a compact fingerprint string. Run:</p> <pre><code>kitt fingerprint\n</code></pre> <p>Example output:</p> <pre><code>rtx4090-24gb_i9-13900k-24c_64gb-ddr5_samsung-990pro-nvme_cuda-12.4_550.90_linux-6.8\n</code></pre> <p>Use <code>--verbose</code> for a full breakdown of every detected component:</p> <pre><code>kitt fingerprint --verbose\n</code></pre> <p>Tip</p> <p>The fingerprint is embedded in every result set so you can always trace which hardware produced a given benchmark.</p>"},{"location":"getting-started/first-benchmark/#2-pull-the-engine-image","title":"2. Pull the Engine Image","text":"<p>Before running benchmarks, make sure the engine's Docker image is available locally. KITT can pull it for you:</p> <pre><code>kitt engines setup vllm\n</code></pre> <p>This downloads the default vLLM image (<code>vllm/vllm-openai:latest</code>). The first pull may take several minutes depending on your connection.</p>"},{"location":"getting-started/first-benchmark/#3-verify-engine-readiness","title":"3. Verify Engine Readiness","text":"<p>List all registered engines and their status:</p> <pre><code>kitt engines list\n</code></pre> <p>You should see vLLM listed with its image marked as available. If the image column shows \"missing\", re-run <code>kitt engines setup vllm</code>.</p>"},{"location":"getting-started/first-benchmark/#4-list-available-benchmarks","title":"4. List Available Benchmarks","text":"<p>See what benchmarks KITT ships with:</p> <pre><code>kitt test list\n</code></pre> <p>Output groups benchmarks by category:</p> Category Benchmarks Performance throughput, latency, memory, warmup_analysis Quality mmlu, gsm8k, truthfulqa, hellaswag <p>Note</p> <p>Quality benchmarks require the <code>datasets</code> extra. Install it with <code>poetry install -E datasets</code> if you haven't already.</p>"},{"location":"getting-started/first-benchmark/#5-run-a-quick-benchmark","title":"5. Run a Quick Benchmark","text":"<p>The <code>quick</code> suite runs a single throughput benchmark -- ideal for verifying that everything works before committing to a full evaluation.</p> <pre><code>kitt run -m /path/to/model -e vllm -s quick\n</code></pre> <p>KITT will:</p> <ol> <li>Start a vLLM container with your model</li> <li>Wait for the health check to pass</li> <li>Execute the throughput benchmark</li> <li>Tear down the container</li> <li>Write results to <code>kitt-results/</code> and store them in KARR</li> </ol> <p>Warning</p> <p>Make sure the model format matches the engine. vLLM and TGI accept safetensors/pytorch; llama.cpp and Ollama require GGUF.</p>"},{"location":"getting-started/first-benchmark/#6-view-the-results","title":"6. View the Results","text":"<p>Each run produces a timestamped directory under <code>kitt-results/</code> containing:</p> File Contents <code>metrics.json</code> Raw benchmark measurements (tokens/sec, latencies, memory) <code>hardware.json</code> System fingerprint captured at run time <code>config.json</code> Exact configuration used for the run <code>summary.md</code> Human-readable Markdown report <p>Open the summary for a quick overview:</p> <pre><code>cat kitt-results/&lt;model&gt;/&lt;engine&gt;/&lt;timestamp&gt;/summary.md\n</code></pre> <p>Or view results as a Rich table in the terminal:</p> <pre><code>kitt results list --model llama-7b --engine vllm\n</code></pre>"},{"location":"getting-started/first-benchmark/#7-browse-results-in-karr","title":"7. Browse Results in KARR","text":"<p>Results are stored in KARR automatically. Initialize the database if this is your first run:</p> <pre><code>kitt storage init\n</code></pre> <p>Then browse and query your stored results:</p> <pre><code>kitt storage list\nkitt storage stats\n</code></pre> <p>You can also import any flat-file results from previous runs:</p> <pre><code>kitt storage import ./kitt-results/\n</code></pre> <p>Tip</p> <p>KARR uses SQLite by default (<code>~/.kitt/kitt.db</code>) with zero configuration. For production or multi-agent setups, see the KARR concepts page for PostgreSQL configuration.</p>"},{"location":"getting-started/first-benchmark/#next-steps","title":"Next Steps","text":"<ul> <li>Run the full <code>standard</code> suite: <code>kitt run -m /path/to/model -e vllm -s standard</code></li> <li>Compare engines: run the same model on multiple engines, then use <code>kitt compare</code></li> <li>Try Docker-based workflows: Docker Quickstart</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":"<p>KITT can run from a pre-built Docker image or be installed from source with Poetry. The Docker method is the fastest way to get started; the source install gives you direct access to the CLI and development tools.</p>"},{"location":"getting-started/installation/#docker-primary-method","title":"Docker (Primary Method)","text":"<p>Build the image and run benchmarks in a single command. No Python environment required on the host.</p> <pre><code>docker build -t kitt .\n</code></pre> <pre><code>docker run --rm --network host \\\n  -v /var/run/docker.sock:/var/run/docker.sock \\\n  -v /path/to/models:/models:ro \\\n  -v ./kitt-results:/app/kitt-results \\\n  kitt run -m /models/llama-7b -e vllm\n</code></pre> Mount Purpose <code>/var/run/docker.sock</code> Lets KITT manage engine containers from inside its own container <code>/path/to/models</code> (read-only) Model weights accessible to both KITT and the engine <code>./kitt-results</code> Benchmark output written back to the host <p>Warning</p> <p>Mounting the Docker socket grants the container full control over Docker on the host. Only use images you trust.</p>"},{"location":"getting-started/installation/#source-install","title":"Source Install","text":""},{"location":"getting-started/installation/#prerequisites","title":"Prerequisites","text":"<ul> <li>Python 3.10+ and Poetry</li> <li>Docker for running inference engines</li> <li>System build tools for native dependencies:</li> </ul> Ubuntu / DebianArch LinuxmacOS <pre><code>sudo apt-get install gcc python3-dev\n</code></pre> <pre><code>sudo pacman -S --needed base-devel\n</code></pre> <pre><code>xcode-select --install\n</code></pre> <ul> <li> <p>NVIDIA Container Toolkit for GPU support (required by all engines except   CPU-only llama.cpp builds):</p> <pre><code># Follow the official NVIDIA guide for your distro:\n# https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html\nnvidia-ctk --version   # verify installation\n</code></pre> </li> </ul>"},{"location":"getting-started/installation/#install","title":"Install","text":"<p>Clone the repository and install with Poetry:</p> <pre><code>git clone https://github.com/kirizan/kitt.git\ncd kitt\npoetry install\n</code></pre> <p>Activate the virtual environment:</p> <pre><code>eval $(poetry env activate)\n</code></pre> <p>Verify the installation:</p> <pre><code>kitt --version\nkitt fingerprint\n</code></pre>"},{"location":"getting-started/installation/#optional-extras","title":"Optional Extras","text":"<p>KITT ships with optional dependency groups for features that not every user needs. Install them individually or pull in everything at once.</p> <pre><code># Individual extras\npoetry install -E datasets\npoetry install -E web\npoetry install -E cli_ui\n\n# Everything\npoetry install -E all\n</code></pre> Extra What It Adds Required For <code>datasets</code> HuggingFace Datasets Quality benchmarks (MMLU, GSM8K, TruthfulQA, HellaSwag) <code>web</code> Flask <code>kitt web</code> dashboard and REST API <code>cli_ui</code> Textual <code>kitt compare</code> interactive TUI <code>all</code> All of the above Full feature set <p>Note</p> <p>Performance benchmarks (throughput, latency, memory, warmup) have no extra dependencies -- they work with the base install.</p>"},{"location":"getting-started/installation/#verify-gpu-access","title":"Verify GPU Access","text":"<p>After installation, confirm that Docker can see your GPU:</p> <pre><code>docker run --rm --gpus all nvidia/cuda:12.4.0-base-ubuntu22.04 nvidia-smi\n</code></pre> <p>Then check KITT's hardware detection:</p> <pre><code>kitt fingerprint --verbose\n</code></pre> <p>This prints a full system profile including GPU model, VRAM, CPU, RAM, storage type, CUDA version, and driver version.</p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Tutorial: First Benchmark -- run an end-to-end test</li> <li>Tutorial: Docker Quickstart -- container-based workflows</li> </ul>"},{"location":"guides/","title":"Guides","text":"<p>How-to guides for specific tasks in KITT. Each guide walks through a particular workflow -- from engine setup to production deployment -- with commands you can copy and adapt.</p> <p>If you are new to KITT, start with the Getting Started section first.</p>"},{"location":"guides/#engine-benchmark-workflows","title":"Engine &amp; Benchmark Workflows","text":"<ul> <li>Engines -- Set up, inspect, and configure inference engines.</li> <li>Benchmarks -- Run test suites, create custom benchmarks, and interpret results.</li> <li>Results &amp; KARR -- Store, compare, import, and export benchmark results through KARR.</li> <li>Campaigns -- Automate multi-model, multi-engine benchmark runs with campaign configs.</li> </ul>"},{"location":"guides/#deployment-infrastructure","title":"Deployment &amp; Infrastructure","text":"<ul> <li>Docker Deployment -- Generate composable Docker stacks with <code>kitt stack</code>.</li> <li>Web Dashboard -- Launch the web UI, configure TLS, and use the REST API.</li> <li>Monitoring -- Deploy Prometheus + Grafana + InfluxDB monitoring stacks locally or remotely.</li> </ul>"},{"location":"guides/#distributed-execution","title":"Distributed Execution","text":"<ul> <li>Agent Daemon -- Run a KITT agent on GPU servers for remote benchmark execution.</li> <li>Remote Execution -- Set up remote hosts and run campaigns over SSH.</li> </ul>"},{"location":"guides/#automation-integration","title":"Automation &amp; Integration","text":"<ul> <li>CI Integration -- Generate reports and post results to GitHub PRs from CI pipelines.</li> <li>Plugins -- Install, list, and remove third-party KITT plugins.</li> <li>Bot Integration -- Run a Slack or Discord bot that responds to benchmark commands.</li> </ul>"},{"location":"guides/#visualization-hardware","title":"Visualization &amp; Hardware","text":"<ul> <li>Charts -- Generate quantization quality tradeoff curves and export data.</li> <li>DGX Spark -- Notes for running KITT on NVIDIA DGX Spark hardware.</li> </ul>"},{"location":"guides/agents/","title":"Agent Daemon","text":"<p>The KITT agent daemon runs on GPU servers and receives benchmark jobs from a central KITT server. This distributed model lets you manage a fleet of GPU machines from a single control plane -- the server dispatches work and agents execute it.</p>"},{"location":"guides/agents/#how-it-works","title":"How It Works","text":"<p>The agent is a lightweight Flask application that listens for commands over HTTPS. When the server sends a <code>run_test</code> command, the agent resolves the model to local storage, starts a benchmark in a background thread, and streams logs back via Server-Sent Events (SSE). A heartbeat thread runs alongside the daemon, reporting status and GPU utilization to the server at a configurable interval (default 30 seconds). Settings configured on the server are synced to the agent via the heartbeat response.</p>"},{"location":"guides/agents/#model-workflow","title":"Model Workflow","text":"<p>When a benchmark is dispatched, the agent resolves the model path through the <code>ModelStorageManager</code>:</p> <ol> <li>Check local storage \u2014 if the model is already under the configured    <code>model_storage_dir</code>, use it directly.</li> <li>Mount NFS share \u2014 if <code>model_share_mount</code> is configured, ensure it is    mounted (via fstab or explicit <code>sudo mount -t nfs</code>).</li> <li>Copy from share \u2014 copy the model from the share to local storage using    <code>shutil.copytree</code>.</li> <li>Run benchmark \u2014 execute the benchmark in a Docker container (falls back    to local <code>kitt run</code> if the image is not available).</li> <li>Cleanup \u2014 if <code>auto_cleanup</code> is enabled, delete the local copy after the    benchmark completes.</li> </ol> <p>This ensures benchmarks always run against a local copy, avoiding NFS latency during inference.</p>"},{"location":"guides/agents/#nfs-share-configuration","title":"NFS share configuration","text":"<p>Set the share source and mount point via the web UI (Agents &gt; Detail &gt; Settings) or during initialization:</p> <pre><code>kitt-agent init --server https://server:8080 \\\n    --model-dir /data/models \\\n    --share-source nas:/volume1/models \\\n    --share-mount /mnt/models\n</code></pre> <p>For passwordless mounts, add an entry to <code>/etc/fstab</code>:</p> <pre><code>nas:/volume1/models  /mnt/models  nfs  defaults,nofail  0  0\n</code></pre>"},{"location":"guides/agents/#initializing-the-agent","title":"Initializing the Agent","text":"<p>Before starting the agent you must register it with a KITT server:</p> <pre><code>kitt-agent init --server https://server:8080\n</code></pre> <p>This command writes <code>~/.kitt/agent.yaml</code> with the server URL, token, agent name, and optional model storage paths.</p> Flag Default Description <code>--token</code> (empty) Bearer token for server authentication <code>--name</code> hostname Friendly agent name <code>--port</code> 8090 Port the agent listens on <code>--model-dir</code> <code>~/.kitt/models</code> Local model storage directory <code>--share-source</code> (empty) NFS share source (e.g., <code>nas:/volume1/models</code>) <code>--share-mount</code> (empty) Local mount point for NFS share"},{"location":"guides/agents/#preflight-checks","title":"Preflight Checks","text":"<p>Run prerequisite checks before starting:</p> <pre><code>kitt-agent preflight --server https://server:8080 --port 8090\n</code></pre> <p>Checks performed:</p> Check Required How Python &gt;= 3.10 Yes <code>sys.version_info</code> Docker available Yes <code>docker info</code> subprocess Docker GPU access Yes <code>docker run --gpus all nvidia/cuda:...</code> KITT Docker image No <code>docker image inspect kitt:latest</code> NVIDIA drivers Yes <code>nvidia-smi</code> subprocess NFS utilities No Check for <code>mount.nfs</code> in PATH Disk space (&gt;= 50GB) No <code>shutil.disk_usage</code> on model dir Server reachable Yes HTTP GET to <code>/api/v1/health</code> Port available No <code>socket.bind</code> on agent port <p>Required checks that fail cause exit code 1.</p> <p>The install script runs preflight automatically. You can also use the <code>--preflight</code> flag on start:</p> <pre><code>kitt-agent start --preflight\n</code></pre>"},{"location":"guides/agents/#building-the-docker-image","title":"Building the Docker Image","text":"<p>The agent runs benchmarks inside a Docker container built from the KITT source. Each agent builds the image locally so it is native to the host architecture (amd64 or arm64). The install script does this automatically, but you can also build or rebuild manually:</p> <pre><code>kitt-agent build\n</code></pre> <p>The command downloads the build context tarball from the server, verifies its SHA-256 digest, and runs <code>docker build</code> locally. The resulting image is tagged as <code>kitt:latest</code> (and with any custom tag from <code>kitt_image</code> in agent settings).</p> Flag Description <code>--server</code> KITT server URL (reads from <code>agent.yaml</code> if not set) <code>--tag</code> Override the image tag <code>--no-cache</code> Build without Docker cache <p>Rebuild the image after updating the agent to pick up new KITT changes:</p> <pre><code>kitt-agent update\nkitt-agent build\n</code></pre>"},{"location":"guides/agents/#starting-the-agent","title":"Starting the Agent","text":"<pre><code>kitt-agent start\n</code></pre> <p>On startup the agent:</p> <ol> <li>Loads <code>~/.kitt/agent.yaml</code> (override with <code>--config</code>).</li> <li>Detects hardware \u2014 GPU (with unified memory fallback for architectures like    DGX Spark GB10), CPU, RAM, storage, CUDA version, driver version, environment    type, and compute capability.</li> <li>Initializes <code>ModelStorageManager</code> from config.</li> <li>Registers with the server via <code>POST /api/v1/agents/register</code>, sending a full    hardware fingerprint and detailed hardware info.</li> <li>Starts a <code>HeartbeatThread</code> that sends periodic status, GPU utilization,    memory usage, and storage availability to the server.</li> <li>Launches the Flask app on the configured port with optional TLS.</li> </ol> <p>Use <code>--insecure</code> to skip TLS verification during development.</p>"},{"location":"guides/agents/#agent-settings","title":"Agent Settings","text":"<p>Settings are stored on the server in the <code>agent_settings</code> table and synced to the agent via the heartbeat response. Edit them from the web UI on the agent detail page or via the REST API.</p> Setting Default Description <code>model_storage_dir</code> <code>~/.kitt/models</code> Local directory for model copies <code>model_share_source</code> (empty) NFS share source <code>model_share_mount</code> (empty) Local mount point for NFS share <code>auto_cleanup</code> <code>true</code> Delete local model copies after benchmarks <code>heartbeat_interval_s</code> <code>30</code> Seconds between heartbeats (10-300) <code>kitt_image</code> (empty) Docker image tag for benchmark containers"},{"location":"guides/agents/#mtls-communication","title":"mTLS Communication","text":"<p>When the server uses HTTPS, agent-server communication is secured with mutual TLS. During <code>kitt-agent init</code> KITT generates a client certificate and stores the paths in <code>agent.yaml</code> under the <code>tls</code> key:</p> <pre><code>tls:\n  cert: /home/user/.kitt/certs/agent.pem\n  key: /home/user/.kitt/certs/agent-key.pem\n  ca: /home/user/.kitt/certs/ca.pem\n</code></pre> <p>Both the heartbeat and the registration request present the client certificate. The server validates it against the same CA.</p>"},{"location":"guides/agents/#systemd-service","title":"Systemd Service","text":"<p>For production deployments, install the agent as a systemd service:</p> <pre><code>~/.kitt/agent-venv/bin/kitt-agent service install\n</code></pre> <p>This generates a systemd unit file, installs it via <code>sudo</code>, and starts the service. The agent will survive reboots and restart automatically on failure.</p> <p>Manage the service:</p> <pre><code>kitt-agent service status      # check service status\nkitt-agent service uninstall   # stop, disable, and remove the service\n</code></pre>"},{"location":"guides/agents/#updating-the-agent","title":"Updating the Agent","text":"<pre><code>kitt-agent update              # download and install latest from server\nkitt-agent update --restart    # update and restart in one step\n</code></pre> <p>The <code>update</code> command downloads the latest agent package from the KITT server (<code>/api/v1/agent/package</code>) and reinstalls it into the agent's virtual environment. Use <code>--restart</code> to automatically stop the running agent and start the new version.</p> <p>After updating, rebuild the Docker image to pick up new KITT changes:</p> <pre><code>kitt-agent build\n</code></pre> <p>If the agent is managed by systemd, restart the service after updating:</p> <pre><code>kitt-agent update\nkitt-agent build\nsudo systemctl restart kitt-agent\n</code></pre>"},{"location":"guides/agents/#heartbeat-and-command-dispatch","title":"Heartbeat and Command Dispatch","text":"<p>The <code>HeartbeatThread</code> sends a JSON payload to <code>/api/v1/agents/&lt;agent_id&gt;/heartbeat</code> at the configured interval. The payload includes:</p> <ul> <li>Agent status (<code>idle</code>, <code>running</code>, <code>error</code>)</li> <li>Current task identifier</li> <li>GPU utilization percentage (via pynvml)</li> <li>GPU memory used in GB</li> <li>Storage free space in GB</li> <li>Agent uptime</li> </ul> <p>During active benchmarks, the heartbeat interval is automatically increased to at least 60 seconds to reduce overhead.</p> <p>The heartbeat response includes:</p> <ul> <li><code>commands</code> \u2014 pending jobs (e.g., quick tests queued from the web UI)</li> <li><code>settings</code> \u2014 current agent settings for sync</li> </ul> <p>The agent processes each command automatically \u2014 for <code>run_test</code> commands it resolves the model, starts the benchmark, and streams log lines back to the server via <code>POST /api/v1/quicktest/&lt;test_id&gt;/logs</code>. Status transitions are reported via <code>POST /api/v1/quicktest/&lt;test_id&gt;/status</code>.</p>"},{"location":"guides/agents/#log-streaming","title":"Log Streaming","text":"<p>When a benchmark runs, the agent captures output through a <code>LogStreamer</code> and exposes it as an SSE endpoint at <code>/api/logs/&lt;command_id&gt;</code>. The server or any authorized client can subscribe to this stream for real-time log output.</p>"},{"location":"guides/agents/#checking-status","title":"Checking Status","text":"<pre><code>kitt-agent status\n</code></pre> <p>This reads <code>~/.kitt/agent.yaml</code> and probes the local agent at <code>http://127.0.0.1:&lt;port&gt;/api/status</code> to report whether the daemon is running and whether a benchmark is currently active.</p>"},{"location":"guides/agents/#managing-tests","title":"Managing Tests","text":"<p>List tests dispatched to this agent:</p> <pre><code>kitt-agent test list                    # show all tests for this agent\nkitt-agent test list --status running   # filter by status\nkitt-agent test list --limit 5          # limit results\n</code></pre> <p>Stop a running or queued test:</p> <pre><code>kitt-agent test stop &lt;test_id&gt;\n</code></pre> <p>The <code>stop</code> command marks the test as failed on the server with an \"Cancelled by user\" error and sends a cancel signal to the local daemon to kill the running process.</p>"},{"location":"guides/agents/#stopping-the-agent","title":"Stopping the Agent","text":"<pre><code>kitt-agent stop\n</code></pre> <p>Sends <code>SIGTERM</code> to the agent process using the PID stored in <code>~/.kitt/agent.pid</code>.</p>"},{"location":"guides/benchmarks/","title":"Benchmarks","text":"<p>KITT ships with quality and performance benchmarks organized into test suites. You can also define custom benchmarks in YAML.</p>"},{"location":"guides/benchmarks/#test-suites","title":"Test Suites","text":"Suite Description Benchmarks Default Runs <code>quick</code> Smoke test Throughput only 1 <code>standard</code> Full evaluation All quality + performance 3 <code>performance</code> Performance-focused Throughput, latency, memory, warmup 3"},{"location":"guides/benchmarks/#built-in-benchmarks","title":"Built-in Benchmarks","text":""},{"location":"guides/benchmarks/#quality","title":"Quality","text":"Benchmark Description MMLU Massive Multitask Language Understanding -- broad knowledge evaluation GSM8K Grade school math reasoning TruthfulQA Factual consistency evaluation HellaSwag Commonsense reasoning <p>Quality benchmarks require the <code>datasets</code> extra (<code>poetry install -E datasets</code>).</p>"},{"location":"guides/benchmarks/#performance","title":"Performance","text":"Benchmark Description Throughput Requests per second at various concurrency levels Latency Time-to-first-token and end-to-end response time Memory Peak VRAM and CPU memory usage during inference Warmup Analysis Measures performance stabilization over initial requests"},{"location":"guides/benchmarks/#running-benchmarks","title":"Running Benchmarks","text":"<p>Use <code>kitt run</code> to execute a test suite against a model and engine:</p> <pre><code>kitt run -m MODEL -e ENGINE -s SUITE -o OUTPUT\n</code></pre> Option Short Description <code>--model</code> <code>-m</code> Path to model or model identifier (required) <code>--engine</code> <code>-e</code> Inference engine key (required) <code>--suite</code> <code>-s</code> Test suite: <code>quick</code>, <code>standard</code>, <code>performance</code> (default: <code>quick</code>) <code>--output</code> <code>-o</code> Output directory for results <code>--runs</code> Override the number of runs per benchmark <code>--skip-warmup</code> Skip the warmup phase <code>--config</code> Path to custom engine configuration YAML <code>--store-karr</code> Also store results in KARR's legacy Git-backed backend <p>Examples:</p> <pre><code># Quick throughput test with Ollama\nkitt run -m llama3 -e ollama\n\n# Full standard suite with vLLM\nkitt run -m /models/llama2-7b -e vllm -s standard -o ./my-results\n\n# Performance suite with legacy Git-backed KARR storage\nkitt run -m /models/mistral-7b -e llama_cpp -s performance --store-karr\n\n# Override run count\nkitt run -m /models/qwen-7b -e tgi -s standard --runs 5\n</code></pre>"},{"location":"guides/benchmarks/#output-artifacts","title":"Output Artifacts","text":"<p>Each run produces the following files in the output directory:</p> File Description <code>metrics.json</code> Full benchmark metrics in JSON format <code>hardware.json</code> Detected hardware information <code>config.json</code> Configuration used for the run <code>summary.md</code> Human-readable Markdown summary <code>outputs/</code> Compressed benchmark outputs (chunked)"},{"location":"guides/benchmarks/#custom-benchmarks","title":"Custom Benchmarks","text":""},{"location":"guides/benchmarks/#creating-a-new-benchmark","title":"Creating a New Benchmark","text":"<p>Generate a YAML template with <code>kitt test new</code>:</p> <pre><code>kitt test new my-eval\nkitt test new my-eval --category performance\n</code></pre> <p>This creates a file at <code>configs/tests/quality/custom/my-eval.yaml</code> (or in the <code>performance</code> directory if you set <code>--category performance</code>).</p>"},{"location":"guides/benchmarks/#yaml-benchmark-template","title":"YAML Benchmark Template","text":"<pre><code>name: my-eval\ncategory: quality_custom\ndescription: \"My custom evaluation\"\ndataset:\n  source: local\n  path: ./data/my-eval.jsonl\nprompts:\n  template: \"{question}\"\n  answer_key: \"answer\"\nsampling:\n  max_tokens: 256\n  temperature: 0.0\nscoring:\n  method: exact_match\nruns: 3\n</code></pre> <p>Edit the template to point at your dataset, adjust the prompt template, and configure scoring. KITT picks up custom YAML benchmarks automatically when they are placed in <code>configs/tests/</code>.</p>"},{"location":"guides/benchmarks/#listing-benchmarks","title":"Listing Benchmarks","text":"<p>List all available benchmarks (built-in and custom):</p> <pre><code>kitt test list\n</code></pre> <p>Filter by category:</p> <pre><code>kitt test list -c performance\nkitt test list -c quality\n</code></pre>"},{"location":"guides/benchmarks/#checkpoint-recovery","title":"Checkpoint Recovery","text":"<p>Long-running benchmarks save checkpoints every 100 items. If a run is interrupted, restarting with the same output directory resumes from the last checkpoint rather than starting over.</p>"},{"location":"guides/bots/","title":"Bot Integration","text":"<p>KITT provides Slack and Discord bots that let team members trigger benchmarks and view results directly from chat.</p>"},{"location":"guides/bots/#installation","title":"Installation","text":"<p>The bot dependencies are optional extras:</p> <pre><code># Slack (uses slack-bolt)\npoetry install -E slack\n\n# Discord (uses discord.py)\npoetry install -E discord\n</code></pre>"},{"location":"guides/bots/#slack-bot-setup","title":"Slack Bot Setup","text":"<ol> <li>Create a Slack app at https://api.slack.com/apps.</li> <li>Enable Socket Mode and generate an app-level token (<code>xapp-...</code>).</li> <li>Under Slash Commands, add a <code>/kitt</code> command.</li> <li>Install the app to your workspace and copy the bot token (<code>xoxb-...</code>).</li> <li>Start the bot:</li> </ol> <pre><code>kitt bot start --platform slack --token xoxb-... --app-token xapp-...\n</code></pre> <p>The Slack bot uses <code>slack-bolt</code> in Socket Mode, so no public URL or ingress is required. The <code>/kitt</code> slash command accepts the same arguments as the CLI.</p>"},{"location":"guides/bots/#discord-bot-setup","title":"Discord Bot Setup","text":"<ol> <li>Create a Discord application at https://discord.com/developers.</li> <li>Under Bot, create a bot and copy the token.</li> <li>Enable the Message Content Intent under Privileged Gateway Intents.</li> <li>Generate an invite URL with the Send Messages permission and add the bot    to your server.</li> <li>Start the bot:</li> </ol> <pre><code>kitt bot start --platform discord --token &lt;BOT_TOKEN&gt;\n</code></pre> <p>The Discord bot listens for messages prefixed with <code>!kitt</code> and responds with benchmark results.</p>"},{"location":"guides/bots/#bot-commands","title":"Bot Commands","text":"<p>Both platforms support the same core commands:</p> Command Description <code>run -m MODEL -e ENGINE -s SUITE</code> Start a benchmark <code>engines list</code> List available engines <code>results list</code> List stored results <code>status</code> Show current run status"},{"location":"guides/bots/#configuration-reference","title":"Configuration Reference","text":"<p>View setup instructions for both platforms:</p> <pre><code>kitt bot config\n</code></pre> <p>This prints step-by-step instructions for creating the Slack and Discord applications, obtaining tokens, and starting the bot.</p>"},{"location":"guides/bots/#running-in-production","title":"Running in Production","text":"<p>For long-lived deployments, run the bot as a systemd service:</p> <pre><code>[Unit]\nDescription=KITT Slack Bot\nAfter=network-online.target\n\n[Service]\nType=simple\nExecStart=/usr/local/bin/kitt bot start --platform slack --token xoxb-... --app-token xapp-...\nRestart=on-failure\n\n[Install]\nWantedBy=multi-user.target\n</code></pre> <p>Store tokens in environment variables or a secrets manager rather than passing them directly on the command line in production.</p>"},{"location":"guides/campaigns/","title":"Campaigns","text":"<p>Campaigns automate benchmark runs across multiple models, engines, and quantization variants. Define the matrix in a YAML file and KITT handles scheduling, state tracking, and failure recovery.</p>"},{"location":"guides/campaigns/#campaign-yaml-structure","title":"Campaign YAML Structure","text":"<pre><code>campaign_name: my-campaign\ndescription: \"Benchmark Llama and Qwen across all engines\"\n\nmodels:\n  - name: Llama-3.1-8B-Instruct\n    params: \"8B\"\n    safetensors_repo: meta-llama/Llama-3.1-8B-Instruct\n    gguf_repo: bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\n    ollama_tag: \"llama3.1:8b\"\n    estimated_size_gb: 16.0\n\n  - name: Qwen2.5-7B-Instruct\n    params: \"7B\"\n    gguf_repo: Qwen/Qwen2.5-7B-Instruct-GGUF\n    ollama_tag: \"qwen2.5:7b\"\n    estimated_size_gb: 14.0\n\nengines:\n  - name: llama_cpp\n    suite: standard\n    formats: [gguf]\n  - name: ollama\n    suite: standard\n    formats: [gguf]\n\ndisk:\n  reserve_gb: 100.0\n  cleanup_after_run: true\n\nnotifications:\n  desktop: true\n  on_complete: true\n  on_failure: true\n\nquant_filter:\n  skip_patterns: [\"IQ1_*\", \"IQ2_*\"]\n\nresource_limits:\n  max_model_size_gb: 100.0\n\nparallel: false\ndevon_managed: true\n</code></pre> <p>Each model can specify multiple source repositories (safetensors, GGUF, Ollama tag). KITT automatically matches models to compatible engines based on format.</p>"},{"location":"guides/campaigns/#running-a-campaign","title":"Running a Campaign","text":"<pre><code>kitt campaign run configs/campaigns/example.yaml\n</code></pre> <p>Use <code>--dry-run</code> to preview the planned runs without executing them:</p> <pre><code>kitt campaign run configs/campaigns/example.yaml --dry-run\n</code></pre>"},{"location":"guides/campaigns/#campaign-wizard","title":"Campaign Wizard","text":"<p>Build a campaign config interactively:</p> <pre><code>kitt campaign wizard\n</code></pre> <p>The wizard walks through model selection, engine configuration, disk limits, and notification settings, then outputs a YAML file you can save and edit.</p>"},{"location":"guides/campaigns/#campaign-lifecycle-and-state","title":"Campaign Lifecycle and State","text":"<p>KITT persists campaign state so you can track progress and resume interrupted runs.</p> <pre><code># Check latest campaign status\nkitt campaign status\n\n# Check a specific campaign\nkitt campaign status &lt;campaign-id&gt;\n\n# List all campaigns\nkitt campaign list\n</code></pre> <p>Campaign states: pending, running, completed, failed.</p>"},{"location":"guides/campaigns/#resuming-and-rerunning-failures","title":"Resuming and Rerunning Failures","text":"<p>If a campaign is interrupted or some runs fail, resume from where it left off:</p> <pre><code>kitt campaign run configs/campaigns/example.yaml --resume\nkitt campaign run configs/campaigns/example.yaml --resume --campaign-id &lt;id&gt;\n</code></pre> <p>Only pending and failed runs are re-executed. Successful runs are skipped.</p> <p>You can also create a dedicated failure-rerun config that targets specific models and quants. See <code>configs/campaigns/rerun-failures.yaml</code> for an example.</p>"},{"location":"guides/campaigns/#scheduling","title":"Scheduling","text":"<p>Schedule a campaign to run on a cron expression:</p> <pre><code>kitt campaign schedule configs/campaigns/example.yaml --cron \"0 2 * * *\"\nkitt campaign cron-status\nkitt campaign unschedule &lt;schedule-id&gt;\n</code></pre>"},{"location":"guides/campaigns/#generating-a-config-from-existing-results","title":"Generating a Config from Existing Results","text":"<p>Create a campaign config that replays the model/engine combinations found in an existing results directory:</p> <pre><code>kitt campaign create --from-results ./kitt-results -o replay.yaml\n</code></pre>"},{"location":"guides/campaigns/#key-options-reference","title":"Key Options Reference","text":"Option Description <code>parallel</code> Run models in parallel (requires multiple GPUs) <code>devon_managed</code> Use DEVON for model download management <code>quant_filter.skip_patterns</code> Glob patterns for quantization variants to skip <code>quant_filter.include_only</code> Only include these specific quant names <code>resource_limits.max_model_size_gb</code> Skip models exceeding this size <code>disk.reserve_gb</code> Minimum free disk space to maintain <code>disk.cleanup_after_run</code> Delete downloaded models after each run"},{"location":"guides/charts/","title":"Charts and Visualization","text":"<p>KITT can generate charts from stored benchmark results using Matplotlib. This is useful for visualizing quantization quality tradeoffs, comparing engine performance, and tracking metric trends over time.</p>"},{"location":"guides/charts/#installation","title":"Installation","text":"<p>Charts require the optional <code>charts</code> extra:</p> <pre><code>poetry install -E charts\n</code></pre> <p>This pulls in Matplotlib and its dependencies.</p>"},{"location":"guides/charts/#quantization-curves","title":"Quantization Curves","text":"<p>Generate a chart showing how quantization levels affect quality metrics across model families:</p> <pre><code>kitt charts quant-curves\n</code></pre> <p>Filter by model family:</p> <pre><code>kitt charts quant-curves --model-family Llama-3\n</code></pre> <p>Change the output file:</p> <pre><code>kitt charts quant-curves --output llama3_quant.svg\n</code></pre> <p>Export the underlying data as CSV instead of rendering a chart:</p> <pre><code>kitt charts quant-curves --csv\n</code></pre>"},{"location":"guides/charts/#output-formats","title":"Output Formats","text":"<p>Charts are saved as SVG by default. The output format is determined by the file extension you provide:</p> Extension Format <code>.svg</code> Scalable Vector Graphics (default) <code>.png</code> Raster image <code>.pdf</code> PDF document <p>Example:</p> <pre><code>kitt charts quant-curves --output curves.png\n</code></pre>"},{"location":"guides/charts/#data-sources","title":"Data Sources","text":"<p>Chart commands read from the KITT storage backend. KITT tries SQLite first (<code>SQLiteStore</code>) and falls back to the JSON store (<code>JsonStore</code>). Make sure you have stored results (via <code>kitt run</code> or the storage commands) before generating charts.</p>"},{"location":"guides/charts/#chart-types","title":"Chart Types","text":"Command Description <code>kitt charts quant-curves</code> Quality vs. quantization level curves <p>Additional chart types may be added through the plugin system. The chart generation logic lives in <code>src/kitt/reporters/quant_curves.py</code> and uses the same result store interface as the rest of KITT.</p>"},{"location":"guides/ci-integration/","title":"CI/CD Integration","text":"<p>KITT integrates with CI/CD pipelines to automate benchmark runs, post results as PR comments, and gate deployments on performance thresholds.</p>"},{"location":"guides/ci-integration/#ci-report-command","title":"CI Report Command","text":"<p>The <code>kitt ci report</code> command generates a summary from benchmark results and optionally posts it to a GitHub pull request:</p> <pre><code>kitt ci report \\\n    --results-dir ./benchmark-output \\\n    --baseline-dir ./baseline-results \\\n    --github-token \"$GITHUB_TOKEN\" \\\n    --repo owner/repo \\\n    --pr 42\n</code></pre> Flag Description <code>--results-dir</code> Directory containing the latest benchmark <code>metrics.json</code> <code>--baseline-dir</code> Previous results to compare against (optional) <code>--github-token</code> GitHub API token for posting comments <code>--repo</code> Repository in <code>owner/repo</code> format <code>--pr</code> Pull request number <code>--output</code> Write the report to a local file instead of posting <p>When both <code>--results-dir</code> and <code>--baseline-dir</code> are provided, the report includes a comparison showing regressions and improvements.</p> <p>If a KITT comment already exists on the PR (identified by a hidden HTML marker), it is updated in place rather than creating a duplicate.</p>"},{"location":"guides/ci-integration/#github-actions-workflow","title":"GitHub Actions Workflow","text":"<p>Below is a complete workflow that runs benchmarks on every pull request and posts results as a comment:</p> <pre><code>name: KITT Benchmark\n\non:\n  pull_request:\n    branches: [main]\n\njobs:\n  benchmark:\n    runs-on: [self-hosted, gpu]\n    steps:\n      - uses: actions/checkout@v4\n\n      - name: Install KITT\n        run: pip install kitt-bench\n\n      - name: Pull engine image\n        run: kitt engines setup vllm\n\n      - name: Run benchmarks\n        run: |\n          kitt run \\\n            -m ./models/llama-3-8b \\\n            -e vllm \\\n            -s quick \\\n            -o ./results\n\n      - name: Post CI report\n        if: github.event_name == 'pull_request'\n        run: |\n          kitt ci report \\\n            --results-dir ./results \\\n            --github-token \"${{ secrets.GITHUB_TOKEN }}\" \\\n            --repo \"${{ github.repository }}\" \\\n            --pr \"${{ github.event.pull_request.number }}\"\n\n      - name: Upload artifacts\n        uses: actions/upload-artifact@v4\n        with:\n          name: benchmark-results\n          path: ./results/\n</code></pre>"},{"location":"guides/ci-integration/#using-a-baseline","title":"Using a Baseline","text":"<p>To detect regressions, store baseline results as a build artifact or in a dedicated branch and compare against them:</p> <pre><code>      - name: Download baseline\n        uses: actions/download-artifact@v4\n        with:\n          name: benchmark-baseline\n          path: ./baseline\n        continue-on-error: true\n\n      - name: Post report with comparison\n        run: |\n          kitt ci report \\\n            --results-dir ./results \\\n            --baseline-dir ./baseline \\\n            --github-token \"${{ secrets.GITHUB_TOKEN }}\" \\\n            --repo \"${{ github.repository }}\" \\\n            --pr \"${{ github.event.pull_request.number }}\"\n</code></pre>"},{"location":"guides/ci-integration/#artifact-collection","title":"Artifact Collection","text":"<p>Benchmark runs produce the following files under the output directory:</p> File Contents <code>metrics.json</code> Raw benchmark metrics <code>summary.md</code> Human-readable Markdown summary <code>hardware.json</code> Hardware fingerprint of the runner <code>config.json</code> Configuration used for the run <p>Upload these as workflow artifacts to preserve a history of benchmark results across builds.</p>"},{"location":"guides/ci-integration/#local-report-generation","title":"Local Report Generation","text":"<p>Generate a report file without posting to GitHub:</p> <pre><code>kitt ci report --results-dir ./results --output report.md\n</code></pre> <p>This is useful for local review or integration with other reporting systems.</p>"},{"location":"guides/ci-integration/#exit-codes","title":"Exit Codes","text":"<p>KITT commands use standard exit codes so CI systems can detect failures:</p> Code Meaning 0 Success 1 Benchmark failure, missing results, or API error <p>Use these exit codes to gate merge or deployment steps in your pipeline.</p>"},{"location":"guides/deployment/","title":"Docker Deployment","text":"<p>KITT can generate composable Docker deployment stacks using <code>kitt stack</code>. Each stack is a <code>docker-compose.yaml</code> built from the components you select, stored at <code>~/.kitt/stacks/&lt;name&gt;/</code>.</p>"},{"location":"guides/deployment/#generating-a-stack","title":"Generating a Stack","text":"<pre><code>kitt stack generate &lt;name&gt; [OPTIONS]\n</code></pre> <p>At least one component flag is required. Available components:</p> Flag Component Description <code>--web</code> Web UI + REST API Full Flask dashboard with agents, campaigns, results, and models <code>--reporting</code> Reporting dashboard Lightweight read-only results viewer <code>--agent</code> Agent daemon Runs on GPU servers, executes benchmarks on behalf of the web server <code>--postgres</code> PostgreSQL Persistent database backend <code>--monitoring</code> Monitoring stack Prometheus + Grafana + InfluxDB <p><code>--web</code> and <code>--reporting</code> are mutually exclusive -- use one or the other.</p>"},{"location":"guides/deployment/#port-configuration","title":"Port Configuration","text":"Option Default Component <code>--port</code> 8080 Web UI or reporting dashboard <code>--agent-port</code> 8090 Agent daemon <code>--postgres-port</code> 5432 PostgreSQL <code>--grafana-port</code> 3000 Grafana <code>--prometheus-port</code> 9090 Prometheus <code>--influxdb-port</code> 8086 InfluxDB"},{"location":"guides/deployment/#auth-tokens-and-secrets","title":"Auth Tokens and Secrets","text":"Option Description <code>--auth-token</code> Bearer token for REST API authentication <code>--secret-key</code> Flask secret key for session signing <code>--postgres-password</code> PostgreSQL password (default: <code>kitt</code>) <code>--server-url</code> KITT server URL for agent registration <p>Tokens and passwords are written into the generated <code>.env</code> file alongside the <code>docker-compose.yaml</code>.</p>"},{"location":"guides/deployment/#stack-lifecycle","title":"Stack Lifecycle","text":""},{"location":"guides/deployment/#generate","title":"Generate","text":"<pre><code>kitt stack generate prod --web --postgres --monitoring --auth-token mytoken\n</code></pre>"},{"location":"guides/deployment/#start","title":"Start","text":"<pre><code>kitt stack start --name prod\n</code></pre>"},{"location":"guides/deployment/#check-status","title":"Check Status","text":"<pre><code>kitt stack status --name prod\n</code></pre>"},{"location":"guides/deployment/#stop","title":"Stop","text":"<pre><code>kitt stack stop --name prod\n</code></pre>"},{"location":"guides/deployment/#list-all-stacks","title":"List All Stacks","text":"<pre><code>kitt stack list\n</code></pre>"},{"location":"guides/deployment/#remove","title":"Remove","text":"<pre><code>kitt stack remove prod\nkitt stack remove prod --delete-files   # also removes generated files\n</code></pre>"},{"location":"guides/deployment/#example-workflows","title":"Example Workflows","text":""},{"location":"guides/deployment/#full-web-stack-with-database","title":"Full Web Stack with Database","text":"<p>A production-like deployment with the web UI, PostgreSQL for persistent storage, and API authentication:</p> <pre><code>kitt stack generate prod --web --postgres --port 8080 --auth-token mytoken\nkitt stack start --name prod\n</code></pre>"},{"location":"guides/deployment/#reporting-only-stack","title":"Reporting-Only Stack","text":"<p>A lightweight read-only dashboard for viewing existing results:</p> <pre><code>kitt stack generate reports --reporting --port 8080\nkitt stack start --name reports\n</code></pre>"},{"location":"guides/deployment/#gpu-agent","title":"GPU Agent","text":"<p>Deploy an agent daemon on a GPU server that registers with a central KITT web server:</p> <pre><code>kitt stack generate gpu1 --agent --agent-port 8090 --server-url https://server:8080\nkitt stack start --name gpu1\n</code></pre>"},{"location":"guides/deployment/#full-stack-with-monitoring","title":"Full Stack with Monitoring","text":"<p>Everything together -- web UI, database, and metrics collection:</p> <pre><code>kitt stack generate full --web --postgres --monitoring --port 8080\nkitt stack start --name full\n</code></pre>"},{"location":"guides/deployment/#production-considerations","title":"Production Considerations","text":"<ul> <li> <p>TLS: The web component auto-generates self-signed TLS certificates by   default. For production, supply your own certificates via <code>--tls-cert</code> and   <code>--tls-key</code> on the <code>kitt web</code> command, or terminate TLS at a reverse proxy.</p> </li> <li> <p>Secrets: Generate strong values for <code>--auth-token</code>, <code>--secret-key</code>, and   <code>--postgres-password</code>. Do not use the defaults in production.</p> </li> <li> <p>Volumes: The generated <code>docker-compose.yaml</code> uses named Docker volumes   for PostgreSQL data and Grafana dashboards. Back these up regularly.</p> </li> <li> <p>Networking: All containers run on the host network by default. Adjust   port flags if you have conflicts with other services.</p> </li> <li> <p>Resource limits: For GPU-heavy workloads, consider running the agent   stack on dedicated GPU hosts and the web + database stack on a separate   management node.</p> </li> </ul>"},{"location":"guides/dgx-spark/","title":"DGX Spark","text":"<p>KITT includes specific detection and handling for NVIDIA DGX Spark systems. This guide covers what KITT does differently on DGX hardware and any special considerations for running benchmarks on these machines.</p>"},{"location":"guides/dgx-spark/#environment-detection","title":"Environment Detection","text":"<p>KITT automatically identifies DGX Spark systems during hardware fingerprinting. The detection checks:</p> <ol> <li><code>/etc/dgx-release</code> -- if present and contains \"spark\", the environment is    classified as <code>dgx_spark</code>.</li> <li><code>/etc/nvidia/nvidia-dgs.conf</code> -- presence of this file also triggers    <code>dgx_spark</code> classification.</li> <li>If <code>/etc/dgx-release</code> exists but does not contain \"spark\", the environment    is classified as <code>dgx</code> (standard DGX).</li> </ol> <p>You can verify the detected environment with:</p> <pre><code>kitt fingerprint --verbose\n</code></pre> <p>The output includes the environment type (<code>dgx_spark</code>, <code>dgx</code>, <code>native_linux</code>, etc.) alongside GPU, CPU, RAM, and storage details.</p>"},{"location":"guides/dgx-spark/#gpu-detection-on-dgx-spark-gh200","title":"GPU Detection on DGX Spark (GH200)","text":"<p>The DGX Spark uses the NVIDIA GH200 Grace Hopper Superchip, which has a unified memory architecture. This means standard VRAM queries may not return a meaningful value:</p> <ul> <li>pynvml: <code>nvmlDeviceGetMemoryInfo</code> may fail or return zero on unified   memory systems. KITT catches this and logs a debug message rather than   crashing.</li> <li>nvidia-smi: The memory column may report <code>[N/A]</code>. KITT handles this   gracefully and sets VRAM to 0 GB in the fingerprint.</li> </ul> <p>Despite the memory query limitations, GPU model name and compute capability detection work normally.</p>"},{"location":"guides/dgx-spark/#docker-on-dgx","title":"Docker on DGX","text":"<p>DGX systems ship with Docker and the NVIDIA Container Toolkit pre-installed. KITT uses <code>--network host</code> for all engine containers, which works out of the box on DGX. No additional Docker configuration is required.</p> <p>If you are running KITT inside a container on DGX, make sure the container has access to the GPU:</p> <pre><code>docker run --gpus all --network host ...\n</code></pre>"},{"location":"guides/dgx-spark/#tested-environments","title":"Tested Environments","text":"<p>KITT is tested on the following DGX platforms:</p> Platform Environment Type DGX Spark (GH200) <code>dgx_spark</code> DGX Station / DGX A100 / DGX H100 <code>dgx</code> <p>Both environment types receive enhanced diagnostic messages when GPU detection fails. If KITT cannot find a GPU on a system classified as <code>dgx_spark</code> or <code>dgx</code>, it logs a warning with specific guidance to check that NVIDIA drivers are loaded and accessible.</p>"},{"location":"guides/dgx-spark/#build-and-deployment-notes","title":"Build and Deployment Notes","text":"<p>On DGX Spark, the GH200 has ARM (Grace) CPU cores. Make sure any Docker images you use are built for <code>linux/arm64</code>. The standard KITT engine images (vLLM, TGI, llama.cpp, Ollama) publish multi-architecture images that include ARM support.</p> <p>When generating deployment stacks with <code>kitt stack generate</code>, the generated <code>docker-compose.yaml</code> does not pin a platform architecture, so Docker will automatically pull the correct image for the host.</p>"},{"location":"guides/engines/","title":"Engines","text":"<p>KITT supports four inference engines. Every engine runs inside a Docker container -- no host-level installs are needed beyond Docker itself.</p>"},{"location":"guides/engines/#supported-engines","title":"Supported Engines","text":"Engine Key Docker Image API Format Default Port Model Formats vLLM <code>vllm</code> <code>vllm/vllm-openai:latest</code> OpenAI <code>/v1/completions</code> 8000 safetensors, pytorch TGI <code>tgi</code> <code>ghcr.io/huggingface/text-generation-inference:latest</code> HuggingFace <code>/generate</code> 8080 safetensors, pytorch llama.cpp <code>llama_cpp</code> <code>ghcr.io/ggerganov/llama.cpp:server</code> OpenAI <code>/v1/completions</code> 8081 gguf Ollama <code>ollama</code> <code>ollama/ollama:latest</code> Ollama <code>/api/generate</code> 11434 gguf"},{"location":"guides/engines/#listing-engines","title":"Listing Engines","text":"<p>Display all registered engines, their Docker images, pull status, and supported model formats:</p> <pre><code>kitt engines list\n</code></pre> <p>The output shows Ready for engines whose Docker image is already pulled, and Not Pulled for those that still need to be fetched.</p>"},{"location":"guides/engines/#checking-availability","title":"Checking Availability","text":"<p>Run diagnostics on a single engine to see whether its image is available, Docker is reachable, and which model formats it accepts:</p> <pre><code>kitt engines check vllm\nkitt engines check ollama\n</code></pre> <p>If the image has not been pulled, the output includes a <code>Fix:</code> hint with the exact setup command. If Docker itself is not running, you will see a link to the Docker installation guide.</p>"},{"location":"guides/engines/#pulling-images","title":"Pulling Images","text":"<p>Download the Docker image for an engine:</p> <pre><code>kitt engines setup vllm\nkitt engines setup ollama\n</code></pre> <p>Use <code>--dry-run</code> to see the <code>docker pull</code> command without executing it:</p> <pre><code>kitt engines setup --dry-run tgi\n</code></pre>"},{"location":"guides/engines/#model-format-compatibility","title":"Model Format Compatibility","text":"<p>Engines are divided into two groups by the model formats they accept:</p> <p>safetensors / pytorch -- vLLM and TGI load models in their native HuggingFace format. Point the <code>--model</code> flag at a directory containing <code>model.safetensors</code> or <code>pytorch_model.bin</code> files (or a HuggingFace repo ID for engines that support it).</p> <p>gguf -- llama.cpp and Ollama load quantized GGUF files. Point <code>--model</code> at a single <code>.gguf</code> file or a directory containing one. Ollama also accepts its own tag syntax (e.g. <code>llama3.1:8b</code>).</p> <p>Attempting to load a safetensors model in llama.cpp (or a GGUF file in vLLM) will fail at container startup with a clear error message.</p>"},{"location":"guides/engines/#custom-engine-configuration","title":"Custom Engine Configuration","text":"<p>Override engine defaults by writing a YAML config and passing it with <code>--config</code>:</p> <pre><code>kitt run -m /models/llama2-7b -e vllm --config ./my-engine.yaml\n</code></pre> <p>Engine config files live in <code>configs/engines/</code> and follow this structure:</p> <pre><code># configs/engines/vllm.yaml\nname: vllm\nimage: vllm/vllm-openai:latest\nport: 8000\nhealth_endpoint: /health\nenv:\n  VLLM_ATTENTION_BACKEND: FLASH_ATTN\nextra_args:\n  - --max-model-len\n  - \"4096\"\n</code></pre> <p>Each engine's built-in config is in <code>configs/engines/&lt;key&gt;.yaml</code>. You can copy one of these as a starting point and adjust image tags, environment variables, or extra CLI arguments passed to the engine server inside the container.</p>"},{"location":"guides/engines/#engine-lifecycle","title":"Engine Lifecycle","text":"<p>When you run <code>kitt run</code>, KITT automatically:</p> <ol> <li>Starts a Docker container with <code>--gpus all</code> and <code>--network host</code>.</li> <li>Mounts the model directory into the container.</li> <li>Polls the health endpoint with exponential backoff (up to 300 seconds).</li> <li>Runs benchmarks via the engine's HTTP API on <code>localhost:&lt;port&gt;</code>.</li> <li>Stops and removes the container when benchmarks finish.</li> </ol> <p>Container names follow the pattern <code>kitt-&lt;timestamp&gt;</code> so they are easy to identify in <code>docker ps</code> output.</p>"},{"location":"guides/monitoring/","title":"Monitoring","text":"<p>KITT includes a Prometheus + Grafana + InfluxDB monitoring stack for tracking benchmark and system metrics over time. You can run the built-in stack locally, generate customized stacks with specific scrape targets, and deploy them to remote hosts.</p>"},{"location":"guides/monitoring/#local-stack","title":"Local Stack","text":"<p>Start the built-in monitoring stack from the <code>docker/monitoring/</code> directory:</p> <pre><code>kitt monitoring start\nkitt monitoring status\nkitt monitoring stop\n</code></pre> <p>Target a named generated stack instead of the built-in one:</p> <pre><code>kitt monitoring start --name lab\nkitt monitoring stop --name lab\n</code></pre>"},{"location":"guides/monitoring/#generating-a-custom-stack","title":"Generating a Custom Stack","text":"<p>Create a monitoring stack with custom Prometheus scrape targets:</p> <pre><code>kitt monitoring generate &lt;name&gt; -t &lt;host:port&gt; [-t &lt;host:port&gt; ...] [OPTIONS]\n</code></pre> <p>Generated stacks are stored at <code>~/.kitt/monitoring/&lt;name&gt;/</code>.</p> Option Default Description <code>-t</code> / <code>--target</code> (required) Scrape target <code>host:port</code> (repeatable) <code>--grafana-port</code> 3000 Grafana dashboard port <code>--prometheus-port</code> 9090 Prometheus port <code>--influxdb-port</code> 8086 InfluxDB port <code>--grafana-password</code> kitt Grafana admin password <code>--influxdb-token</code> (auto) InfluxDB admin token <p>Example:</p> <pre><code>kitt monitoring generate lab -t 192.168.1.10:9100 -t 192.168.1.11:9100\n</code></pre>"},{"location":"guides/monitoring/#port-configuration","title":"Port Configuration","text":"<p>Override default ports when generating a stack:</p> <pre><code>kitt monitoring generate lab \\\n  -t 10.0.0.5:9100 \\\n  --grafana-port 3001 \\\n  --prometheus-port 9091 \\\n  --influxdb-port 8087\n</code></pre>"},{"location":"guides/monitoring/#credentials","title":"Credentials","text":"<p>Set custom Grafana and InfluxDB credentials at generation time:</p> <pre><code>kitt monitoring generate lab \\\n  -t 10.0.0.5:9100 \\\n  --grafana-password my-secret \\\n  --influxdb-token my-influx-token\n</code></pre>"},{"location":"guides/monitoring/#remote-deployment","title":"Remote Deployment","text":"<p>Deploy a generated stack to a remote host and manage its lifecycle over SSH. Remote hosts are configured in <code>~/.kitt/hosts.yaml</code>.</p>"},{"location":"guides/monitoring/#deploy","title":"Deploy","text":"<p>Upload the generated stack to a remote host:</p> <pre><code>kitt monitoring deploy lab --host dgx01\n</code></pre> <p>You can also deploy immediately after generation:</p> <pre><code>kitt monitoring generate lab -t 10.0.0.5:9100 --deploy --host dgx01\n</code></pre>"},{"location":"guides/monitoring/#remote-start-stop-status","title":"Remote Start / Stop / Status","text":"<pre><code>kitt monitoring remote-start lab --host dgx01\nkitt monitoring remote-stop lab --host dgx01\nkitt monitoring remote-status lab --host dgx01\n</code></pre>"},{"location":"guides/monitoring/#stack-management","title":"Stack Management","text":""},{"location":"guides/monitoring/#list-stacks","title":"List Stacks","text":"<pre><code>kitt monitoring list-stacks\n</code></pre>"},{"location":"guides/monitoring/#remove-a-stack","title":"Remove a Stack","text":"<pre><code>kitt monitoring remove-stack lab\nkitt monitoring remove-stack lab --delete-files\n</code></pre>"},{"location":"guides/monitoring/#example-workflow","title":"Example Workflow","text":"<p>A complete workflow from generation to remote teardown:</p> <pre><code># Generate a stack targeting two hosts\nkitt monitoring generate lab -t 192.168.1.10:9100 -t 192.168.1.11:9100\n\n# Deploy to a remote DGX host\nkitt monitoring deploy lab --host dgx01\n\n# Check status on the remote host\nkitt monitoring remote-status lab --host dgx01\n\n# View Grafana dashboards\n# Open http://dgx01:3000 in your browser (admin / kitt)\n\n# Stop when done\nkitt monitoring remote-stop lab --host dgx01\n\n# Clean up\nkitt monitoring remove-stack lab --delete-files\n</code></pre>"},{"location":"guides/plugins/","title":"Plugin System","text":"<p>KITT uses a plugin architecture for both inference engines and benchmarks. The built-in engines and benchmarks are registered the same way external plugins are, so the extension mechanism is consistent throughout the codebase.</p>"},{"location":"guides/plugins/#engine-plugins","title":"Engine Plugins","text":"<p>Every engine extends the <code>InferenceEngine</code> abstract base class and is registered with <code>@register_engine</code>:</p> <pre><code>from kitt.engines.base import InferenceEngine\nfrom kitt.engines.registry import register_engine\n\n\n@register_engine\nclass MyEngine(InferenceEngine):\n    @classmethod\n    def name(cls) -&gt; str:\n        return \"my-engine\"\n\n    @classmethod\n    def supported_formats(cls) -&gt; list[str]:\n        return [\"safetensors\"]\n\n    @classmethod\n    def default_image(cls) -&gt; str:\n        return \"myorg/my-engine:latest\"\n\n    @classmethod\n    def default_port(cls) -&gt; int:\n        return 8000\n\n    @classmethod\n    def container_port(cls) -&gt; int:\n        return 8000\n\n    @classmethod\n    def health_endpoint(cls) -&gt; str:\n        return \"/health\"\n\n    def initialize(self, model_path: str, config: dict) -&gt; None:\n        ...\n\n    def generate(self, prompt: str, **kwargs) -&gt; \"GenerationResult\":\n        ...\n</code></pre> <p>The <code>InferenceEngine</code> ABC requires you to implement:</p> <ul> <li><code>name()</code> -- unique engine identifier</li> <li><code>supported_formats()</code> -- model file formats the engine can load</li> <li><code>default_image()</code> / <code>default_port()</code> / <code>container_port()</code> -- Docker settings</li> <li><code>health_endpoint()</code> -- path the health check probes</li> <li><code>initialize()</code> -- start the Docker container and wait for health</li> <li><code>generate()</code> -- send a prompt and return a <code>GenerationResult</code></li> </ul> <p>The <code>cleanup()</code> method is provided by the base class and stops the container.</p>"},{"location":"guides/plugins/#benchmark-plugins","title":"Benchmark Plugins","text":"<p>Benchmarks extend <code>LLMBenchmark</code> and register with <code>@register_benchmark</code>:</p> <pre><code>from kitt.benchmarks.base import BenchmarkResult, LLMBenchmark\nfrom kitt.benchmarks.registry import register_benchmark\n\n\n@register_benchmark\nclass MyBenchmark(LLMBenchmark):\n    name = \"my-benchmark\"\n    version = \"1.0.0\"\n    category = \"quality_custom\"\n    description = \"Measures something useful.\"\n\n    def _execute(self, engine, config: dict) -&gt; BenchmarkResult:\n        result = engine.generate(prompt=\"Hello\", max_tokens=50)\n        return BenchmarkResult(\n            test_name=self.name,\n            test_version=self.version,\n            passed=True,\n            metrics={\"tokens_generated\": result.completion_tokens},\n            outputs=[result.output],\n        )\n</code></pre> <p>The base class provides the <code>run()</code> method which handles warmup iterations before calling your <code>_execute()</code> implementation.</p>"},{"location":"guides/plugins/#plugin-discovery","title":"Plugin Discovery","text":"<p>KITT discovers external plugins through Python entry points. Declare your plugin in <code>pyproject.toml</code>:</p> <pre><code>[project.entry-points.\"kitt.engines\"]\nmy-engine = \"my_package.engine:MyEngine\"\n\n[project.entry-points.\"kitt.benchmarks\"]\nmy-benchmark = \"my_package.benchmark:MyBenchmark\"\n</code></pre> <p>Supported entry point groups:</p> Group Class Base <code>kitt.engines</code> <code>InferenceEngine</code> <code>kitt.benchmarks</code> <code>LLMBenchmark</code> <code>kitt.reporters</code> Reporter classes <p>When KITT calls <code>EngineRegistry.auto_discover()</code> or <code>BenchmarkRegistry.auto_discover()</code>, it first registers all built-in implementations and then scans entry points for external plugins.</p>"},{"location":"guides/plugins/#managing-plugins-with-the-cli","title":"Managing Plugins with the CLI","text":"<p>Install a third-party plugin package:</p> <pre><code>kitt plugin install kitt-engine-triton\n</code></pre> <p>List installed plugins and discovered classes:</p> <pre><code>kitt plugin list\n</code></pre> <p>Remove a plugin:</p> <pre><code>kitt plugin remove kitt-engine-triton\n</code></pre> <p>After installation, the new engine or benchmark is immediately available in <code>kitt run</code>, <code>kitt engines list</code>, and <code>kitt test list</code>.</p>"},{"location":"guides/plugins/#plugin-file-locations","title":"Plugin File Locations","text":"Location Purpose <code>src/kitt/engines/</code> Built-in engine implementations <code>src/kitt/benchmarks/performance/</code> Built-in performance benchmarks <code>src/kitt/benchmarks/quality/standard/</code> Built-in quality benchmarks <code>src/kitt/plugins/discovery.py</code> Entry point scanning logic <code>src/kitt/plugins/installer.py</code> <code>pip install</code> / <code>pip uninstall</code> wrapper <code>src/kitt/plugins/validator.py</code> Plugin compatibility checks"},{"location":"guides/plugins/#tips-for-plugin-authors","title":"Tips for Plugin Authors","text":"<ol> <li>Always subclass the ABC (<code>InferenceEngine</code> or <code>LLMBenchmark</code>) so KITT can    validate your plugin at registration time.</li> <li>Use the decorator (<code>@register_engine</code> / <code>@register_benchmark</code>) if your    module will be imported directly, or declare an entry point if the plugin    is distributed as a separate package.</li> <li>Test your plugin in isolation with <code>kitt engines check &lt;name&gt;</code> or    <code>kitt test list --category quality_custom</code> before running full suites.</li> </ol>"},{"location":"guides/remote-execution/","title":"Remote Execution","text":"<p>KITT can run benchmark campaigns on remote GPU servers over SSH. This is useful when you have multiple machines and want to coordinate work from a single workstation without deploying the full agent daemon.</p>"},{"location":"guides/remote-execution/#how-it-works","title":"How It Works","text":"<p>The <code>kitt remote</code> commands manage a registry of remote hosts stored in <code>~/.kitt/hosts.yaml</code>. KITT connects via SSH to upload campaign configs, start benchmarks, stream logs, and sync results back to your local machine.</p>"},{"location":"guides/remote-execution/#setting-up-a-remote-host","title":"Setting Up a Remote Host","text":"<pre><code>kitt remote setup user@gpu-server-01 --name dgx01\n</code></pre> <p>This command:</p> <ol> <li>Opens an SSH connection to the host.</li> <li>Checks prerequisites -- Python version, Docker availability, GPU info.</li> <li>Optionally installs KITT on the remote host (skip with <code>--no-install</code>).</li> <li>Saves the host configuration to <code>~/.kitt/hosts.yaml</code>.</li> </ol> <p>You can also supply a specific SSH key:</p> <pre><code>kitt remote setup user@gpu-server-01 --name dgx01 --ssh-key ~/.ssh/id_ed25519\n</code></pre>"},{"location":"guides/remote-execution/#host-configuration","title":"Host Configuration","text":"<p>Hosts are stored in <code>~/.kitt/hosts.yaml</code> with the following structure:</p> <pre><code>hosts:\n  dgx01:\n    name: dgx01\n    hostname: gpu-server-01\n    user: user\n    ssh_key: ~/.ssh/id_ed25519\n    port: 22\n    kitt_path: ~/.local/bin/kitt\n    storage_path: ~/kitt-results\n    gpu_info: NVIDIA GH200\n    gpu_count: 1\n    python_version: \"3.11\"\n</code></pre> <p>List all configured hosts:</p> <pre><code>kitt remote list\n</code></pre> <p>Test connectivity:</p> <pre><code>kitt remote test dgx01\n</code></pre> <p>Remove a host:</p> <pre><code>kitt remote remove dgx01\n</code></pre>"},{"location":"guides/remote-execution/#running-campaigns-remotely","title":"Running Campaigns Remotely","text":"<p>Upload a campaign config and start it on a remote host:</p> <pre><code>kitt remote run campaign.yaml --host dgx01\n</code></pre> <p>Add <code>--wait</code> to block until the campaign finishes:</p> <pre><code>kitt remote run campaign.yaml --host dgx01 --wait\n</code></pre> <p>Use <code>--dry-run</code> to validate the config without executing benchmarks.</p>"},{"location":"guides/remote-execution/#monitoring-remote-campaigns","title":"Monitoring Remote Campaigns","text":"<p>Check the status of a running campaign:</p> <pre><code>kitt remote status --host dgx01\n</code></pre> <p>View live logs:</p> <pre><code>kitt remote logs --host dgx01 --tail 100\n</code></pre>"},{"location":"guides/remote-execution/#syncing-results","title":"Syncing Results","text":"<p>After a campaign completes, pull results to your local machine:</p> <pre><code>kitt remote sync --host dgx01\n</code></pre> <p>By default results are saved to the local KITT results directory. Override with <code>--output</code>:</p> <pre><code>kitt remote sync --host dgx01 --output ./results/dgx01\n</code></pre>"},{"location":"guides/remote-execution/#coordinating-distributed-benchmarks","title":"Coordinating Distributed Benchmarks","text":"<p>To run the same campaign across multiple hosts, script the <code>kitt remote run</code> command for each host:</p> <pre><code>for host in dgx01 dgx02 dgx03; do\n    kitt remote run campaign.yaml --host \"$host\" &amp;\ndone\nwait\nfor host in dgx01 dgx02 dgx03; do\n    kitt remote sync --host \"$host\" --output \"./results/$host\"\ndone\n</code></pre> <p>Then use <code>kitt results compare</code> or <code>kitt compare</code> to analyze results across machines.</p>"},{"location":"guides/results/","title":"Results &amp; KARR","text":"<p>KARR (Kitt's AI Results Repository) persists all benchmark results. The current backend is a relational database (SQLite by default, PostgreSQL for production). Flat JSON files are still written for convenience, and the legacy Git-backed backend remains available.</p>"},{"location":"guides/results/#database-storage-default","title":"Database Storage (Default)","text":""},{"location":"guides/results/#initialize-the-database","title":"Initialize the Database","text":"<p>Create tables in the default SQLite database (<code>~/.kitt/kitt.db</code>) or in a PostgreSQL instance pointed to by <code>KITT_DB_DSN</code>:</p> <pre><code>kitt storage init\n</code></pre> <p>Results are saved to the database automatically after every <code>kitt run</code>. No extra flags are needed.</p>"},{"location":"guides/results/#import-existing-json-results","title":"Import Existing JSON Results","text":"<p>Bring previously exported or flat-file results into KARR:</p> <pre><code>kitt storage import ./kitt-results/run1/metrics.json\nkitt storage import ./kitt-results/               # imports all runs found\n</code></pre>"},{"location":"guides/results/#export-results","title":"Export Results","text":"<p>Export runs from KARR back to JSON files:</p> <pre><code>kitt storage export --output ./export/\nkitt storage export --model llama --engine vllm --output ./export/\n</code></pre>"},{"location":"guides/results/#list-stored-runs","title":"List Stored Runs","text":"<p>Browse what is stored in KARR with optional filters:</p> <pre><code>kitt storage list\nkitt storage list --model llama --engine vllm --limit 20\n</code></pre> <p>Output includes run ID, model, engine, suite, timestamp, and pass/fail counts.</p>"},{"location":"guides/results/#database-statistics","title":"Database Statistics","text":"<p>Get a high-level summary of KARR contents:</p> <pre><code>kitt storage stats\n</code></pre> <p>This shows total runs, unique models, unique engines, date range, and storage size.</p>"},{"location":"guides/results/#querying-results","title":"Querying Results","text":"<p>The <code>ResultStore</code> interface supports filtered queries and aggregation from the CLI or programmatically.</p>"},{"location":"guides/results/#filter-by-model-engine-or-suite","title":"Filter by Model, Engine, or Suite","text":"<pre><code>kitt storage list --model \"Llama-3.1-8B\" --engine vllm\nkitt storage list --suite performance --limit 5\n</code></pre>"},{"location":"guides/results/#aggregation","title":"Aggregation","text":"<p>Group results by model or engine to compare averages:</p> <pre><code>kitt storage stats --group-by model\nkitt storage stats --group-by engine\n</code></pre> <p>This is useful for spotting regressions across a fleet of models or engines.</p>"},{"location":"guides/results/#schema-migrations","title":"Schema Migrations","text":"<p>When upgrading KITT, apply any pending database migrations:</p> <pre><code>kitt storage migrate\n</code></pre> <p>The current schema version is 2. See the Database Schema Reference for full table documentation.</p>"},{"location":"guides/results/#flat-file-output","title":"Flat File Output","text":"<p>Every <code>kitt run</code> writes JSON results to a <code>kitt-results/</code> directory in the current working directory, regardless of database settings. These files are handy for:</p> <ul> <li>Quick inspection with <code>jq</code> or a text editor</li> <li>Archiving to external storage</li> <li>Sharing individual run data without database access</li> </ul> <p>The flat files mirror the content stored in <code>runs.raw_json</code> in the database.</p>"},{"location":"guides/results/#comparing-results","title":"Comparing Results","text":""},{"location":"guides/results/#cli-comparison","title":"CLI Comparison","text":"<p>Compare metrics across two or more benchmark runs:</p> <pre><code>kitt results compare ./run1 ./run2\nkitt results compare ./run1 ./run2 --additional ./run3 --format json\n</code></pre> <p>The table output shows min, max, average, standard deviation, and coefficient of variation for each metric. Paths can point to flat-file result directories or exported database runs.</p>"},{"location":"guides/results/#interactive-tui","title":"Interactive TUI","text":"<p>Launch a side-by-side terminal comparison (requires the <code>cli_ui</code> extra):</p> <pre><code>kitt compare ./run1 ./run2\n</code></pre> <p>Both <code>kitt results compare</code> and <code>kitt compare</code> work with flat-file directories and database-exported results interchangeably.</p>"},{"location":"guides/results/#git-backed-storage-legacy","title":"Git-Backed Storage (Legacy)","text":"<p>Legacy Backend</p> <p>Git-backed KARR storage is the previous generation (Gen 2). It remains available via <code>--store-karr</code> for backward compatibility, but the database backend is recommended for all new deployments.</p> <p>The previous generation of KARR stored results in a Git repository with LFS tracking. To use it, add <code>--store-karr</code> to a run:</p> <pre><code>kitt results init --path ./my-results\nkitt run -m /models/llama-7b -e vllm -s standard --store-karr ./my-results\nkitt results list --karr ./my-results\n</code></pre>"},{"location":"guides/results/#directory-structure-gen-2","title":"Directory Structure (Gen 2)","text":"<pre><code>karr-&lt;fingerprint&gt;/\n  &lt;model&gt;/\n    &lt;engine&gt;/\n      &lt;timestamp&gt;/\n        metrics.json\n        summary.md\n        hardware.json\n        config.json\n        outputs/          # compressed .jsonl.gz, tracked by Git LFS\n</code></pre> <p>For Docker-based deployments, Git repos are cumbersome to mount and manage. The database backend removes this friction entirely.</p>"},{"location":"guides/results/#next-steps","title":"Next Steps","text":"<ul> <li>KARR \u2014 Results Storage -- architecture and design decisions</li> <li>Database Schema Reference -- full table and column documentation</li> <li>Hardware Fingerprinting -- how system identity is captured</li> </ul>"},{"location":"guides/web-dashboard/","title":"Web Dashboard","text":"<p>The KITT web dashboard provides a browser-based UI for browsing results, managing agents, running campaigns, and interacting with the REST API. It requires the <code>web</code> extra (<code>poetry install -E web</code>).</p>"},{"location":"guides/web-dashboard/#launching-the-dashboard","title":"Launching the Dashboard","text":"<pre><code>kitt web --port 8080 --results-dir ./results\n</code></pre> <p>The full set of options:</p> Option Default Description <code>--port</code> 8080 Port to serve on <code>--host</code> 0.0.0.0 Host to bind to <code>--results-dir</code> current directory Path to results directory <code>--debug</code> off Enable Flask debug mode with auto-reload <code>--legacy</code> off Use the legacy read-only dashboard <code>--insecure</code> off Disable TLS (development only) <code>--tls-cert</code> auto Path to TLS certificate <code>--tls-key</code> auto Path to TLS private key <code>--tls-ca</code> auto Path to CA certificate <code>--auth-token</code> none Bearer token for API authentication"},{"location":"guides/web-dashboard/#tls-configuration","title":"TLS Configuration","text":"<p>By default, KITT auto-generates a self-signed CA and server certificate on first launch. The CA fingerprint is printed to the console so agents and clients can verify the server identity.</p> <p>Custom certificates: Supply your own with <code>--tls-cert</code> and <code>--tls-key</code>:</p> <pre><code>kitt web --tls-cert /path/to/cert.pem --tls-key /path/to/key.pem\n</code></pre> <p>Development mode: Disable TLS entirely with <code>--insecure</code>. This is intended only for local development -- do not use it in production:</p> <pre><code>kitt web --insecure --debug\n</code></pre>"},{"location":"guides/web-dashboard/#authentication","title":"Authentication","text":"<p>Enable API authentication with <code>--auth-token</code>. Clients must include <code>Authorization: Bearer &lt;token&gt;</code> in API requests:</p> <pre><code>kitt web --auth-token my-secret-token\n</code></pre> <p>The token can also be set via the <code>KITT_AUTH_TOKEN</code> environment variable.</p>"},{"location":"guides/web-dashboard/#legacy-mode","title":"Legacy Mode","text":"<p>The legacy dashboard is a read-only single-page viewer from KITT v1. It scans <code>kitt-results/</code> and legacy <code>karr-*</code> directories for <code>metrics.json</code> files and renders a summary table:</p> <pre><code>kitt web --legacy --results-dir ./kitt-results\n</code></pre> <p>Legacy mode does not require a database and does not support agents, campaigns, or the REST API beyond basic result listing.</p>"},{"location":"guides/web-dashboard/#rest-api-endpoints","title":"REST API Endpoints","text":"<p>The full dashboard registers API blueprints under <code>/api/v1/</code>:</p> Endpoint Description <code>GET /api/v1/health</code> Health check <code>GET /api/v1/results</code> List and query benchmark results <code>GET /api/v1/agents</code> List registered agents <code>POST /api/v1/agents/register</code> Agent registration <code>GET /api/v1/agents/&lt;id&gt;/settings</code> Get agent settings <code>PUT /api/v1/agents/&lt;id&gt;/settings</code> Update agent settings <code>POST /api/v1/agents/&lt;id&gt;/cleanup</code> Trigger storage cleanup <code>GET /api/v1/campaigns</code> List campaigns <code>POST /api/v1/campaigns</code> Create a new campaign <code>GET /api/v1/models</code> List known models <code>POST /api/v1/quicktest</code> Submit a quick benchmark run <code>GET /api/v1/events</code> Server-sent events for live updates <p>All mutable endpoints require a valid <code>Authorization</code> header when <code>--auth-token</code> is set.</p>"},{"location":"guides/web-dashboard/#quick-test","title":"Quick Test","text":"<p>Quick Test lets you run a single benchmark on a remote agent directly from the browser. The feature has three pages:</p> <p>History (<code>/quicktest</code>): Lists all past and in-progress tests with status filter chips (queued, dispatched, running, completed, failed). Shows model, engine, agent, status badge, and creation time. Click any row to open the detail page.</p> <p>New Test (<code>/quicktest/new</code>): A searchable model dropdown loads models from Devon's <code>manifest.json</code> in the configured model directory. Type to filter by model name with fuzzy substring matching, or enter a custom path manually. Select an agent, engine, and benchmark, then launch. The page redirects to the detail view automatically.</p> <p>Detail (<code>/quicktest/&lt;id&gt;</code>): Displays test metadata (agent, engine, benchmark, suite, timestamps) and log output. For active tests (queued, dispatched, running), the page subscribes to <code>/api/v1/events/stream/&lt;test_id&gt;</code> via SSE and streams log lines in real time. For completed or failed tests, stored log lines are loaded from the database. This lets you navigate away and return to view full logs later.</p>"},{"location":"guides/web-dashboard/#settings","title":"Settings","text":"<p>The Settings page lets you configure key paths and integrations directly from the web UI without restarting the server:</p> Setting Environment Variable Default Model Directory <code>KITT_MODEL_DIR</code> <code>~/.kitt/models</code> Devon URL <code>DEVON_URL</code> (none) Results Directory <code>--results-dir</code> CLI flag Current directory <p>Values saved through the UI are stored in the database and take priority over environment variables. Clearing a field reverts to the environment variable or default. Changes take effect immediately without a restart.</p> <p>The Devon URL can also be configured inline on the Devon page when it hasn't been set yet.</p>"},{"location":"guides/web-dashboard/#agent-settings","title":"Agent Settings","text":"<p>Per-agent settings are configured on each agent's detail page, not the global Settings page. Navigate to Agents &gt; (agent name) to find the Settings card. These settings are synced to the agent via the heartbeat response:</p> Setting Description Model Storage Directory Local directory for model copies Model Share Source (NFS) NFS share source (e.g., <code>nas:/volume1/models</code>) Model Share Mount Point Local mount point for the NFS share Auto Cleanup Delete local model copies after benchmarks Heartbeat Interval Seconds between heartbeats (10-300)"},{"location":"guides/web-dashboard/#storage-monitoring","title":"Storage Monitoring","text":"<p>The agent detail page also shows a Storage card with the current disk usage from heartbeat data. The \"Clean Storage\" button queues a <code>cleanup_storage</code> command that the agent will pick up on its next heartbeat, deleting all cached models from local storage.</p>"},{"location":"guides/web-dashboard/#database","title":"Database","text":"<p>The full dashboard uses SQLite stored at <code>~/.kitt/kitt.db</code>. Schema migrations run automatically on startup. The database tracks agents, campaigns, and indexed results.</p>"},{"location":"reference/","title":"Reference","text":"<p>Technical reference material for KITT. This section provides detailed specifications for the CLI, configuration file schemas, Docker setup, REST API, and environment variables.</p> <p>Use these pages when you need exact field names, endpoint paths, or default values. For guided walkthroughs, see the Getting Started section instead.</p>"},{"location":"reference/#contents","title":"Contents","text":"<ul> <li> <p>CLI Reference -- Complete command and option listing,   auto-generated from Click decorators.</p> </li> <li> <p>Configuration Files -- YAML schema reference   for suites, engines, campaigns, and custom benchmarks.</p> <ul> <li>Suite Configuration</li> <li>Engine Configuration</li> <li>Campaign Configuration</li> <li>Custom Benchmark Configuration</li> </ul> </li> <li> <p>Docker Files -- Dockerfile stages, docker-compose   services, GPU passthrough, and network requirements.</p> </li> <li> <p>REST API -- Endpoints exposed by <code>kitt web</code>, including   authentication, request/response formats, and SSE streaming.</p> </li> <li> <p>Environment Variables -- Every environment variable   KITT reads, with descriptions and default values.</p> </li> </ul>"},{"location":"reference/api/","title":"REST API","text":"<p>KITT exposes a REST API through the <code>kitt web</code> command. The API is served by Flask and provides endpoints for managing results, agents, campaigns, models, quick tests, and real-time event streaming.</p>"},{"location":"reference/api/#base-url","title":"Base URL","text":"<pre><code>https://&lt;host&gt;:&lt;port&gt;/api/v1/\n</code></pre> <p>Default: <code>https://0.0.0.0:8080/api/v1/</code></p> <p>TLS is enabled by default. Pass <code>--insecure</code> to disable it during development. Pass <code>--legacy</code> for the read-only v1 dashboard which does not require TLS.</p>"},{"location":"reference/api/#authentication","title":"Authentication","text":"<p>Protected endpoints require a Bearer token in the <code>Authorization</code> header:</p> <pre><code>Authorization: Bearer &lt;token&gt;\n</code></pre> <p>Set the token with <code>--auth-token</code> on the CLI or the <code>KITT_AUTH_TOKEN</code> environment variable. Agent registration, heartbeat, and result-reporting endpoints all require authentication.</p>"},{"location":"reference/api/#endpoints","title":"Endpoints","text":""},{"location":"reference/api/#health-and-version","title":"Health and version","text":"Method Path Auth Description GET <code>/api/v1/health</code> No Health check (returns <code>{\"status\": \"ok\"}</code>) GET <code>/api/v1/version</code> No Version info GET <code>/api/health</code> No Legacy health endpoint"},{"location":"reference/api/#results","title":"Results","text":"Method Path Auth Description GET <code>/api/v1/results/</code> No List results (query: <code>model</code>, <code>engine</code>, <code>suite_name</code>, <code>page</code>, <code>per_page</code>) GET <code>/api/v1/results/&lt;id&gt;</code> No Get a single result DELETE <code>/api/v1/results/&lt;id&gt;</code> Yes Delete a result GET <code>/api/v1/results/aggregate</code> Yes Aggregate results (query: <code>group_by</code>, <code>metric</code>) POST <code>/api/v1/results/compare</code> Yes Compare results (body: <code>{\"ids\": [...]}</code>)"},{"location":"reference/api/#agents","title":"Agents","text":"Method Path Auth Description GET <code>/api/v1/agents/</code> No List all agents GET <code>/api/v1/agents/&lt;id&gt;</code> No Get agent details POST <code>/api/v1/agents/register</code> Yes Register a new agent POST <code>/api/v1/agents/&lt;id&gt;/heartbeat</code> Yes Agent heartbeat (response includes <code>settings</code>) POST <code>/api/v1/agents/&lt;id&gt;/results</code> Yes Report benchmark result PATCH <code>/api/v1/agents/&lt;id&gt;</code> Yes Update agent fields DELETE <code>/api/v1/agents/&lt;id&gt;</code> Yes Remove an agent GET <code>/api/v1/agents/&lt;id&gt;/settings</code> Yes Get agent settings PUT <code>/api/v1/agents/&lt;id&gt;/settings</code> Yes Update agent settings (body: <code>{\"key\": \"value\", ...}</code>) POST <code>/api/v1/agents/&lt;id&gt;/cleanup</code> Yes Queue storage cleanup command GET <code>/api/v1/agent/install.sh</code> No Agent bootstrap install script GET <code>/api/v1/agent/package</code> No Agent package tarball GET <code>/api/v1/agent/package/sha256</code> No SHA-256 of agent package GET <code>/api/v1/agent/build-context</code> No Docker build context tarball GET <code>/api/v1/agent/build-context/sha256</code> No SHA-256 of build context"},{"location":"reference/api/#campaigns","title":"Campaigns","text":"Method Path Auth Description GET <code>/api/v1/campaigns/</code> No List campaigns (query: <code>status</code>, <code>page</code>, <code>per_page</code>) POST <code>/api/v1/campaigns/</code> Yes Create a campaign GET <code>/api/v1/campaigns/&lt;id&gt;</code> No Get campaign details DELETE <code>/api/v1/campaigns/&lt;id&gt;</code> Yes Delete a campaign POST <code>/api/v1/campaigns/&lt;id&gt;/launch</code> Yes Launch a campaign POST <code>/api/v1/campaigns/&lt;id&gt;/cancel</code> Yes Cancel a running campaign PUT <code>/api/v1/campaigns/&lt;id&gt;/config</code> Yes Update campaign config (draft only)"},{"location":"reference/api/#models","title":"Models","text":"Method Path Auth Description GET <code>/api/v1/models/search</code> No Search models via Devon (query: <code>q</code>, <code>limit</code>) GET <code>/api/v1/models/local</code> No List locally available models POST <code>/api/v1/models/download</code> Yes Download a model (body: <code>{\"repo_id\": \"...\"}</code>) DELETE <code>/api/v1/models/&lt;repo_id&gt;</code> Yes Remove a local model"},{"location":"reference/api/#quick-tests","title":"Quick tests","text":"Method Path Auth Description GET <code>/api/v1/quicktest/models</code> No List available models from Devon manifest GET <code>/api/v1/quicktest/</code> No List quick tests (query: <code>status</code>, <code>agent_name</code>, <code>page</code>, <code>per_page</code>) POST <code>/api/v1/quicktest/</code> Yes Launch a quick test (body: <code>agent_id</code>, <code>model_path</code>, <code>engine_name</code>) GET <code>/api/v1/quicktest/&lt;id&gt;</code> No Get quick test status GET <code>/api/v1/quicktest/&lt;id&gt;/logs</code> No Get stored log lines for a test POST <code>/api/v1/quicktest/&lt;id&gt;/logs</code> Yes Post a log line from the agent (body: <code>line</code>) POST <code>/api/v1/quicktest/&lt;id&gt;/status</code> Yes Update test status (body: <code>status</code>: <code>running</code>/<code>completed</code>/<code>failed</code>)"},{"location":"reference/api/#events-sse","title":"Events (SSE)","text":"Method Path Auth Description GET <code>/api/v1/events/stream</code> No Global SSE event stream GET <code>/api/v1/events/stream/&lt;source_id&gt;</code> No Filtered SSE stream by source"},{"location":"reference/api/#response-format","title":"Response format","text":"<p>All endpoints return JSON. List endpoints support pagination with <code>page</code> and <code>per_page</code> query parameters and return results in an envelope:</p> <pre><code>{\n  \"items\": [...],\n  \"total\": 42,\n  \"page\": 1,\n  \"per_page\": 25\n}\n</code></pre> <p>Error responses use a standard structure:</p> <pre><code>{\n  \"error\": \"Description of the problem\"\n}\n</code></pre> <p>HTTP status codes follow REST conventions: 200 for success, 201 for created, 202 for accepted (async), 400 for bad request, 401 for unauthorized, 403 for forbidden, 404 for not found.</p>"},{"location":"reference/database/","title":"Database Schema Reference","text":"<p>This is the schema reference for KARR's database backend. KARR uses a relational database to store benchmark results, agent state, campaign progress, and event logs. The schema is versioned and managed through an automatic migration system.</p> <p>Current schema version: 8</p>"},{"location":"reference/database/#migration-system","title":"Migration System","text":"<p>The <code>schema_version</code> table tracks which migrations have been applied:</p> Column Type Description <code>version</code> INTEGER Migration version number <code>applied_at</code> TEXT ISO-8601 timestamp when the migration was applied <p>Run <code>kitt storage migrate</code> to apply any pending migrations. Migrations are forward-only; downgrades are not supported.</p>"},{"location":"reference/database/#tables","title":"Tables","text":""},{"location":"reference/database/#runs","title":"runs","text":"<p>Primary table for benchmark run results. Each row represents one complete <code>kitt run</code> execution.</p> Column Type Constraints Description <code>id</code> TEXT PRIMARY KEY Unique run identifier (UUID) <code>model</code> TEXT Model path or name <code>engine</code> TEXT Inference engine used <code>suite_name</code> TEXT Test suite that was executed <code>timestamp</code> TEXT ISO-8601 run start time <code>passed</code> INTEGER 1 if the run passed overall, 0 otherwise <code>total_benchmarks</code> INTEGER Number of benchmarks executed <code>passed_count</code> INTEGER Number of benchmarks that passed <code>failed_count</code> INTEGER Number of benchmarks that failed <code>total_time_seconds</code> REAL Wall-clock duration of the entire run <code>kitt_version</code> TEXT KITT version that produced the result <code>raw_json</code> TEXT Full JSON output for lossless round-tripping <code>created_at</code> TEXT ISO-8601 timestamp when the row was inserted"},{"location":"reference/database/#benchmarks","title":"benchmarks","text":"<p>Individual benchmark results within a run.</p> Column Type Constraints Description <code>id</code> INTEGER PRIMARY KEY AUTOINCREMENT Row identifier <code>run_id</code> TEXT FOREIGN KEY -&gt; runs(id) ON DELETE CASCADE Parent run <code>test_name</code> TEXT Benchmark name (e.g., <code>throughput</code>, <code>mmlu</code>) <code>test_version</code> TEXT Benchmark version string <code>run_number</code> INTEGER Iteration number within the suite <code>passed</code> INTEGER 1 if the benchmark passed, 0 otherwise <code>timestamp</code> TEXT ISO-8601 benchmark start time <code>created_at</code> TEXT Row insertion timestamp"},{"location":"reference/database/#metrics","title":"metrics","text":"<p>Numeric metric values attached to a benchmark result.</p> Column Type Constraints Description <code>id</code> INTEGER PRIMARY KEY AUTOINCREMENT Row identifier <code>benchmark_id</code> INTEGER FOREIGN KEY -&gt; benchmarks(id) ON DELETE CASCADE Parent benchmark <code>metric_name</code> TEXT Metric key (e.g., <code>tokens_per_second</code>, <code>accuracy</code>) <code>metric_value</code> REAL Numeric value"},{"location":"reference/database/#hardware","title":"hardware","text":"<p>Hardware snapshot captured at the time of a run.</p> Column Type Constraints Description <code>id</code> INTEGER PRIMARY KEY AUTOINCREMENT Row identifier <code>run_id</code> TEXT FOREIGN KEY -&gt; runs(id) ON DELETE CASCADE Parent run <code>gpu_model</code> TEXT GPU model name <code>gpu_vram_gb</code> INTEGER GPU VRAM in gigabytes <code>gpu_count</code> INTEGER Number of GPUs <code>cpu_model</code> TEXT CPU model name <code>cpu_cores</code> INTEGER Number of CPU cores <code>ram_gb</code> INTEGER System RAM in gigabytes <code>environment_type</code> TEXT Environment (e.g., <code>native_linux</code>, <code>wsl2</code>, <code>dgx</code>) <code>fingerprint</code> TEXT Full hardware fingerprint string"},{"location":"reference/database/#agents","title":"agents","text":"<p>Registered agent daemons for distributed execution.</p> Column Type Constraints Description <code>id</code> TEXT PRIMARY KEY Agent identifier (UUID) <code>name</code> TEXT UNIQUE Human-readable agent name <code>hostname</code> TEXT Agent hostname or IP <code>port</code> INTEGER Agent listen port <code>token</code> TEXT Legacy raw token (migrated to hash) <code>token_hash</code> TEXT SHA-256 hash of authentication token (v4) <code>token_prefix</code> TEXT First 8 chars of raw token for display (v4) <code>status</code> TEXT Current status (<code>online</code>, <code>offline</code>, <code>busy</code>) <code>gpu_info</code> TEXT GPU description string <code>gpu_count</code> INTEGER Number of GPUs on the agent <code>cpu_info</code> TEXT CPU description string <code>ram_gb</code> INTEGER System RAM in gigabytes <code>environment_type</code> TEXT Environment type <code>fingerprint</code> TEXT Hardware fingerprint <code>kitt_version</code> TEXT KITT version running on the agent <code>hardware_details</code> TEXT Detailed hardware JSON blob (v5) <code>last_heartbeat</code> TEXT ISO-8601 timestamp of last heartbeat <code>registered_at</code> TEXT ISO-8601 registration timestamp <code>notes</code> TEXT Free-form notes <code>tags</code> TEXT JSON array of tags for filtering"},{"location":"reference/database/#web_campaigns","title":"web_campaigns","text":"<p>Campaign definitions and progress tracking for the web dashboard.</p> Column Type Constraints Description <code>id</code> TEXT PRIMARY KEY Campaign identifier (UUID) <code>name</code> TEXT Campaign display name <code>description</code> TEXT Human-readable description <code>config_json</code> TEXT Full campaign configuration as JSON <code>status</code> TEXT Current status (<code>pending</code>, <code>running</code>, <code>completed</code>, <code>failed</code>) <code>agent_id</code> TEXT FOREIGN KEY -&gt; agents(id) Assigned agent <code>created_at</code> TEXT Creation timestamp <code>started_at</code> TEXT Execution start timestamp <code>completed_at</code> TEXT Completion timestamp <code>total_runs</code> INTEGER Total runs planned <code>succeeded</code> INTEGER Runs that succeeded <code>failed</code> INTEGER Runs that failed <code>skipped</code> INTEGER Runs that were skipped <code>error</code> TEXT Error message if the campaign failed"},{"location":"reference/database/#quick_tests","title":"quick_tests","text":"<p>Single benchmark dispatches from the web UI.</p> Column Type Constraints Description <code>id</code> TEXT PRIMARY KEY Quick test identifier (UUID) <code>agent_id</code> TEXT FOREIGN KEY -&gt; agents(id) Target agent <code>model_path</code> TEXT Model path or name <code>engine_name</code> TEXT Inference engine <code>benchmark_name</code> TEXT Benchmark to run <code>suite_name</code> TEXT Suite name (if applicable) <code>status</code> TEXT Current status <code>command_id</code> TEXT Command ID for heartbeat dispatch (v6) <code>created_at</code> TEXT Creation timestamp <code>started_at</code> TEXT Execution start timestamp <code>completed_at</code> TEXT Completion timestamp <code>result_id</code> TEXT FOREIGN KEY -&gt; runs(id) Associated run result <code>error</code> TEXT Error message on failure"},{"location":"reference/database/#events","title":"events","text":"<p>Append-only event log for auditing and real-time feeds.</p> Column Type Constraints Description <code>id</code> INTEGER PRIMARY KEY AUTOINCREMENT Event sequence number <code>event_type</code> TEXT Event category (e.g., <code>run.started</code>, <code>agent.registered</code>) <code>source_id</code> TEXT ID of the entity that produced the event <code>data</code> TEXT JSON payload with event-specific details <code>created_at</code> TEXT ISO-8601 event timestamp"},{"location":"reference/database/#web_settings","title":"web_settings","text":"<p>Server-wide key-value settings (v3).</p> Column Type Constraints Description <code>key</code> TEXT PRIMARY KEY Setting key <code>value</code> TEXT Setting value"},{"location":"reference/database/#quick_test_logs","title":"quick_test_logs","text":"<p>Persistent log storage for quick test output (v7).</p> Column Type Constraints Description <code>id</code> INTEGER PRIMARY KEY AUTOINCREMENT Row identifier <code>test_id</code> TEXT FOREIGN KEY -&gt; quick_tests(id) Parent quick test <code>line</code> TEXT Log line content <code>created_at</code> TEXT ISO-8601 timestamp"},{"location":"reference/database/#agent_settings","title":"agent_settings","text":"<p>Per-agent configurable settings, synced to agents via heartbeat (v8).</p> Column Type Constraints Description <code>id</code> INTEGER PRIMARY KEY AUTOINCREMENT Row identifier <code>agent_id</code> TEXT FOREIGN KEY -&gt; agents(id) ON DELETE CASCADE Parent agent <code>key</code> TEXT UNIQUE(agent_id, key) Setting key <code>value</code> TEXT Setting value"},{"location":"reference/database/#indexes","title":"Indexes","text":"<p>The following indexes are created to accelerate common query patterns:</p> Index Table Columns <code>idx_runs_model</code> runs <code>model</code> <code>idx_runs_engine</code> runs <code>engine</code> <code>idx_runs_suite_name</code> runs <code>suite_name</code> <code>idx_runs_timestamp</code> runs <code>timestamp</code> <code>idx_benchmarks_run_id</code> benchmarks <code>run_id</code> <code>idx_metrics_benchmark_id</code> metrics <code>benchmark_id</code> <code>idx_hardware_run_id</code> hardware <code>run_id</code> <code>idx_events_event_type</code> events <code>event_type</code> <code>idx_events_source_id</code> events <code>source_id</code> <code>idx_events_created_at</code> events <code>created_at</code>"},{"location":"reference/database/#postgresql-differences","title":"PostgreSQL Differences","text":"<p>When using PostgreSQL instead of SQLite, the following type mappings apply:</p> SQLite PostgreSQL Affected columns <code>INTEGER PRIMARY KEY AUTOINCREMENT</code> <code>SERIAL PRIMARY KEY</code> <code>benchmarks.id</code>, <code>metrics.id</code>, <code>hardware.id</code>, <code>events.id</code> <code>TEXT</code> (timestamps) <code>TIMESTAMPTZ</code> All <code>*_at</code> and <code>timestamp</code> columns <code>TEXT</code> (JSON) <code>JSONB</code> <code>runs.raw_json</code>, <code>web_campaigns.config_json</code>, <code>events.data</code> <code>INTEGER</code> (booleans) <code>BOOLEAN</code> <code>runs.passed</code>, <code>benchmarks.passed</code> <code>REAL</code> <code>DOUBLE PRECISION</code> <code>runs.total_time_seconds</code>, <code>metrics.metric_value</code> <p>PostgreSQL's <code>JSONB</code> type enables indexing and querying inside JSON columns directly with operators like <code>-&gt;</code>, <code>-&gt;&gt;</code>, and <code>@&gt;</code>.</p>"},{"location":"reference/docker-files/","title":"Docker Files","text":"<p>KITT uses Docker both for its own container image and for launching inference engines as sibling containers. This page documents the project Dockerfile, the monitoring docker-compose stack, and the Docker patterns KITT relies on.</p>"},{"location":"reference/docker-files/#dockerfile","title":"Dockerfile","text":"<p>Location: <code>Dockerfile</code> (project root)</p>"},{"location":"reference/docker-files/#build-stages","title":"Build stages","text":"<p>The Dockerfile uses a single-stage build based on <code>python:3.12-slim</code>:</p> <pre><code>FROM python:3.12-slim\n\n# System dependencies for psutil/pynvml compilation and Docker CLI\nRUN apt-get update &amp;&amp; apt-get install -y --no-install-recommends \\\n    gcc python3-dev docker.io \\\n    &amp;&amp; rm -rf /var/lib/apt/lists/*\n\nWORKDIR /app\n\n# Install Poetry and project dependencies\nCOPY pyproject.toml poetry.lock ./\nRUN pip install poetry &amp;&amp; poetry install --no-root --without dev\n\nCOPY src/ ./src/\nCOPY configs/ ./configs/\nRUN poetry install --only-root\n\nENTRYPOINT [\"poetry\", \"run\", \"kitt\"]\n</code></pre> <p>Key points:</p> <ul> <li><code>docker.io</code> is installed inside the container so KITT can call the   Docker CLI to manage inference engine containers.</li> <li><code>gcc</code> and <code>python3-dev</code> are needed to compile native extensions for   <code>psutil</code> and <code>pynvml</code>.</li> <li>The entry point runs <code>poetry run kitt</code>, so any CLI command can be passed   as arguments (e.g. <code>docker run kitt run -m /model -e vllm</code>).</li> </ul>"},{"location":"reference/docker-files/#multi-architecture-support","title":"Multi-architecture support","text":"<p>The base image <code>python:3.12-slim</code> supports both amd64 and arm64, so the Dockerfile is architecture-agnostic. Agents build the image locally via <code>kitt-agent build</code>, producing a native image for the host architecture. This avoids cross-architecture issues when the server (amd64) and agents (e.g. ARM64 NVIDIA Grace Blackwell) differ.</p>"},{"location":"reference/docker-files/#additional-dockerfiles","title":"Additional Dockerfiles","text":"File Purpose <code>docker/web/Dockerfile</code> Web dashboard image (also used by <code>kitt-agent build</code>) <code>docker/llama_cpp/Dockerfile.spark</code> llama.cpp build for DGX Spark <code>docker/tgi/Dockerfile.spark</code> TGI build for DGX Spark"},{"location":"reference/docker-files/#docker-composeyaml-monitoring","title":"docker-compose.yaml (Monitoring)","text":"<p>Location: <code>docker/monitoring/docker-compose.yaml</code></p> <p>This stack provides metrics collection and visualization:</p> Service Image Port Purpose <code>prometheus</code> <code>prom/prometheus:latest</code> 9090 Metrics scraping <code>grafana</code> <code>grafana/grafana:latest</code> 3000 Dashboard visualization <code>influxdb</code> <code>influxdb:2</code> 8086 Time-series storage for benchmark data <p>Named volumes persist data across restarts: <code>prometheus_data</code>, <code>grafana_data</code>, <code>influxdb_data</code>.</p>"},{"location":"reference/docker-files/#docker-socket-mounting","title":"Docker socket mounting","text":"<p>KITT manages inference engines as sibling containers -- it calls the Docker CLI from inside its own container to start/stop engine containers on the host. This requires mounting the Docker socket:</p> <pre><code>docker run -v /var/run/docker.sock:/var/run/docker.sock kitt run ...\n</code></pre> <p>Without the socket mount, KITT cannot launch or manage engine containers.</p>"},{"location":"reference/docker-files/#network-mode","title":"Network mode","text":"<p>All engine containers use <code>--network host</code> so they bind directly to the host network. This avoids port-mapping complexity and lets KITT reach engines at <code>localhost:&lt;port&gt;</code>:</p> Engine Default Port vLLM 8000 TGI 8080 llama.cpp 8081 Ollama 11434"},{"location":"reference/docker-files/#gpu-passthrough","title":"GPU passthrough","text":"<p>Engine containers require GPU access. KITT passes <code>--gpus all</code> to Docker when launching engine containers:</p> <pre><code>docker run --gpus all --network host vllm/vllm-openai:latest ...\n</code></pre> <p>Ensure the NVIDIA Container Toolkit is installed on the host system. KITT detects GPU availability through <code>pynvml</code> or by parsing <code>nvidia-smi</code> output.</p>"},{"location":"reference/docker-files/#model-volume-mounting","title":"Model volume mounting","text":"<p>Models stored on the host are mounted into engine containers. The host path is typically set via the <code>MODEL_PATH</code> environment variable or the <code>-m</code> CLI flag:</p> <pre><code>docker run --gpus all --network host \\\n  -v /models/llama-8b:/models/llama-8b \\\n  vllm/vllm-openai:latest --model /models/llama-8b\n</code></pre>"},{"location":"reference/environment/","title":"Environment Variables","text":"<p>KITT reads the following environment variables. All are optional and have sensible defaults.</p>"},{"location":"reference/environment/#general","title":"General","text":"Variable Description Default <code>KITT_HOME</code> Configuration and data directory <code>~/.kitt/</code> <code>KITT_SECRET_KEY</code> Flask session secret key Random 32-byte hex"},{"location":"reference/environment/#docker-and-models","title":"Docker and models","text":"Variable Description Default <code>DOCKER_HOST</code> Docker daemon socket URL <code>unix:///var/run/docker.sock</code> <code>MODEL_PATH</code> Default path to model weights on the host (none)"},{"location":"reference/environment/#tls-and-certificates","title":"TLS and certificates","text":"Variable Description Default <code>KITT_TLS_CERT</code> Path to TLS certificate file Auto-generated in <code>~/.kitt/certs/</code> <code>KITT_TLS_KEY</code> Path to TLS private key file Auto-generated in <code>~/.kitt/certs/</code> <code>KITT_TLS_CA</code> Path to CA certificate for mTLS Auto-generated in <code>~/.kitt/certs/</code> <p>When no certificate paths are set and TLS is not disabled (<code>--insecure</code>), KITT auto-generates a self-signed CA and server certificate on first launch.</p>"},{"location":"reference/environment/#api-authentication","title":"API authentication","text":"Variable Description Default <code>KITT_AUTH_TOKEN</code> Bearer token for REST API authentication (none) <p>If neither <code>--auth-token</code> nor <code>KITT_AUTH_TOKEN</code> is set, authenticated endpoints will reject all requests.</p>"},{"location":"reference/environment/#monitoring-stack","title":"Monitoring stack","text":"Variable Description Default <code>KITT_PROMETHEUS_PORT</code> Prometheus listen port <code>9090</code> <code>KITT_GRAFANA_PORT</code> Grafana listen port <code>3000</code> <code>KITT_GRAFANA_PASSWORD</code> Grafana admin password <code>kitt</code> <code>KITT_INFLUXDB_PORT</code> InfluxDB listen port <code>8086</code> <code>KITT_INFLUXDB_TOKEN</code> InfluxDB admin API token <code>kitt-influx-token</code> <code>KITT_INFLUXDB_PASSWORD</code> InfluxDB admin password <code>kittpwd123</code>"},{"location":"reference/environment/#database","title":"Database","text":"Variable Description Default <code>KITT_DB_PATH</code> Path to SQLite database file <code>~/.kitt/kitt.db</code>"},{"location":"reference/environment/#web-ui","title":"Web UI","text":"Variable Description Default <code>KITT_MODEL_DIR</code> Directory the Models tab scans for local model files <code>~/.kitt/models</code> <code>DEVON_URL</code> Devon server URL for the Devon tab iframe and API access (none) <code>DEVON_API_KEY</code> API key for authenticating with remote Devon (none) <p><code>KITT_MODEL_DIR</code>, <code>DEVON_URL</code>, and the results directory can also be configured from the Settings page in the web dashboard. UI-saved values take priority: DB &gt; environment variable &gt; default.</p>"},{"location":"reference/environment/#agent","title":"Agent","text":"Variable Description Default <code>KITT_AGENT_ID</code> Override agent identifier Auto-generated UUID <code>KITT_CONTROLLER_URL</code> URL of the controller (web) instance (none)"},{"location":"reference/cli/","title":"CLI Reference","text":"<p>Complete reference for all KITT commands. This page is auto-generated from the Click decorators in the source code, so it always reflects the current state of the CLI.</p> <p>KITT exposes 18 command groups through a single entry point:</p> Group Purpose <code>run</code> Execute benchmarks against an engine <code>test</code> List benchmarks, create custom tests <code>engines</code> List, check, and set up inference engines <code>results</code> KARR result management, listing, and comparison <code>campaign</code> Run multi-model, multi-engine campaigns <code>monitoring</code> Generate and deploy monitoring stacks <code>stack</code> Generate and manage composable Docker stacks <code>agent</code> Manage distributed benchmark agents <code>remote</code> Remote deployment commands <code>ci</code> CI/CD integration helpers <code>bot</code> Chat-bot testing utilities <code>plugin</code> Manage engine and benchmark plugins <code>charts</code> Generate result charts and visualizations <code>recommend</code> Hardware and configuration recommendations <code>storage</code> Manage result storage backends <code>compare</code> Launch interactive TUI for comparing runs <code>fingerprint</code> Display hardware fingerprint <code>web</code> Launch the web dashboard and REST API"},{"location":"reference/cli/#kitt","title":"kitt","text":"<p>KITT - Kirizan's Inference Testing Tools</p> <p>End-to-end testing suite for LLM inference engines.</p> <p>Usage:</p> <pre><code>kitt [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--version</code> boolean Show the version and exit. <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#agent","title":"agent","text":"<p>Manage the KITT agent daemon.</p> <p>These commands proxy to the thin agent (kitt-agent). Install the agent package first: pip install kitt-agent</p> <p>Usage:</p> <pre><code>kitt agent [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#init","title":"init","text":"<p>Initialize agent configuration.</p> <p>Proxies to: kitt-agent init</p> <p>Usage:</p> <pre><code>kitt agent init [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--server</code> text KITT server URL (e.g., https://server:8080) <code>Sentinel.UNSET</code> <code>--token</code> text Bearer token for authentication (optional) `` <code>--name</code> text Agent name (defaults to hostname) `` <code>--port</code> integer Agent listening port <code>8090</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#start","title":"start","text":"<p>Start the KITT agent daemon.</p> <p>Proxies to: kitt-agent start</p> <p>Usage:</p> <pre><code>kitt agent start [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--config</code> path Path to agent.yaml <code>Sentinel.UNSET</code> <code>--insecure</code> boolean Disable TLS verification <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#status","title":"status","text":"<p>Check agent status.</p> <p>Usage:</p> <pre><code>kitt agent status [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#stop","title":"stop","text":"<p>Stop the KITT agent daemon.</p> <p>Usage:</p> <pre><code>kitt agent stop [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#update","title":"update","text":"<p>Update the agent to the latest version from the server.</p> <p>Proxies to: kitt-agent update</p> <p>Usage:</p> <pre><code>kitt agent update [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--config</code> path Path to agent.yaml <code>Sentinel.UNSET</code> <code>--restart</code> boolean Restart the agent after update <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#bot","title":"bot","text":"<p>Chat bot integration commands.</p> <p>Usage:</p> <pre><code>kitt bot [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#config","title":"config","text":"<p>Show bot configuration info.</p> <p>Usage:</p> <pre><code>kitt bot config [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#start_1","title":"start","text":"<p>Start a chat bot.</p> <p>Usage:</p> <pre><code>kitt bot start [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--platform</code> choice (<code>slack</code> | <code>discord</code>) Bot platform <code>Sentinel.UNSET</code> <code>--token</code> text Bot token <code>Sentinel.UNSET</code> <code>--app-token</code> text App token (Slack only) None <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#campaign","title":"campaign","text":"<p>Manage benchmark campaigns.</p> <p>Usage:</p> <pre><code>kitt campaign [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#create","title":"create","text":"<p>Generate a campaign config from existing benchmark results.</p> <p>Usage:</p> <pre><code>kitt campaign create [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--from-results</code> path Generate campaign config from existing results directory <code>Sentinel.UNSET</code> <code>--output</code>, <code>-o</code> text Output YAML path None <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#cron-status","title":"cron-status","text":"<p>Show scheduled campaigns.</p> <p>Usage:</p> <pre><code>kitt campaign cron-status [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#list","title":"list","text":"<p>List all campaigns.</p> <p>Usage:</p> <pre><code>kitt campaign list [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#run","title":"run","text":"<p>Run a benchmark campaign from a YAML config file.</p> <p>Usage:</p> <pre><code>kitt campaign run [OPTIONS] CONFIG_PATH\n</code></pre> <p>Options:</p> Name Type Description Default <code>--resume</code> boolean Resume a previous campaign run <code>False</code> <code>--dry-run</code> boolean Print planned runs without executing <code>False</code> <code>--campaign-id</code> text Explicit campaign ID (for resume) None <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#schedule","title":"schedule","text":"<p>Schedule a campaign to run on a cron schedule.</p> <p>Usage:</p> <pre><code>kitt campaign schedule [OPTIONS] CONFIG_PATH\n</code></pre> <p>Options:</p> Name Type Description Default <code>--cron</code> text Cron expression (e.g. \"0 2 * * *\") <code>Sentinel.UNSET</code> <code>--id</code> text Schedule identifier None <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#status_1","title":"status","text":"<p>Show status of a campaign.</p> <p>Usage:</p> <pre><code>kitt campaign status [OPTIONS] [CAMPAIGN_ID]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#unschedule","title":"unschedule","text":"<p>Remove a scheduled campaign.</p> <p>Usage:</p> <pre><code>kitt campaign unschedule [OPTIONS] SCHEDULE_ID\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#wizard","title":"wizard","text":"<p>Interactive campaign builder wizard.</p> <p>Usage:</p> <pre><code>kitt campaign wizard [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#charts","title":"charts","text":"<p>Generate charts and visualizations.</p> <p>Usage:</p> <pre><code>kitt charts [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#quant-curves","title":"quant-curves","text":"<p>Generate quantization quality tradeoff curves.</p> <p>Usage:</p> <pre><code>kitt charts quant-curves [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--model-family</code> text Filter by model family (e.g. Llama-3) None <code>--output</code>, <code>-o</code> text Output file path <code>quant_curves.svg</code> <code>--csv</code> boolean Export data as CSV instead <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#ci","title":"ci","text":"<p>CI/CD integration commands.</p> <p>Usage:</p> <pre><code>kitt ci [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#report","title":"report","text":"<p>Generate CI report from benchmark results.</p> <p>Usage:</p> <pre><code>kitt ci report [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--results-dir</code> path Results directory <code>Sentinel.UNSET</code> <code>--baseline-dir</code> path Baseline results for comparison None <code>--github-token</code> text GitHub API token None <code>--repo</code> text GitHub repo (owner/repo) None <code>--pr</code> integer Pull request number None <code>--output</code>, <code>-o</code> text Write report to file instead of posting None <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#compare","title":"compare","text":"<p>Launch TUI for comparing benchmark results.</p> <p>Pass paths to result directories or metrics.json files.</p> <p>Example: kitt compare ./result-1 ./result-2</p> <p>Usage:</p> <pre><code>kitt compare [OPTIONS] RUNS...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#engines","title":"engines","text":"<p>Manage inference engines.</p> <p>Usage:</p> <pre><code>kitt engines [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#check","title":"check","text":"<p>Check if a specific engine is available and show details.</p> <p>Usage:</p> <pre><code>kitt engines check [OPTIONS] ENGINE_NAME\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#list_1","title":"list","text":"<p>List all registered inference engines and their availability.</p> <p>Usage:</p> <pre><code>kitt engines list [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#setup","title":"setup","text":"<p>Pull or build the Docker image for an engine.</p> <p>Usage:</p> <pre><code>kitt engines setup [OPTIONS] ENGINE_NAME\n</code></pre> <p>Options:</p> Name Type Description Default <code>--dry-run</code> boolean Show commands without executing them <code>False</code> <code>--force-rebuild</code> boolean Rebuild even if image already exists <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#fingerprint","title":"fingerprint","text":"<p>Display hardware fingerprint for this system.</p> <p>Usage:</p> <pre><code>kitt fingerprint [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--verbose</code> boolean Show detailed hardware info <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#monitoring","title":"monitoring","text":"<p>Manage the Grafana/Prometheus monitoring stack.</p> <p>Usage:</p> <pre><code>kitt monitoring [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#deploy","title":"deploy","text":"<p>Deploy a generated monitoring stack to a remote host.</p> <p>Usage:</p> <pre><code>kitt monitoring deploy [OPTIONS] NAME\n</code></pre> <p>Options:</p> Name Type Description Default <code>--host</code> text Remote host name (from ~/.kitt/hosts.yaml). <code>Sentinel.UNSET</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#generate","title":"generate","text":"<p>Generate a customized monitoring stack.</p> <p>Creates a docker-compose stack at ~/.kitt/monitoring// with Prometheus, Grafana, and InfluxDB configured for the given scrape targets. <p>Usage:</p> <pre><code>kitt monitoring generate [OPTIONS] NAME\n</code></pre> <p>Options:</p> Name Type Description Default <code>-t</code>, <code>--target</code> text Scrape target host:port (repeatable). <code>Sentinel.UNSET</code> <code>--grafana-port</code> integer Grafana port (default: 3000). <code>3000</code> <code>--prometheus-port</code> integer Prometheus port (default: 9090). <code>9090</code> <code>--influxdb-port</code> integer InfluxDB port (default: 8086). <code>8086</code> <code>--grafana-password</code> text Grafana admin password. <code>kitt</code> <code>--influxdb-token</code> text InfluxDB admin token. <code>kitt-influx-token</code> <code>--deploy</code> boolean Deploy to remote host after generation. <code>False</code> <code>--host</code> text Remote host name (from ~/.kitt/hosts.yaml). None <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#list-stacks","title":"list-stacks","text":"<p>List all generated monitoring stacks.</p> <p>Usage:</p> <pre><code>kitt monitoring list-stacks [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#remote-start","title":"remote-start","text":"<p>Start a deployed monitoring stack on a remote host.</p> <p>Usage:</p> <pre><code>kitt monitoring remote-start [OPTIONS] NAME\n</code></pre> <p>Options:</p> Name Type Description Default <code>--host</code> text Remote host name (from ~/.kitt/hosts.yaml). <code>Sentinel.UNSET</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#remote-status","title":"remote-status","text":"<p>Check status of a deployed monitoring stack on a remote host.</p> <p>Usage:</p> <pre><code>kitt monitoring remote-status [OPTIONS] NAME\n</code></pre> <p>Options:</p> Name Type Description Default <code>--host</code> text Remote host name (from ~/.kitt/hosts.yaml). <code>Sentinel.UNSET</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#remote-stop","title":"remote-stop","text":"<p>Stop a deployed monitoring stack on a remote host.</p> <p>Usage:</p> <pre><code>kitt monitoring remote-stop [OPTIONS] NAME\n</code></pre> <p>Options:</p> Name Type Description Default <code>--host</code> text Remote host name (from ~/.kitt/hosts.yaml). <code>Sentinel.UNSET</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#remove-stack","title":"remove-stack","text":"<p>Remove a monitoring stack configuration.</p> <p>Usage:</p> <pre><code>kitt monitoring remove-stack [OPTIONS] NAME\n</code></pre> <p>Options:</p> Name Type Description Default <code>--delete-files</code> boolean Also delete generated files on disk. <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#start_2","title":"start","text":"<p>Start the monitoring stack (Prometheus, Grafana, InfluxDB).</p> <p>Usage:</p> <pre><code>kitt monitoring start [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--compose-dir</code> path Path to docker-compose directory (auto-detected by default). None <code>--name</code> text Name of a generated monitoring stack to start. None <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#status_2","title":"status","text":"<p>Show monitoring stack status.</p> <p>Usage:</p> <pre><code>kitt monitoring status [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--compose-dir</code> path Path to docker-compose directory. None <code>--name</code> text Name of a generated monitoring stack to check. None <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#stop_1","title":"stop","text":"<p>Stop the monitoring stack.</p> <p>Usage:</p> <pre><code>kitt monitoring stop [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--compose-dir</code> path Path to docker-compose directory. None <code>--name</code> text Name of a generated monitoring stack to stop. None <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#plugin","title":"plugin","text":"<p>Manage KITT plugins.</p> <p>Usage:</p> <pre><code>kitt plugin [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#install","title":"install","text":"<p>Install a KITT plugin package.</p> <p>Usage:</p> <pre><code>kitt plugin install [OPTIONS] PACKAGE\n</code></pre> <p>Options:</p> Name Type Description Default <code>--version</code> text Version constraint (e.g. '&gt;=0.2.0') None <code>--upgrade</code> boolean Upgrade if already installed <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#list_2","title":"list","text":"<p>List installed KITT plugins.</p> <p>Usage:</p> <pre><code>kitt plugin list [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#remove","title":"remove","text":"<p>Remove a KITT plugin package.</p> <p>Usage:</p> <pre><code>kitt plugin remove [OPTIONS] PACKAGE\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#recommend","title":"recommend","text":"<p>Recommend models based on benchmark history.</p> <p>Usage:</p> <pre><code>kitt recommend [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--max-vram</code> float Maximum VRAM in GB None <code>--max-ram</code> float Maximum RAM in GB None <code>--min-throughput</code> float Minimum throughput (tokens/sec) None <code>--min-accuracy</code> float Minimum accuracy (0-1) None <code>--max-latency</code> float Maximum latency (ms) None <code>--engine</code> text Restrict to engine None <code>--sort</code> choice (<code>score</code> | <code>throughput</code> | <code>accuracy</code>) N/A <code>score</code> <code>--pareto</code> boolean Show only Pareto-optimal models <code>False</code> <code>--limit</code> integer Number of recommendations <code>10</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#remote","title":"remote","text":"<p>Remote host management and execution.</p> <p>Usage:</p> <pre><code>kitt remote [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#list_3","title":"list","text":"<p>List configured remote hosts.</p> <p>Usage:</p> <pre><code>kitt remote list [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#logs","title":"logs","text":"<p>View campaign logs from a remote host.</p> <p>Usage:</p> <pre><code>kitt remote logs [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--host</code> text Remote host name <code>Sentinel.UNSET</code> <code>--tail</code> integer Number of log lines <code>50</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#remove_1","title":"remove","text":"<p>Remove a configured remote host.</p> <p>Usage:</p> <pre><code>kitt remote remove [OPTIONS] NAME\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#run_1","title":"run","text":"<p>Run a campaign on a remote host.</p> <p>Usage:</p> <pre><code>kitt remote run [OPTIONS] CONFIG_PATH\n</code></pre> <p>Options:</p> Name Type Description Default <code>--host</code> text Remote host name <code>Sentinel.UNSET</code> <code>--dry-run</code> boolean Dry run mode <code>False</code> <code>--wait</code> boolean Wait for completion <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#setup_1","title":"setup","text":"<p>Set up a remote host for KITT execution.</p> <p>HOST_SPEC: user@hostname or hostname</p> <p>Usage:</p> <pre><code>kitt remote setup [OPTIONS] HOST_SPEC\n</code></pre> <p>Options:</p> Name Type Description Default <code>--name</code> text Friendly name for the host None <code>--ssh-key</code> text Path to SSH key None <code>--no-install</code> boolean Skip KITT installation <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#status_3","title":"status","text":"<p>Check campaign status on a remote host.</p> <p>Usage:</p> <pre><code>kitt remote status [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--host</code> text Remote host name <code>Sentinel.UNSET</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#sync","title":"sync","text":"<p>Sync results from a remote host.</p> <p>Usage:</p> <pre><code>kitt remote sync [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--host</code> text Remote host name <code>Sentinel.UNSET</code> <code>--output</code>, <code>-o</code> path Local results directory None <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#test","title":"test","text":"<p>Test connectivity to a remote host.</p> <p>Usage:</p> <pre><code>kitt remote test [OPTIONS] NAME\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#results","title":"results","text":"<p>Manage benchmark results.</p> <p>Usage:</p> <pre><code>kitt results [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#cleanup","title":"cleanup","text":"<p>Clean up old Git LFS objects to reduce repository size.</p> <p>Usage:</p> <pre><code>kitt results cleanup [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--repo</code> path Results repository path <code>Sentinel.UNSET</code> <code>--days</code> integer Keep objects from last N days <code>90</code> <code>--dry-run</code> boolean Show what would be deleted <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#compare_1","title":"compare","text":"<p>Compare benchmark results from different runs.</p> <p>Usage:</p> <pre><code>kitt results compare [OPTIONS] RUN1 RUN2\n</code></pre> <p>Options:</p> Name Type Description Default <code>--additional</code> text Additional runs to compare <code>Sentinel.UNSET</code> <code>--format</code> choice (<code>table</code> | <code>json</code>) N/A <code>table</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#import","title":"import","text":"<p>Import results from a directory into a KARR repo.</p> <p>Usage:</p> <pre><code>kitt results import [OPTIONS] SOURCE\n</code></pre> <p>Options:</p> Name Type Description Default <code>--karr</code> path KARR repo to import into <code>Sentinel.UNSET</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#init_1","title":"init","text":"<p>Initialize a new KARR results repository.</p> <p>Usage:</p> <pre><code>kitt results init [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--path</code>, <code>-p</code> path Path for KARR repo <code>Sentinel.UNSET</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#list_4","title":"list","text":"<p>List local benchmark results.</p> <p>Usage:</p> <pre><code>kitt results list [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--model</code> text Filter by model <code>Sentinel.UNSET</code> <code>--engine</code> text Filter by engine <code>Sentinel.UNSET</code> <code>--karr</code> path Path to KARR repo <code>Sentinel.UNSET</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#submit","title":"submit","text":"<p>Submit results via pull request.</p> <p>Usage:</p> <pre><code>kitt results submit [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--repo</code> path Results repository path <code>Sentinel.UNSET</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#run_2","title":"run","text":"<p>Run benchmarks against a model using a specified engine.</p> <p>Usage:</p> <pre><code>kitt run [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--model</code>, <code>-m</code> text Path to model or model identifier <code>Sentinel.UNSET</code> <code>--engine</code>, <code>-e</code> text Inference engine to use (vllm, tgi, llama_cpp, ollama) <code>Sentinel.UNSET</code> <code>--suite</code>, <code>-s</code> text Test suite to run (quick, standard, performance) <code>quick</code> <code>--output</code>, <code>-o</code> path Output directory for results <code>Sentinel.UNSET</code> <code>--skip-warmup</code> boolean Skip warmup phase for all benchmarks <code>False</code> <code>--runs</code> integer Override number of runs per benchmark None <code>--config</code> path Path to custom configuration file <code>Sentinel.UNSET</code> <code>--store-karr</code> boolean Store results in KARR repository <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#stack","title":"stack","text":"<p>Generate and manage composable Docker deployment stacks.</p> <p>Usage:</p> <pre><code>kitt stack [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#generate_1","title":"generate","text":"<p>Generate a composable Docker deployment stack.</p> <p>Creates a docker-compose stack at ~/.kitt/stacks// with the selected components. <p>Usage:</p> <pre><code>kitt stack generate [OPTIONS] NAME\n</code></pre> <p>Options:</p> Name Type Description Default <code>--web</code> boolean Include web UI + REST API service. <code>False</code> <code>--reporting</code> boolean Include lightweight read-only dashboard. <code>False</code> <code>--agent</code> boolean Include agent daemon for GPU servers. <code>False</code> <code>--postgres</code> boolean Include PostgreSQL database. <code>False</code> <code>--monitoring</code> boolean Include Prometheus + Grafana + InfluxDB. <code>False</code> <code>--port</code> integer Web/reporting port (default: 8080). <code>8080</code> <code>--agent-port</code> integer Agent port (default: 8090). <code>8090</code> <code>--postgres-port</code> integer PostgreSQL port (default: 5432). <code>5432</code> <code>--grafana-port</code> integer Grafana port (default: 3000). <code>3000</code> <code>--prometheus-port</code> integer Prometheus port (default: 9090). <code>9090</code> <code>--influxdb-port</code> integer InfluxDB port (default: 8086). <code>8086</code> <code>--auth-token</code> text Bearer token for API auth. <code>changeme</code> <code>--secret-key</code> text Flask secret key. `` <code>--postgres-password</code> text PostgreSQL password (default: kitt). <code>kitt</code> <code>--server-url</code> text KITT server URL (for agent registration). `` <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#list_5","title":"list","text":"<p>List all generated stacks.</p> <p>Usage:</p> <pre><code>kitt stack list [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#remove_2","title":"remove","text":"<p>Remove a stack configuration.</p> <p>Usage:</p> <pre><code>kitt stack remove [OPTIONS] NAME\n</code></pre> <p>Options:</p> Name Type Description Default <code>--delete-files</code> boolean Also delete generated files on disk. <code>False</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#start_3","title":"start","text":"<p>Start a generated stack.</p> <p>Usage:</p> <pre><code>kitt stack start [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--name</code> text Name of the stack to start. <code>Sentinel.UNSET</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#status_4","title":"status","text":"<p>Show stack status.</p> <p>Usage:</p> <pre><code>kitt stack status [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--name</code> text Name of the stack to check. <code>Sentinel.UNSET</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#stop_2","title":"stop","text":"<p>Stop a running stack.</p> <p>Usage:</p> <pre><code>kitt stack stop [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--name</code> text Name of the stack to stop. <code>Sentinel.UNSET</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#storage","title":"storage","text":"<p>Manage the KITT result storage backend.</p> <p>Usage:</p> <pre><code>kitt storage [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#export","title":"export","text":"<p>Export a result from the database to a JSON file.</p> <p>Usage:</p> <pre><code>kitt storage export [OPTIONS] RESULT_ID\n</code></pre> <p>Options:</p> Name Type Description Default <code>--output</code>, <code>-o</code> path Output JSON path <code>Sentinel.UNSET</code> <code>--db-path</code> path N/A None <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#import_1","title":"import","text":"<p>Import results from a directory or JSON file into the database.</p> <p>Usage:</p> <pre><code>kitt storage import [OPTIONS] SOURCE\n</code></pre> <p>Options:</p> Name Type Description Default <code>--db-path</code> path SQLite database path None <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#init_2","title":"init","text":"<p>Initialize the SQLite storage database.</p> <p>Usage:</p> <pre><code>kitt storage init [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--db-path</code> path SQLite database path (default: ~/.kitt/kitt.db) None <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#list_6","title":"list","text":"<p>List results stored in the database.</p> <p>Usage:</p> <pre><code>kitt storage list [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--db-path</code> path N/A None <code>--model</code> text Filter by model None <code>--engine</code> text Filter by engine None <code>--limit</code> integer Max results to show <code>50</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#migrate","title":"migrate","text":"<p>Run pending database migrations.</p> <p>Usage:</p> <pre><code>kitt storage migrate [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--db-path</code> path SQLite database path None <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#stats","title":"stats","text":"<p>Show storage statistics.</p> <p>Usage:</p> <pre><code>kitt storage stats [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--db-path</code> path N/A None <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#test_1","title":"test","text":"<p>Manage benchmarks and test definitions.</p> <p>Usage:</p> <pre><code>kitt test [OPTIONS] COMMAND [ARGS]...\n</code></pre> <p>Options:</p> Name Type Description Default <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#list_7","title":"list","text":"<p>List available benchmarks.</p> <p>Usage:</p> <pre><code>kitt test list [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--category</code>, <code>-c</code> text Filter by category <code>Sentinel.UNSET</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#new","title":"new","text":"<p>Create a new benchmark definition from template.</p> <p>Usage:</p> <pre><code>kitt test new [OPTIONS] NAME\n</code></pre> <p>Options:</p> Name Type Description Default <code>--category</code>, <code>-c</code> text Benchmark category <code>quality_custom</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/cli/#web","title":"web","text":"<p>Launch web dashboard for viewing results.</p> <p>Usage:</p> <pre><code>kitt web [OPTIONS]\n</code></pre> <p>Options:</p> Name Type Description Default <code>--port</code> integer Port for web UI <code>8080</code> <code>--host</code> text Host to bind to <code>0.0.0.0</code> <code>--results-dir</code> path Results directory <code>Sentinel.UNSET</code> <code>--debug</code> boolean Enable debug mode <code>False</code> <code>--legacy</code> boolean Use legacy read-only dashboard <code>False</code> <code>--insecure</code> boolean Disable TLS (development only) <code>False</code> <code>--tls-cert</code> path Path to TLS certificate <code>Sentinel.UNSET</code> <code>--tls-key</code> path Path to TLS private key <code>Sentinel.UNSET</code> <code>--tls-ca</code> path Path to CA certificate <code>Sentinel.UNSET</code> <code>--auth-token</code> text Bearer token for API auth <code>Sentinel.UNSET</code> <code>--help</code> boolean Show this message and exit. <code>False</code>"},{"location":"reference/configuration/","title":"Configuration Files","text":"<p>KITT uses YAML configuration files stored in the <code>configs/</code> directory at the project root. Every configuration file is validated at load time with Pydantic v2 models defined in <code>src/kitt/config/models.py</code>.</p>"},{"location":"reference/configuration/#directory-layout","title":"Directory layout","text":"<pre><code>configs/\n\u251c\u2500\u2500 suites/              # Test-suite definitions\n\u2502   \u251c\u2500\u2500 quick.yaml\n\u2502   \u251c\u2500\u2500 standard.yaml\n\u2502   \u2514\u2500\u2500 performance.yaml\n\u251c\u2500\u2500 engines/             # Engine parameter overrides\n\u2502   \u251c\u2500\u2500 vllm.yaml\n\u2502   \u251c\u2500\u2500 tgi.yaml\n\u2502   \u251c\u2500\u2500 llama_cpp.yaml\n\u2502   \u251c\u2500\u2500 ollama.yaml\n\u2502   \u2514\u2500\u2500 profiles/        # Named engine profiles\n\u251c\u2500\u2500 campaigns/           # Multi-model, multi-engine campaign configs\n\u2502   \u251c\u2500\u2500 example.yaml\n\u2502   \u2514\u2500\u2500 scheduled_example.yaml\n\u2514\u2500\u2500 tests/               # Benchmark definitions\n    \u251c\u2500\u2500 performance/     # Built-in performance benchmarks\n    \u2514\u2500\u2500 quality/\n        \u251c\u2500\u2500 standard/    # Built-in quality benchmarks (MMLU, GSM8K, ...)\n        \u2514\u2500\u2500 custom/      # User-created custom benchmarks\n</code></pre>"},{"location":"reference/configuration/#configuration-types","title":"Configuration types","text":"Type Pydantic Model Reference Suite <code>SuiteConfig</code> Suite Configuration Engine <code>EngineConfig</code> Engine Configuration Campaign (free-form YAML) Campaign Configuration Benchmark <code>TestConfig</code> Custom Benchmark Configuration"},{"location":"reference/configuration/#loading-pattern","title":"Loading pattern","text":"<p>All configs follow the same load pattern:</p> <pre><code>from kitt.config.loader import load_config\nfrom kitt.config.models import SuiteConfig\n\nsuite = load_config(\"configs/suites/standard.yaml\", SuiteConfig)\n</code></pre> <p><code>load_config()</code> reads the YAML file, passes the resulting dictionary to the Pydantic model, and raises a validation error if any field is invalid or missing.</p>"},{"location":"reference/configuration/benchmarks/","title":"Custom Benchmark Configuration","text":"<p>Custom benchmarks allow you to define your own evaluation tests using YAML files. They are loaded at runtime by the <code>YAMLBenchmark</code> class from <code>kitt.benchmarks.loader</code> and validated against the <code>TestConfig</code> Pydantic model.</p>"},{"location":"reference/configuration/benchmarks/#file-location","title":"File location","text":"<p>Place custom benchmark YAML files in:</p> <pre><code>configs/tests/quality/custom/\n</code></pre> <p>KITT discovers them automatically when listing tests or running suites.</p>"},{"location":"reference/configuration/benchmarks/#schema","title":"Schema","text":"Field Type Required Description <code>name</code> <code>str</code> Yes Unique benchmark identifier <code>version</code> <code>str</code> No Semantic version (default: <code>\"1.0.0\"</code>) <code>category</code> <code>str</code> Yes Must be <code>quality_custom</code> for custom benchmarks <code>description</code> <code>str</code> No Human-readable description <code>warmup</code> <code>WarmupConfig</code> No Warmup phase settings <code>dataset</code> <code>DatasetConfig</code> No Dataset source configuration <code>prompts</code> <code>PromptConfig</code> No Prompt template and few-shot settings <code>sampling</code> <code>SamplingParams</code> No Generation sampling parameters <code>evaluation</code> <code>EvaluationConfig</code> No Metrics and answer extraction <code>runs</code> <code>int</code> No Number of runs (default: <code>3</code>)"},{"location":"reference/configuration/benchmarks/#datasetconfig","title":"DatasetConfig","text":"Field Type Default Description <code>source</code> <code>str</code> <code>null</code> HuggingFace dataset ID <code>local_path</code> <code>str</code> <code>null</code> Path to a local dataset directory <code>split</code> <code>str</code> <code>\"test\"</code> Dataset split to use <code>sample_size</code> <code>int</code> <code>null</code> Number of samples (<code>null</code> = all)"},{"location":"reference/configuration/benchmarks/#promptconfig","title":"PromptConfig","text":"Field Type Default Description <code>template</code> <code>str</code> <code>\"\"</code> Prompt template with <code>{question}</code> placeholder <code>few_shot</code> <code>int</code> <code>0</code> Number of few-shot examples to prepend <code>few_shot_source</code> <code>str</code> <code>\"dev\"</code> Split to draw few-shot examples from"},{"location":"reference/configuration/benchmarks/#evaluationconfig","title":"EvaluationConfig","text":"Field Type Description <code>metrics</code> <code>list[str]</code> Metric names to compute (e.g. <code>accuracy</code>) <code>answer_extraction</code> <code>dict</code> Rules for extracting answers from output"},{"location":"reference/configuration/benchmarks/#example","title":"Example","text":"<pre><code>name: example_custom\nversion: \"1.0.0\"\ncategory: quality_custom\ndescription: \"Example custom benchmark\"\n\nwarmup:\n  enabled: true\n  iterations: 3\n  log_warmup_times: true\n\ndataset:\n  local_path: ./datasets/custom/\n  sample_size: 10\n\nprompts:\n  template: |\n    Answer the following question concisely.\n\n    Question: {question}\n    Answer:\n\nsampling:\n  temperature: 0.0\n  max_tokens: 256\n\nevaluation:\n  metrics:\n    - accuracy\n\nruns: 1\n</code></pre>"},{"location":"reference/configuration/benchmarks/#creating-a-custom-benchmark","title":"Creating a custom benchmark","text":"<ol> <li> <p>Copy the example file to a new YAML in <code>configs/tests/quality/custom/</code>:</p> <pre><code>cp configs/tests/quality/custom/example_custom.yaml \\\n   configs/tests/quality/custom/my_test.yaml\n</code></pre> </li> <li> <p>Edit the file: set <code>name</code>, <code>category: quality_custom</code>, configure the    dataset source (HuggingFace <code>source</code> or <code>local_path</code>), and customize    the prompt template.</p> </li> <li> <p>Verify the benchmark loads:</p> <pre><code>kitt test list\n</code></pre> </li> <li> <p>Run it as part of a suite or directly:</p> <pre><code>kitt run -m /path/to/model -e vllm -s my_suite\n</code></pre> </li> </ol>"},{"location":"reference/configuration/campaigns/","title":"Campaign Configuration","text":"<p>Campaign configuration files define multi-model, multi-engine benchmark runs. KITT expands the campaign into a matrix of models x engines x suites and executes each combination. Files live in <code>configs/campaigns/</code>.</p>"},{"location":"reference/configuration/campaigns/#schema","title":"Schema","text":"Field Type Required Description <code>campaign_name</code> <code>str</code> Yes Unique campaign identifier <code>description</code> <code>str</code> No Human-readable description <code>schedule</code> <code>str</code> No Cron expression for scheduled runs <code>auto_compare</code> <code>bool</code> No Compare with previous run on completion <code>models</code> <code>list[Model]</code> Yes Models to benchmark <code>engines</code> <code>list[Engine]</code> Yes Engines to test against <code>disk</code> <code>DiskConfig</code> No Disk space management <code>notifications</code> <code>NotifyConfig</code> No Notification settings <code>quant_filter</code> <code>QuantFilter</code> No Quantization file filters <code>resource_limits</code> <code>ResourceLimits</code> No Skip models exceeding limits <code>parallel</code> <code>bool</code> No Run combinations in parallel (default: <code>false</code>) <code>devon_managed</code> <code>bool</code> No Use Devon for model downloads (default: <code>false</code>)"},{"location":"reference/configuration/campaigns/#model-entry","title":"Model entry","text":"Field Type Description <code>name</code> <code>str</code> Model display name <code>params</code> <code>str</code> Parameter count label (e.g. <code>\"8B\"</code>) <code>safetensors_repo</code> <code>str</code> HuggingFace repo for safetensors format <code>gguf_repo</code> <code>str</code> HuggingFace repo for GGUF format <code>ollama_tag</code> <code>str</code> Ollama model tag <code>estimated_size_gb</code> <code>float</code> Approximate disk footprint"},{"location":"reference/configuration/campaigns/#engine-entry","title":"Engine entry","text":"Field Type Description <code>name</code> <code>str</code> Registered engine name <code>suite</code> <code>str</code> Suite to run for this engine <code>formats</code> <code>list[str]</code> Accepted model formats (<code>safetensors</code>, <code>gguf</code>) <code>config</code> <code>dict</code> Engine-specific parameter overrides <code>quant_filter</code> <code>str</code> Specific quantization to select"},{"location":"reference/configuration/campaigns/#example","title":"Example","text":"<pre><code>campaign_name: example-campaign\ndescription: \"Test 2 models across llama.cpp and Ollama\"\n\nmodels:\n  - name: Llama-3.1-8B-Instruct\n    params: \"8B\"\n    safetensors_repo: meta-llama/Llama-3.1-8B-Instruct\n    gguf_repo: bartowski/Meta-Llama-3.1-8B-Instruct-GGUF\n    ollama_tag: \"llama3.1:8b\"\n    estimated_size_gb: 16.0\n\n  - name: Qwen2.5-7B-Instruct\n    params: \"7B\"\n    safetensors_repo: Qwen/Qwen2.5-7B-Instruct\n    gguf_repo: Qwen/Qwen2.5-7B-Instruct-GGUF\n    ollama_tag: \"qwen2.5:7b\"\n    estimated_size_gb: 14.0\n\nengines:\n  - name: llama_cpp\n    suite: standard\n    formats: [gguf]\n    config: {}\n\n  - name: ollama\n    suite: standard\n    formats: [gguf]\n    config: {}\n\ndisk:\n  reserve_gb: 100.0\n  cleanup_after_run: true\n\nnotifications:\n  desktop: true\n  on_complete: true\n  on_failure: true\n\nquant_filter:\n  skip_patterns:\n    - \"IQ1_*\"\n    - \"IQ2_*\"\n    - \"Q4_0_4_4\"\n    - \"Q4_0_4_8\"\n    - \"Q4_0_8_8\"\n\nparallel: false\ndevon_managed: true\n</code></pre>"},{"location":"reference/configuration/campaigns/#matrix-expansion","title":"Matrix expansion","text":"<p>Given 2 models and 2 engines, KITT creates 4 benchmark runs (one per combination). Each run uses the suite specified in the engine entry. Models are matched to engines based on the <code>formats</code> field -- for example, a model without a <code>gguf_repo</code> will be skipped for engines that require the <code>gguf</code> format.</p>"},{"location":"reference/configuration/campaigns/#scheduled-campaigns","title":"Scheduled campaigns","text":"<p>Add a <code>schedule</code> field with a cron expression to run the campaign automatically:</p> <pre><code>schedule: \"0 2 * * *\"   # 2 AM daily\nauto_compare: true\n</code></pre>"},{"location":"reference/configuration/engines/","title":"Engine Configuration","text":"<p>Engine configuration files customize how KITT launches and communicates with each inference engine. Files live in <code>configs/engines/</code> and are validated against the <code>EngineConfig</code> Pydantic model.</p>"},{"location":"reference/configuration/engines/#schema","title":"Schema","text":"Field Type Required Description <code>name</code> <code>str</code> Yes Engine identifier (must match a registered engine) <code>model_path</code> <code>str</code> No Default model path (usually set at runtime) <code>parameters</code> <code>dict</code> No Engine-specific parameters passed at startup"},{"location":"reference/configuration/engines/#built-in-engine-configs","title":"Built-in engine configs","text":""},{"location":"reference/configuration/engines/#vllm","title":"vLLM","text":"<p>File: <code>configs/engines/vllm.yaml</code></p> <pre><code>name: vllm\nparameters:\n  tensor_parallel_size: 1\n  gpu_memory_utilization: 0.9\n  dtype: auto\n  trust_remote_code: false\n</code></pre> Parameter Default Description <code>tensor_parallel_size</code> <code>1</code> Number of GPUs for tensor parallelism <code>gpu_memory_utilization</code> <code>0.9</code> Fraction of GPU memory to use <code>dtype</code> <code>auto</code> Data type (<code>auto</code>, <code>float16</code>, <code>bfloat16</code>) <code>trust_remote_code</code> <code>false</code> Allow custom model code from HuggingFace"},{"location":"reference/configuration/engines/#tgi-text-generation-inference","title":"TGI (Text Generation Inference)","text":"<p>File: <code>configs/engines/tgi.yaml</code></p> <pre><code>name: tgi\nparameters:\n  base_url: \"http://localhost:8080\"\n  max_concurrent_requests: 128\n</code></pre>"},{"location":"reference/configuration/engines/#llamacpp","title":"llama.cpp","text":"<p>File: <code>configs/engines/llama_cpp.yaml</code></p> <pre><code>name: llama_cpp\nparameters:\n  n_ctx: 4096\n  n_gpu_layers: -1\n  n_threads: null\n  verbose: false\n</code></pre> Parameter Default Description <code>n_ctx</code> <code>4096</code> Context window size <code>n_gpu_layers</code> <code>-1</code> GPU layers to offload (<code>-1</code> = all) <code>n_threads</code> <code>null</code> CPU threads (<code>null</code> = auto-detect) <code>verbose</code> <code>false</code> Enable verbose engine logging"},{"location":"reference/configuration/engines/#ollama","title":"Ollama","text":"<p>File: <code>configs/engines/ollama.yaml</code></p> <pre><code>name: ollama\nparameters:\n  base_url: \"http://localhost:11434\"\n</code></pre>"},{"location":"reference/configuration/engines/#engine-profiles","title":"Engine profiles","text":"<p>Named profiles live in <code>configs/engines/profiles/</code> and provide preset parameter combinations. For example, <code>llama_cpp-high-ctx.yaml</code> overrides the default context window for llama.cpp.</p>"},{"location":"reference/configuration/engines/#overriding-engine-settings-at-runtime","title":"Overriding engine settings at runtime","text":"<p>Engine parameters can also be set through the <code>EngineConfig</code> model_path and parameters at runtime via the CLI or campaign configs:</p> <pre><code>kitt run -m /models/llama-8b -e vllm -o ./results\n</code></pre> <p>Campaign configs can supply per-engine settings in the <code>engines[].config</code> field. See Campaign Configuration for details.</p>"},{"location":"reference/configuration/suites/","title":"Suite Configuration","text":"<p>Suite configuration files define which benchmarks to run together and how to configure them. Files live in <code>configs/suites/</code> and are validated against the <code>SuiteConfig</code> Pydantic model.</p>"},{"location":"reference/configuration/suites/#schema","title":"Schema","text":"Field Type Required Description <code>suite_name</code> <code>str</code> Yes Unique identifier for the suite <code>version</code> <code>str</code> No Semantic version (default: <code>\"1.0.0\"</code>) <code>description</code> <code>str</code> No Human-readable description <code>tests</code> <code>list[str]</code> Yes Benchmark names to include <code>global_config</code> <code>dict</code> No Settings applied to every test (e.g. <code>runs</code>) <code>sampling_overrides</code> <code>SamplingParams</code> No Override sampling defaults for all tests <code>test_overrides</code> <code>dict[str, SuiteOverrides]</code> No Per-test overrides keyed by test name"},{"location":"reference/configuration/suites/#suiteoverrides","title":"SuiteOverrides","text":"Field Type Description <code>warmup</code> <code>WarmupConfig</code> Override warmup settings for this test <code>sampling</code> <code>SamplingParams</code> Override sampling parameters for this test <code>runs</code> <code>int</code> Override the number of runs for this test"},{"location":"reference/configuration/suites/#samplingparams","title":"SamplingParams","text":"Field Type Default Description <code>temperature</code> <code>float</code> <code>0.0</code> Sampling temperature (0.0--2.0) <code>top_p</code> <code>float</code> <code>1.0</code> Nucleus sampling threshold (0.0--1.0) <code>top_k</code> <code>int</code> <code>50</code> Top-k sampling <code>max_tokens</code> <code>int</code> <code>2048</code> Maximum tokens to generate"},{"location":"reference/configuration/suites/#built-in-suites","title":"Built-in suites","text":""},{"location":"reference/configuration/suites/#quick","title":"quick","text":"<p>Smoke test that runs only the throughput benchmark with a single run.</p> <pre><code>suite_name: quick\nversion: \"1.0.0\"\ndescription: \"Quick smoke test - runs throughput benchmark only\"\n\ntests:\n  - throughput\n\nglobal_config:\n  runs: 1\n\nsampling_overrides:\n  temperature: 0.0\n  max_tokens: 128\n</code></pre>"},{"location":"reference/configuration/suites/#standard","title":"standard","text":"<p>Full evaluation across all quality and performance benchmarks with three runs per test.</p> <pre><code>suite_name: standard_benchmarks\nversion: \"1.1.0\"\ndescription: \"Standard academic benchmarks for LLM evaluation (V1)\"\n\ntests:\n  - mmlu\n  - gsm8k\n  - truthfulqa\n  - hellaswag\n  - throughput\n  - latency\n  - memory_usage\n  - warmup_analysis\n\nglobal_config:\n  runs: 3\n\nsampling_overrides:\n  temperature: 0.0\n  max_tokens: 2048\n\ntest_overrides:\n  mmlu:\n    runs: 1\n  warmup_analysis:\n    warmup:\n      enabled: false\n</code></pre>"},{"location":"reference/configuration/suites/#performance","title":"performance","text":"<p>Performance-focused suite that skips quality benchmarks entirely.</p> <pre><code>suite_name: performance\nversion: \"1.0.0\"\ndescription: \"Performance-focused benchmark suite\"\n\ntests:\n  - throughput\n  - latency\n  - memory_usage\n  - warmup_analysis\n\nglobal_config:\n  runs: 3\n\nsampling_overrides:\n  temperature: 0.0\n  max_tokens: 2048\n\ntest_overrides:\n  warmup_analysis:\n    warmup:\n      enabled: false\n</code></pre>"},{"location":"reference/configuration/suites/#creating-a-custom-suite","title":"Creating a custom suite","text":"<ol> <li> <p>Create a new YAML file in <code>configs/suites/</code>:</p> <pre><code>suite_name: my_suite\ndescription: \"Custom suite for latency testing\"\n\ntests:\n  - latency\n  - throughput\n\nglobal_config:\n  runs: 5\n\nsampling_overrides:\n  temperature: 0.0\n  max_tokens: 512\n</code></pre> </li> <li> <p>Run it with:</p> <pre><code>kitt run -m /path/to/model -e vllm -s my_suite\n</code></pre> </li> </ol>"}]}